Label,Number,Abstract
1,1,"  This paper proposes a method to address the longstanding problem of lack of
monotonicity in estimation of conditional and structural quantile functions,
also known as the quantile crossing problem. The method consists in sorting or
monotone rearranging the original estimated non-monotone curve into a monotone
rearranged curve. We show that the rearranged curve is closer to the true
quantile curve in finite samples than the original curve, establish a
functional delta method for rearrangement-related operators, and derive
functional limit theory for the entire rearranged curve and its functionals. We
also establish validity of the bootstrap for estimating the limit law of the
the entire rearranged curve and its functionals. Our limit results are generic
in that they apply to every estimator of a monotone econometric function,
provided that the estimator satisfies a functional central limit theorem and
the function satisfies some smoothness conditions. Consequently, our results
apply to estimation of other econometric functions with monotonicity
restrictions, such as demand, production, distribution, and structural
distribution functions. We illustrate the results with an application to
estimation of structural quantile functions using data on Vietnam veteran
status and earnings.
"
1,2,"  Suppose that a target function is monotonic, namely, weakly increasing, and
an original estimate of the target function is available, which is not weakly
increasing. Many common estimation methods used in statistics produce such
estimates. We show that these estimates can always be improved with no harm
using rearrangement techniques: The rearrangement methods, univariate and
multivariate, transform the original estimate to a monotonic estimate, and the
resulting estimate is closer to the true curve in common metrics than the
original estimate. We illustrate the results with a computational example and
an empirical example dealing with age-height growth charts.
"
1,3,"  This paper applies a regularization procedure called increasing rearrangement
to monotonize Edgeworth and Cornish-Fisher expansions and any other related
approximations of distribution and quantile functions of sample statistics.
Besides satisfying the logical monotonicity, required of distribution and
quantile functions, the procedure often delivers strikingly better
approximations to the distribution and quantile functions of the sample mean
than the original Edgeworth-Cornish-Fisher expansions.
"
1,4,"  An evolutionarily stable strategy (ESS) is an equilibrium strategy that is
immune to invasions by rare alternative (``mutant'') strategies. Unlike Nash
equilibria, ESS do not always exist in finite games. In this paper we address
the question of what happens when the size of the game increases: does an ESS
exist for ``almost every large'' game? Letting the entries in the $n\times n$
game matrix be independently randomly chosen according to a distribution $F$,
we study the number of ESS with support of size $2.$ In particular, we show
that, as $n\to \infty$, the probability of having such an ESS: (i) converges to
1 for distributions $F$ with ``exponential and faster decreasing tails'' (e.g.,
uniform, normal, exponential); and (ii) converges to $1-1/\sqrt{e}$ for
distributions $F$ with ``slower than exponential decreasing tails'' (e.g.,
lognormal, Pareto, Cauchy). Our results also imply that the expected number of
vertices of the convex hull of $n$ random points in the plane converges to
infinity for the distributions in (i), and to 4 for the distributions in (ii).
"
1,5,"  Motivated by several classic decision-theoretic paradoxes, and by analogies
with the paradoxes which in physics motivated the development of quantum
mechanics, we introduce a projective generalization of expected utility along
the lines of the quantum-mechanical generalization of probability theory. The
resulting decision theory accommodates the dominant paradoxes, while retaining
significant simplicity and tractability. In particular, every finite game
within this larger class of preferences still has an equilibrium.
"
1,6,"  Suppose that a target function is monotonic, namely, weakly increasing, and
an available original estimate of this target function is not weakly
increasing. Rearrangements, univariate and multivariate, transform the original
estimate to a monotonic estimate that always lies closer in common metrics to
the target function. Furthermore, suppose an original simultaneous confidence
interval, which covers the target function with probability at least
$1-\alpha$, is defined by an upper and lower end-point functions that are not
weakly increasing. Then the rearranged confidence interval, defined by the
rearranged upper and lower end-point functions, is shorter in length in common
norms than the original interval and also covers the target function with
probability at least $1-\alpha$. We demonstrate the utility of the improved
point and interval estimates with an age-height growth chart example.
"
1,7,"  Counterfactual distributions are important ingredients for policy analysis
and decomposition analysis in empirical economics. In this article we develop
modeling and inference tools for counterfactual distributions based on
regression methods. The counterfactual scenarios that we consider consist of
ceteris paribus changes in either the distribution of covariates related to the
outcome of interest or the conditional distribution of the outcome given
covariates. For either of these scenarios we derive joint functional central
limit theorems and bootstrap validity results for regression-based estimators
of the status quo and counterfactual outcome distributions. These results allow
us to construct simultaneous confidence sets for function-valued effects of the
counterfactual changes, including the effects on the entire distribution and
quantile functions of the outcome as well as on related functionals. These
confidence sets can be used to test functional hypotheses such as no-effect,
positive effect, or stochastic dominance. Our theory applies to general
counterfactual changes and covers the main regression methods including
classical, quantile, duration, and distribution regressions. We illustrate the
results with an empirical application to wage decompositions using data for the
United States.
  As a part of developing the main results, we introduce distribution
regression as a comprehensive and flexible tool for modeling and estimating the
\textit{entire} conditional distribution. We show that distribution regression
encompasses the Cox duration regression and represents a useful alternative to
quantile regression. We establish functional central limit theorems and
bootstrap validity results for the empirical distribution regression process
and various related functionals.
"
1,8,"  Nonseparable panel models are important in a variety of economic settings,
including discrete choice. This paper gives identification and estimation
results for nonseparable models under time homogeneity conditions that are like
""time is randomly assigned"" or ""time is an instrument."" Partial identification
results for average and quantile effects are given for discrete regressors,
under static or dynamic conditions, in fully nonparametric and in
semiparametric models, with time effects. It is shown that the usual, linear,
fixed-effects estimator is not a consistent estimator of the identified average
effect, and a consistent estimator is given. A simple estimator of identified
quantile treatment effects is given, providing a solution to the important
problem of estimating quantile treatment effects from panel data. Bounds for
overall effects in static and dynamic models are given. The dynamic bounds
provide a partial identification solution to the important problem of
estimating the effect of state dependence in the presence of unobserved
heterogeneity. The impact of $T$, the number of time periods, is shown by
deriving shrinkage rates for the identified set as $T$ grows. We also consider
semiparametric, discrete-choice models and find that semiparametric panel
bounds can be much tighter than nonparametric bounds.
Computationally-convenient methods for semiparametric models are presented. We
propose a novel inference method that applies in panel data and other settings
and show that it produces uniformly valid confidence regions in large samples.
We give empirical illustrations.
"
1,9,"  We consider median regression and, more generally, a possibly infinite
collection of quantile regressions in high-dimensional sparse models. In these
models the overall number of regressors $p$ is very large, possibly larger than
the sample size $n$, but only $s$ of these regressors have non-zero impact on
the conditional quantile of the response variable, where $s$ grows slower than
$n$. We consider quantile regression penalized by the $\ell_1$-norm of
coefficients ($\ell_1$-QR). First, we show that $\ell_1$-QR is consistent at
the rate $\sqrt{s/n} \sqrt{\log p}$. The overall number of regressors $p$
affects the rate only through the $\log p$ factor, thus allowing nearly
exponential growth in the number of zero-impact regressors. The rate result
holds under relatively weak conditions, requiring that $s/n$ converges to zero
at a super-logarithmic speed and that regularization parameter satisfies
certain theoretical constraints. Second, we propose a pivotal, data-driven
choice of the regularization parameter and show that it satisfies these
theoretical constraints. Third, we show that $\ell_1$-QR correctly selects the
true minimal model as a valid submodel, when the non-zero coefficients of the
true model are well separated from zero. We also show that the number of
non-zero coefficients in $\ell_1$-QR is of same stochastic order as $s$.
Fourth, we analyze the rate of convergence of a two-step estimator that applies
ordinary quantile regression to the selected model. Fifth, we evaluate the
performance of $\ell_1$-QR in a Monte-Carlo experiment, and illustrate its use
on an international economic growth application.
"
1,10,"  This work studies the large sample properties of the posterior-based
inference in the curved exponential family under increasing dimension. The
curved structure arises from the imposition of various restrictions on the
model, such as moment restrictions, and plays a fundamental role in
econometrics and others branches of data analysis. We establish conditions
under which the posterior distribution is approximately normal, which in turn
implies various good properties of estimation and inference procedures based on
the posterior. In the process we also revisit and improve upon previous results
for the exponential family under increasing dimension by making use of
concentration of measure. We also discuss a variety of applications to
high-dimensional versions of the classical econometric models including the
multinomial model with moment restrictions, seemingly unrelated regression
equations, and single structural equation models. In our analysis, both the
parameter dimension and the number of moments are increasing with the sample
size.
"
1,11,"  In most contemporary approaches to decision making, a decision problem is
described by a sets of states and set of outcomes, and a rich set of acts,
which are functions from states to outcomes over which the decision maker (DM)
has preferences. Most interesting decision problems, however, do not come with
a state space and an outcome space. Indeed, in complex problems it is often far
from clear what the state and outcome spaces would be. We present an
alternative foundation for decision making, in which the primitive objects of
choice are syntactic programs. A representation theorem is proved in the spirit
of standard representation theorems, showing that if the DM's preference
relation on objects of choice satisfies appropriate axioms, then there exist a
set S of states, a set O of outcomes, a way of interpreting the objects of
choice as functions from S to O, a probability on S, and a utility function on
O, such that the DM prefers choice a to choice b if and only if the expected
utility of a is higher than that of b. Thus, the state space and outcome space
are subjective, just like the probability and utility; they are not part of the
description of the problem. In principle, a modeler can test for SEU behavior
without having access to states or outcomes. We illustrate the power of our
approach by showing that it can capture decision makers who are subject to
framing effects.
"
1,12,"  Arrow's theorem implies that a social choice function satisfying
Transitivity, the Pareto Principle (Unanimity) and Independence of Irrelevant
Alternatives (IIA) must be dictatorial. When non-strict preferences are
allowed, a dictatorial social choice function is defined as a function for
which there exists a single voter whose strict preferences are followed. This
definition allows for many different dictatorial functions. In particular, we
construct examples of dictatorial functions which do not satisfy Transitivity
and IIA. Thus Arrow's theorem, in the case of non-strict preferences, does not
provide a complete characterization of all social choice functions satisfying
Transitivity, the Pareto Principle, and IIA.
  The main results of this article provide such a characterization for Arrow's
theorem, as well as for follow up results by Wilson. In particular, we
strengthen Arrow's and Wilson's result by giving an exact if and only if
condition for a function to satisfy Transitivity and IIA (and the Pareto
Principle). Additionally, we derive formulas for the number of functions
satisfying these conditions.
"
1,13,"  Quantile regression is an increasingly important empirical tool in economics
and other sciences for analyzing the impact of a set of regressors on the
conditional distribution of an outcome. Extremal quantile regression, or
quantile regression applied to the tails, is of interest in many economic and
financial applications, such as conditional value-at-risk, production
efficiency, and adjustment bands in (S,s) models. In this paper we provide
feasible inference tools for extremal conditional quantile models that rely
upon extreme value approximations to the distribution of self-normalized
quantile regression statistics. The methods are simple to implement and can be
of independent interest even in the non-regression case. We illustrate the
results with two empirical examples analyzing extreme fluctuations of a stock
return and extremely low percentiles of live infants' birthweights in the range
between 250 and 1500 grams.
"
1,14,"  Non-symmetric rectangular correlation matrices occur in many problems in
economics. We test the method of extracting statistically meaningful
correlations between input and output variables of large dimensionality and
build a toy model for artificially included correlations in large random time
series.The results are then applied to analysis of polish macroeconomic data
and can be used as an alternative to classical cointegration approach.
"
1,15,"  We develop results for the use of Lasso and Post-Lasso methods to form
first-stage predictions and estimate optimal instruments in linear instrumental
variables (IV) models with many instruments, $p$. Our results apply even when
$p$ is much larger than the sample size, $n$. We show that the IV estimator
based on using Lasso or Post-Lasso in the first stage is root-n consistent and
asymptotically normal when the first-stage is approximately sparse; i.e. when
the conditional expectation of the endogenous variables given the instruments
can be well-approximated by a relatively small set of variables whose
identities may be unknown. We also show the estimator is semi-parametrically
efficient when the structural error is homoscedastic. Notably our results allow
for imperfect model selection, and do not rely upon the unrealistic ""beta-min""
conditions that are widely used to establish validity of inference following
model selection. In simulation experiments, the Lasso-based IV estimator with a
data-driven penalty performs well compared to recently advocated
many-instrument-robust procedures. In an empirical example dealing with the
effect of judicial eminent domain decisions on economic outcomes, the
Lasso-based IV estimator outperforms an intuitive benchmark.
  In developing the IV results, we establish a series of new results for Lasso
and Post-Lasso estimators of nonparametric conditional expectation functions
which are of independent theoretical and practical interest. We construct a
modification of Lasso designed to deal with non-Gaussian, heteroscedastic
disturbances which uses a data-weighted $\ell_1$-penalty function. Using
moderate deviation theory for self-normalized sums, we provide convergence
rates for the resulting Lasso and Post-Lasso estimators that are as sharp as
the corresponding rates in the homoscedastic Gaussian case under the condition
that $\log p = o(n^{1/3})$.
"
1,16,"  In this note, we propose to use sparse methods (e.g. LASSO, Post-LASSO,
sqrt-LASSO, and Post-sqrt-LASSO) to form first-stage predictions and estimate
optimal instruments in linear instrumental variables (IV) models with many
instruments in the canonical Gaussian case. The methods apply even when the
number of instruments is much larger than the sample size. We derive asymptotic
distributions for the resulting IV estimators and provide conditions under
which these sparsity-based IV estimators are asymptotically oracle-efficient.
In simulation experiments, a sparsity-based IV estimator with a data-driven
penalty performs well compared to recently advocated many-instrument-robust
procedures. We illustrate the procedure in an empirical example using the
Angrist and Krueger (1991) schooling data.
"
1,17,"  In this paper, we develop a new censored quantile instrumental variable
(CQIV) estimator and describe its properties and computation. The CQIV
estimator combines Powell (1986) censored quantile regression (CQR) to deal
with censoring, with a control variable approach to incorporate endogenous
regressors. The CQIV estimator is obtained in two stages that are non-additive
in the unobservables. The first stage estimates a non-additive model with
infinite dimensional parameters for the control variable, such as a quantile or
distribution regression model. The second stage estimates a non-additive
censored quantile regression model for the response variable of interest,
including the estimated control variable to deal with endogeneity. For
computation, we extend the algorithm for CQR developed by Chernozhukov and Hong
(2002) to incorporate the estimation of the control variable. We give generic
regularity conditions for asymptotic normality of the CQIV estimator and for
the validity of resampling methods to approximate its asymptotic distribution.
We verify these conditions for quantile and distribution regression estimation
of the control variable. Our analysis covers two-stage (uncensored) quantile
regression with non-additive first stage as an important special case. We
illustrate the computation and applicability of the CQIV estimator with a
Monte-Carlo numerical example and an empirical application on estimation of
Engel curves for alcohol.
"
1,18,"  Quantile regression (QR) is a principal regression method for analyzing the
impact of covariates on outcomes. The impact is described by the conditional
quantile function and its functionals. In this paper we develop the
nonparametric QR-series framework, covering many regressors as a special case,
for performing inference on the entire conditional quantile function and its
linear functionals. In this framework, we approximate the entire conditional
quantile function by a linear combination of series terms with
quantile-specific coefficients and estimate the function-valued coefficients
from the data. We develop large sample theory for the QR-series coefficient
process, namely we obtain uniform strong approximations to the QR-series
coefficient process by conditionally pivotal and Gaussian processes. Based on
these strong approximations, or couplings, we develop four resampling methods
(pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be
used for inference on the entire QR-series coefficient function.
  We apply these results to obtain estimation and inference methods for linear
functionals of the conditional quantile function, such as the conditional
quantile function itself, its partial derivatives, average partial derivatives,
and conditional average partial derivatives. Specifically, we obtain uniform
rates of convergence and show how to use the four resampling methods mentioned
above for inference on the functionals. All of the above results are for
function-valued parameters, holding uniformly in both the quantile index and
the covariate value, and covering the pointwise case as a by-product. We
demonstrate the practical utility of these results with an example, where we
estimate the price elasticity function and test the Slutsky condition of the
individual demand for gasoline, as indexed by the individual unobserved
propensity for gasoline consumption.
"
1,19,"  In this chapter we discuss conceptually high dimensional sparse econometric
models as well as estimation of these models using L1-penalization and
post-L1-penalization methods. Focusing on linear and nonparametric regression
frameworks, we discuss various econometric examples, present basic theoretical
results, and illustrate the concepts and methods with Monte Carlo simulations
and an empirical application. In the application, we examine and confirm the
empirical validity of the Solow-Swan model for international economic growth.
"
1,20,"  We study team decision problems where communication is not possible, but
coordination among team members can be realized via signals in a shared
environment. We consider a variety of decision problems that differ in what
team members know about one another's actions and knowledge. For each type of
decision problem, we investigate how different assumptions on the available
signals affect team performance. Specifically, we consider the cases of
perfectly correlated, i.i.d., and exchangeable classical signals, as well as
the case of quantum signals. We find that, whereas in perfect-recall trees
(Kuhn [1950], [1953]) no type of signal improves performance, in
imperfect-recall trees quantum signals may bring an improvement. Isbell [1957]
proved that in non-Kuhn trees, classical i.i.d. signals may improve
performance. We show that further improvement may be possible by use of
classical exchangeable or quantum signals. We include an example of the effect
of quantum signals in the context of high-frequency trading.
"
1,21,"  This article is about estimation and inference methods for high dimensional
sparse (HDS) regression models in econometrics. High dimensional sparse models
arise in situations where many regressors (or series terms) are available and
the regression function is well-approximated by a parsimonious, yet unknown set
of regressors. The latter condition makes it possible to estimate the entire
regression function effectively by searching for approximately the right set of
regressors. We discuss methods for identifying this set of regressors and
estimating their coefficients based on $\ell_1$-penalization and describe key
theoretical results. In order to capture realistic practical situations, we
expressly allow for imperfect selection of regressors and study the impact of
this imperfect selection on estimation and inference results. We focus the main
part of the article on the use of HDS models and methods in the instrumental
variables model and the partially linear model. We present a set of novel
inference results for these models and illustrate their use with applications
to returns to schooling and growth regression.
"
1,22,"  We propose robust methods for inference on the effect of a treatment variable
on a scalar outcome in the presence of very many controls. Our setting is a
partially linear model with possibly non-Gaussian and heteroscedastic
disturbances. Our analysis allows the number of controls to be much larger than
the sample size. To make informative inference feasible, we require the model
to be approximately sparse; that is, we require that the effect of confounding
factors can be controlled for up to a small approximation error by conditioning
on a relatively small number of controls whose identities are unknown. The
latter condition makes it possible to estimate the treatment effect by
selecting approximately the right set of controls. We develop a novel
estimation and uniformly valid inference method for the treatment effect in
this setting, called the ""post-double-selection"" method. Our results apply to
Lasso-type methods used for covariate selection as well as to any other model
selection method that is able to find a sparse model with good approximation
properties.
  The main attractive feature of our method is that it allows for imperfect
selection of the controls and provides confidence intervals that are valid
uniformly across a large class of models. In contrast, standard post-model
selection estimators fail to provide uniform inference even in simple cases
with a small, fixed number of controls. Thus our method resolves the problem of
uniform inference after model selection for a large, interesting class of
models. We illustrate the use of the developed methods with numerical
simulations and an application to the effect of abortion on crime rates.
"
1,23,"  Maximizing the revenue from selling _more than one_ good (or item) to a
single buyer is a notoriously difficult problem, in stark contrast to the
one-good case. For two goods, we show that simple ""one-dimensional"" mechanisms,
such as selling the goods separately, _guarantee_ at least 73% of the optimal
revenue when the valuations of the two goods are independent and identically
distributed, and at least $50\%$ when they are independent. For the case of
$k>2$ independent goods, we show that selling them separately guarantees at
least a $c/\log^2 k$ fraction of the optimal revenue; and, for independent and
identically distributed goods, we show that selling them as one bundle
guarantees at least a $c/\log k$ fraction of the optimal revenue. Additional
results compare the revenues from the two simple mechanisms of selling the
goods separately and bundled, identify situations where bundling is optimal,
and extend the analysis to multiple buyers.
"
1,24,"  This paper considers fixed effects estimation and inference in linear and
nonlinear panel data models with random coefficients and endogenous regressors.
The quantities of interest -- means, variances, and other moments of the random
coefficients -- are estimated by cross sectional sample moments of GMM
estimators applied separately to the time series of each individual. To deal
with the incidental parameter problem introduced by the noise of the
within-individual estimators in short panels, we develop bias corrections.
These corrections are based on higher-order asymptotic expansions of the GMM
estimators and produce improved point and interval estimates in moderately long
panels. Under asymptotic sequences where the cross sectional and time series
dimensions of the panel pass to infinity at the same rate, the uncorrected
estimator has an asymptotic bias of the same order as the asymptotic variance.
The bias corrections remove the bias without increasing variance. An empirical
example on cigarette demand based on Becker, Grossman and Murphy (1994) shows
significant heterogeneity in the price effect across U.S. states.
"
1,25,"  We consider a large class of social learning models in which a group of
agents face uncertainty regarding a state of the world, share the same utility
function, observe private signals, and interact in a general dynamic setting.
We introduce Social Learning Equilibria, a static equilibrium concept that
abstracts away from the details of the given extensive form, but nevertheless
captures the corresponding asymptotic equilibrium behavior. We establish
general conditions for agreement, herding, and information aggregation in
equilibrium, highlighting a connection between agreement and information
aggregation.
"
1,26,"  We consider a group of strategic agents who must each repeatedly take one of
two possible actions. They learn which of the two actions is preferable from
initial private signals, and by observing the actions of their neighbors in a
social network.
  We show that the question of whether or not the agents learn efficiently
depends on the topology of the social network. In particular, we identify a
geometric ""egalitarianism"" condition on the social network that guarantees
learning in infinite networks, or learning with high probability in large
finite networks, in any equilibrium. We also give examples of non-egalitarian
networks with equilibria in which learning fails.
"
1,27,"  We propose dual regression as an alternative to the quantile regression
process for the global estimation of conditional distribution functions under
minimal assumptions. Dual regression provides all the interpretational power of
the quantile regression process while avoiding the need for repairing the
intersecting conditional quantile surfaces that quantile regression often
produces in practice. Our approach introduces a mathematical programming
characterization of conditional distribution functions which, in its simplest
form, is the dual program of a simultaneous estimator for linear location-scale
models. We apply our general characterization to the specification and
estimation of a flexible class of conditional distribution functions, and
present asymptotic theory for the corresponding empirical dual regression
process.
"
1,28,"  In applications it is common that the exact form of a conditional expectation
is unknown and having flexible functional forms can lead to improvements.
Series method offers that by approximating the unknown function based on $k$
basis functions, where $k$ is allowed to grow with the sample size $n$. We
consider series estimators for the conditional mean in light of: (i) sharp LLNs
for matrices derived from the noncommutative Khinchin inequalities, (ii) bounds
on the Lebesgue factor that controls the ratio between the $L^\infty$ and
$L_2$-norms of approximation errors, (iii) maximal inequalities for processes
whose entropy integrals diverge, and (iv) strong approximations to series-type
processes.
  These technical tools allow us to contribute to the series literature,
specifically the seminal work of Newey (1997), as follows. First, we weaken the
condition on the number $k$ of approximating functions used in series
estimation from the typical $k^2/n \to 0$ to $k/n \to 0$, up to log factors,
which was available only for spline series before. Second, we derive $L_2$
rates and pointwise central limit theorems results when the approximation error
vanishes. Under an incorrectly specified model, i.e. when the approximation
error does not vanish, analogous results are also shown. Third, under stronger
conditions we derive uniform rates and functional central limit theorems that
hold if the approximation error vanishes or not. That is, we derive the strong
approximation for the entire estimate of the nonparametric function.
  We derive uniform rates, Gaussian approximations, and uniform confidence
bands for a wide collection of linear functionals of the conditional
expectation function.
"
1,29,"  We provide a comprehensive semi-parametric study of Bayesian partially
identified econometric models. While the existing literature on Bayesian
partial identification has mostly focused on the structural parameter, our
primary focus is on Bayesian credible sets (BCS's) of the unknown identified
set and the posterior distribution of its support function. We construct a
(two-sided) BCS based on the support function of the identified set. We prove
the Bernstein-von Mises theorem for the posterior distribution of the support
function. This powerful result in turn infers that, while the BCS and the
frequentist confidence set for the partially identified parameter are
asymptotically different, our constructed BCS for the identified set has an
asymptotically correct frequentist coverage probability. Importantly, we
illustrate that the constructed BCS for the identified set does not require a
prior on the structural parameter. It can be computed efficiently for subset
inference, especially when the target of interest is a sub-vector of the
partially identified parameter, where projecting to a low-dimensional subset is
often required. Hence, the proposed methods are useful in many applications.
  The Bayesian partial identification literature has been assuming a known
parametric likelihood function. However, econometric models usually only
identify a set of moment inequalities, and therefore using an incorrect
likelihood function may result in misleading inferences. In contrast, with a
nonparametric prior on the unknown likelihood function, our proposed Bayesian
procedure only requires a set of moment conditions, and can efficiently make
inference about both the partially identified parameter and its identified set.
This makes it widely applicable in general moment inequality models. Finally,
the proposed method is illustrated in a financial asset pricing problem.
"
1,30,"  We derive a Gaussian approximation result for the maximum of a sum of
high-dimensional random vectors. Specifically, we establish conditions under
which the distribution of the maximum is approximated by that of the maximum of
a sum of the Gaussian random vectors with the same covariance matrices as the
original vectors. This result applies when the dimension of random vectors
($p$) is large compared to the sample size ($n$); in fact, $p$ can be much
larger than $n$, without restricting correlations of the coordinates of these
vectors. We also show that the distribution of the maximum of a sum of the
random vectors with unknown covariance matrices can be consistently estimated
by the distribution of the maximum of a sum of the conditional Gaussian random
vectors obtained by multiplying the original vectors with i.i.d. Gaussian
multipliers. This is the Gaussian multiplier (or wild) bootstrap procedure.
Here too, $p$ can be large or even much larger than $n$. These distributional
approximations, either Gaussian or conditional Gaussian, yield a high-quality
approximation to the distribution of the original maximum, often with
approximation error decreasing polynomially in the sample size, and hence are
of interest in many applications. We demonstrate how our Gaussian
approximations and the multiplier bootstrap can be used for modern
high-dimensional estimation, multiple hypothesis testing, and adaptive
specification testing. All these results contain nonasymptotic bounds on
approximation errors.
"
1,31,"  In this article, we review quantile models with endogeneity. We focus on
models that achieve identification through the use of instrumental variables
and discuss conditions under which partial and point identification are
obtained. We discuss key conditions, which include monotonicity and
full-rank-type conditions, in detail. In providing this review, we update the
identification results of Chernozhukov and Hansen (2005, Econometrica). We
illustrate the modeling assumptions through economically motivated examples. We
also briefly review the literature on estimation and inference.
  Key Words: identification, treatment effects, structural models, instrumental
variables
"
1,32,"  We develop uniformly valid confidence regions for regression coefficients in
a high-dimensional sparse median regression model with homoscedastic errors.
Our methods are based on a moment equation that is immunized against
non-regular estimation of the nuisance part of the median regression function
by using Neyman's orthogonalization. We establish that the resulting
instrumental median regression estimator of a target regression coefficient is
asymptotically normally distributed uniformly with respect to the underlying
sparse model and is semi-parametrically efficient. We also generalize our
method to a general non-smooth Z-estimation framework with the number of target
parameters $p_1$ being possibly much larger than the sample size $n$. We extend
Huber's results on asymptotic normality to this setting, demonstrating uniform
asymptotic normality of the proposed estimators over $p_1$-dimensional
rectangles, constructing simultaneous confidence bands on all of the $p_1$
target parameters, and establishing asymptotic validity of the bands uniformly
over underlying approximately sparse models.
  Keywords: Instrument; Post-selection inference; Sparsity; Neyman's Orthogonal
Score test; Uniformly valid inference; Z-estimation.
"
1,33,"  This paper considers generalized linear models in the presence of many
controls. We lay out a general methodology to estimate an effect of interest
based on the construction of an instrument that immunize against model
selection mistakes and apply it to the case of logistic binary choice model.
More specifically we propose new methods for estimating and constructing
confidence regions for a regression parameter of primary interest $\alpha_0$, a
parameter in front of the regressor of interest, such as the treatment variable
or a policy variable. These methods allow to estimate $\alpha_0$ at the
root-$n$ rate when the total number $p$ of other regressors, called controls,
potentially exceed the sample size $n$ using sparsity assumptions. The sparsity
assumption means that there is a subset of $s<n$ controls which suffices to
accurately approximate the nuisance part of the regression function.
Importantly, the estimators and these resulting confidence regions are valid
uniformly over $s$-sparse models satisfying $s^2\log^2 p = o(n)$ and other
technical conditions. These procedures do not rely on traditional consistent
model selection arguments for their validity. In fact, they are robust with
respect to moderate model selection mistakes in variable selection. Under
suitable conditions, the estimators are semi-parametrically efficient in the
sense of attaining the semi-parametric efficiency bounds for the class of
models in this paper.
"
1,34,"  We consider the well known, and notoriously difficult, problem of a single
revenue-maximizing seller selling two or more heterogeneous goods to a single
buyer whose private values for the goods are drawn from a (possibly correlated)
known distribution, and whose valuation is additive over the goods. We show
that when there are two (or more) goods, _simple mechanisms_ -- such as selling
the goods separately or as a bundle -- _may yield only a negligible fraction of
the optimal revenue_. This resolves the open problem of Briest, Chawla,
Kleinberg, and Weinberg (JET 2015) who prove the result for at least three
goods in the related setup of a unit-demand buyer. We also introduce the menu
size as a simple measure of the complexity of mechanisms, and show that the
revenue may increase polynomially with _menu size_ and that no bounded menu
size can ensure any positive fraction of the optimal revenue. The menu size
also turns out to ""pin down"" the revenue properties of deterministic
mechanisms.
"
1,35,"  We consider the complexity of finding a correlated equilibrium of an
$n$-player game in a model that allows the algorithm to make queries on
players' payoffs at pure strategy profiles. Randomized regret-based dynamics
are known to yield an approximate correlated equilibrium efficiently, namely,
in time that is polynomial in the number of players $n$. Here we show that both
randomization and approximation are necessary: no efficient deterministic
algorithm can reach even an approximate correlated equilibrium, and no
efficient randomized algorithm can reach an exact correlated equilibrium. The
results are obtained by bounding from below the number of payoff queries that
are needed.
"
1,36,"  In this supplementary appendix we provide additional results, omitted proofs
and extensive simulations that complement the analysis of the main text
(arXiv:1201.0224).
"
1,37,"  We introduce a new solution concept, called periodicity, for selecting
optimal strategies in strategic form games. This periodicity solution concept
yields new insight into non-trivial games. In mixed strategy strategic form
games, periodic solutions yield values for the utility function of each player
that are equal to the Nash equilibrium ones. In contrast to the Nash
strategies, here the payoffs of each player are robust against what the
opponent plays. Sometimes, periodicity strategies yield higher utilities, and
sometimes the Nash strategies do, but often the utilities of these two
strategies coincide. We formally define and study periodic strategies in two
player perfect information strategic form games with pure strategies and we
prove that every non-trivial finite game has at least one periodic strategy,
with non-trivial meaning non-degenerate payoffs. In some classes of games where
mixed strategies are used, we identify quantitative features. Particularly
interesting are the implications for collective action games, since there the
collective action strategy can be incorporated in a purely non-cooperative
context. Moreover, we address the periodicity issue when the players have a
continuum set of strategies available.
"
1,38,"  This paper concerns robust inference on average treatment effects following
model selection. In the selection on observables framework, we show how to
construct confidence intervals based on a doubly-robust estimator that are
robust to model selection errors and prove that they are valid uniformly over a
large class of treatment effect models. The class allows for multivalued
treatments with heterogeneous effects (in observables), general
heteroskedasticity, and selection amongst (possibly) more covariates than
observations. Our estimator attains the semiparametric efficiency bound under
appropriate conditions. Precise conditions are given for any model selector to
yield these results, and we show how to combine data-driven selection with
economic theory. For implementation, we give a specific proposal for selection
based on the group lasso, which is particularly well-suited to treatment
effects data, and derive new results for high-dimensional, sparse multinomial
logistic regression. A simulation study shows our estimator performs very well
in finite samples over a wide range of models. Revisiting the National
Supported Work demonstration data, our method yields accurate estimates and
tight confidence intervals.
"
1,39,"  We study the problem of nonparametric regression when the regressor is
endogenous, which is an important nonparametric instrumental variables (NPIV)
regression in econometrics and a difficult ill-posed inverse problem with
unknown operator in statistics. We first establish a general upper bound on the
sup-norm (uniform) convergence rate of a sieve estimator, allowing for
endogenous regressors and weakly dependent data. This result leads to the
optimal sup-norm convergence rates for spline and wavelet least squares
regression estimators under weakly dependent data and heavy-tailed error terms.
This upper bound also yields the sup-norm convergence rates for sieve NPIV
estimators under i.i.d. data: the rates coincide with the known optimal
$L^2$-norm rates for severely ill-posed problems, and are power of $\log(n)$
slower than the optimal $L^2$-norm rates for mildly ill-posed problems. We then
establish the minimax risk lower bound in sup-norm loss, which coincides with
our upper bounds on sup-norm rates for the spline and wavelet sieve NPIV
estimators. This sup-norm rate optimality provides another justification for
the wide application of sieve NPIV estimators. Useful results on
weakly-dependent random matrices are also provided.
"
1,40,"  In this paper, we provide efficient estimators and honest confidence bands
for a variety of treatment effects including local average (LATE) and local
quantile treatment effects (LQTE) in data-rich environments. We can handle very
many control variables, endogenous receipt of treatment, heterogeneous
treatment effects, and function-valued outcomes. Our framework covers the
special case of exogenous receipt of treatment, either conditional on controls
or unconditionally as in randomized control trials. In the latter case, our
approach produces efficient estimators and honest bands for (functional)
average treatment effects (ATE) and quantile treatment effects (QTE). To make
informative inference possible, we assume that key reduced form predictive
relationships are approximately sparse. This assumption allows the use of
regularization and selection methods to estimate those relations, and we
provide methods for post-regularization and post-selection inference that are
uniformly valid (honest) across a wide-range of models. We show that a key
ingredient enabling honest inference is the use of orthogonal or doubly robust
moment conditions in estimating certain reduced form functional parameters. We
illustrate the use of the proposed methods with an application to estimating
the effect of 401(k) eligibility and participation on accumulated assets.
"
1,41,"  We derive fixed effects estimators of parameters and average partial effects
in (possibly dynamic) nonlinear panel data models with individual and time
effects. They cover logit, probit, ordered probit, Poisson and Tobit models
that are important for many empirical applications in micro and macroeconomics.
Our estimators use analytical and jackknife bias corrections to deal with the
incidental parameter problem, and are asymptotically unbiased under asymptotic
sequences where $N/T$ converges to a constant. We develop inference methods and
show that they perform well in numerical examples.
"
1,42,"  This paper considers identification and estimation of ceteris paribus effects
of continuous regressors in nonseparable panel models with time homogeneity.
The effects of interest are derivatives of the average and quantile structural
functions of the model. We find that these derivatives are identified with two
time periods for ""stayers"", i.e. for individuals with the same regressor values
in two time periods. We show that the identification results carry over to
models that allow location and scale time effects. We propose nonparametric
series methods and a weighted bootstrap scheme to estimate and make inference
on the identified effects. The bootstrap proposed allows uniform inference for
function-valued parameters such as quantile effects uniformly over a region of
quantile indices and/or regressor values. An empirical application to Engel
curve estimation with panel data illustrates the results.
"
1,43,"  This work proposes new inference methods for a regression coefficient of
interest in a (heterogeneous) quantile regression model. We consider a
high-dimensional model where the number of regressors potentially exceeds the
sample size but a subset of them suffice to construct a reasonable
approximation to the conditional quantile function. The proposed methods are
(explicitly or implicitly) based on orthogonal score functions that protect
against moderate model selection mistakes, which are often inevitable in the
approximately sparse model considered in the present paper. We establish the
uniform validity of the proposed confidence regions for the quantile regression
coefficient. Importantly, these methods directly apply to more than one
variable and a continuum of quantile indices. In addition, the performance of
the proposed methods is illustrated through Monte-Carlo experiments and an
empirical example, dealing with risk factors in childhood malnutrition.
"
1,44,"  Biondi et al. (2012) develop an analytical model to examine the emergent
dynamic properties of share market price formation over time, capable to
capture important stylized facts. These latter properties prove to be sensitive
to regulatory regimes for fundamental information provision, as well as to
market confidence conditions among actual and potential investors. Regimes
based upon mark-to-market (fair value) measurement of traded security, while
generating higher linear correlation between market prices and fundamental
signals, also involve higher market instability and volatility. These regimes
also incur more relevant episodes of market exuberance and vagary in some
regions of the market confidence space, where lower market liquidity further
occurs.
"
1,45,"  This paper considers the problem of testing many moment inequalities where
the number of moment inequalities, denoted by $p$, is possibly much larger than
the sample size $n$. There is a variety of economic applications where solving
this problem allows to carry out inference on causal and structural parameters,
a notable example is the market structure model of Ciliberto and Tamer (2009)
where $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter
the market. We consider the test statistic given by the maximum of $p$
Studentized (or $t$-type) inequality-specific statistics, and analyze various
ways to compute critical values for the test statistic. Specifically, we
consider critical values based upon (i) the union bound combined with a
moderate deviation inequality for self-normalized sums, (ii) the multiplier and
empirical bootstraps, and (iii) two-step and three-step variants of (i) and
(ii) by incorporating the selection of uninformative inequalities that are far
from being binding and a novel selection of weakly informative inequalities
that are potentially binding but do not provide first order information. We
prove validity of these methods, showing that under mild conditions, they lead
to tests with the error in size decreasing polynomially in $n$ while allowing
for $p$ being much larger than $n$, indeed $p$ can be of order $\exp (n^{c})$
for some $c > 0$. Importantly, all these results hold without any restriction
on the correlation structure between $p$ Studentized statistics, and also hold
uniformly with respect to suitably large classes of underlying distributions.
Moreover, in the online supplement, we show validity of a test based on the
block multiplier bootstrap in the case of dependent data under some general
mixing conditions.
"
1,46,"  The increasing importance of renewable energy, especially solar and wind
power, has led to new forces in the formation of electricity prices. Hence,
this paper introduces an econometric model for the hourly time series of
electricity prices of the European Power Exchange (EPEX) which incorporates
specific features like renewable energy. The model consists of several
sophisticated and established approaches and can be regarded as a periodic
VAR-TARCH with wind power, solar power, and load as influences on the time
series. It is able to map the distinct and well-known features of electricity
prices in Germany. An efficient iteratively reweighted lasso approach is used
for the estimation. Moreover, it is shown that several existing models are
outperformed by the procedure developed in this paper.
"
1,47,"  We study the class of potential games that are also graphical games with
respect to a given graph $G$ of connections between the players. We show that,
up to strategic equivalence, this class of games can be identified with the set
of Markov random fields on $G$.
  From this characterization, and from the Hammersley-Clifford theorem, it
follows that the potentials of such games can be decomposed to local
potentials. We use this decomposition to strongly bound the number of strategy
changes of a single player along a better response path. This result extends to
generalized graphical potential games, which are played on infinite graphs.
"
1,48,"  During the Great Recession, Democrats in the United States argued that
government spending could be utilized to ""grease the wheels"" of the economy in
order to create wealth and to increase employment; Republicans, on the other
hand, contended that government spending is wasteful and discouraged
investment, thereby increasing unemployment. Today, in 2020, we find ourselves
in the midst of another crisis where government spending and fiscal stimulus is
again being considered as a solution. In the present paper, we address this
question by formulating an optimal control problem generalizing the model of
Radner & Shepp (1996). The model allows for the company to borrow continuously
from the government. We prove that there exists an optimal strategy; rigorous
verification proofs for its optimality are provided. We proceed to prove that
government loans increase the expected net value of a company. We also examine
the consequences of different profit-taking behaviors among firms who receive
fiscal stimulus.
"
1,49,"  This paper considers inference on functionals of semi/nonparametric
conditional moment restrictions with possibly nonsmooth generalized residuals,
which include all of the (nonlinear) nonparametric instrumental variables (IV)
as special cases. These models are often ill-posed and hence it is difficult to
verify whether a (possibly nonlinear) functional is root-$n$ estimable or not.
We provide computationally simple, unified inference procedures that are
asymptotically valid regardless of whether a functional is root-$n$ estimable
or not. We establish the following new useful results: (1) the asymptotic
normality of a plug-in penalized sieve minimum distance (PSMD) estimator of a
(possibly nonlinear) functional; (2) the consistency of simple sieve variance
estimators for the plug-in PSMD estimator, and hence the asymptotic chi-square
distribution of the sieve Wald statistic; (3) the asymptotic chi-square
distribution of an optimally weighted sieve quasi likelihood ratio (QLR) test
under the null hypothesis; (4) the asymptotic tight distribution of a
non-optimally weighted sieve QLR statistic under the null; (5) the consistency
of generalized residual bootstrap sieve Wald and QLR tests; (6) local power
properties of sieve Wald and QLR tests and of their bootstrap versions; (7)
asymptotic properties of sieve Wald and SQLR for functionals of increasing
dimension. Simulation studies and an empirical illustration of a nonparametric
quantile IV regression are presented.
"
1,50,"  This paper establishes consistency of the weighted bootstrap for quadratic
forms $\left( n^{-1/2} \sum_{i=1}^{n} Z_{i,n} \right)^{T}\left( n^{-1/2}
\sum_{i=1}^{n} Z_{i,n} \right)$ where $(Z_{i,n})_{i=1}^{n}$ are mean zero,
independent $\mathbb{R}^{d}$-valued random variables and $d=d(n)$ is allowed to
grow with the sample size $n$, slower than $n^{1/4}$. The proof relies on an
adaptation of Lindeberg interpolation technique whereby we simplify the
original problem to a Gaussian approximation problem. We apply our bootstrap
results to model-specification testing problems when the number of moments is
allowed to grow with the sample size.
"
1,51,"  We consider estimation and inference in panel data models with additive
unobserved individual specific heterogeneity in a high dimensional setting. The
setting allows the number of time varying regressors to be larger than the
sample size. To make informative estimation and inference feasible, we require
that the overall contribution of the time varying variables after eliminating
the individual specific heterogeneity can be captured by a relatively small
number of the available variables whose identities are unknown. This
restriction allows the problem of estimation to proceed as a variable selection
problem. Importantly, we treat the individual specific heterogeneity as fixed
effects which allows this heterogeneity to be related to the observed time
varying variables in an unspecified way and allows that this heterogeneity may
be non-zero for all individuals. Within this framework, we provide procedures
that give uniformly valid inference over a fixed subset of parameters in the
canonical linear fixed effects model and over coefficients on a fixed vector of
endogenous variables in panel data instrumental variables models with fixed
effects and many instruments. An input to developing the properties of our
proposed procedures is the use of a variant of the Lasso estimator that allows
for a grouped data structure where data across groups are independent and
dependence within groups is unrestricted. We provide formal conditions within
this structure under which the proposed Lasso variant selects a sparse model
with good approximation properties. We present simulation results in support of
the theoretical developments and illustrate the use of the methods in an
application aimed at estimating the effect of gun prevalence on crime rates.
"
1,52,"  Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman
filter, transfer-function and intervention models, unit root tests,
cointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M,
Taylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly
time series of GDP and Government Consumption Expenditures & Gross Investment
(GCEGI) from 1980 to 2013. The article is organized as: (I) Definition; (II)
Regression Models; (III) Discussion. Additionally, I discovered a unique
interaction between GDP and GCEGI in both the short-run and the long-run and
provided policy makers with some suggestions. For example in the short run, GDP
responded positively and very significantly (0.00248) to GCEGI, while GCEGI
reacted positively but not too significantly (0.08051) to GDP. In the long run,
current GDP responded negatively and permanently (0.09229) to a shock in past
GCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a
shock in past GDP. Therefore, policy makers should not adjust current GCEGI
based merely on the condition of current and past GDP. Although increasing
GCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI
might not be good to the long-term health of GDP. Instead, a balanced,
sustainable, and economically viable solution is recommended, so that the
short-term benefits to the current economy from increasing GCEGI often largely
secured by the long-term loan outweigh or at least equal to the negative effect
to the future economy from the long-term debt incurred by the loan. Finally, I
found that non-normally distributed volatility models generally perform better
than normally distributed ones. More specifically, TARCH-GED performs the best
in the group of non-normally distributed, while GARCH-M does the best in the
group of normally distributed.
"
1,53,"  Factor structures or interactive effects are convenient devices to
incorporate latent variables in panel data models. We consider fixed effect
estimation of nonlinear panel single-index models with factor structures in the
unobservables, which include logit, probit, ordered probit and Poisson
specifications. We establish that fixed effect estimators of model parameters
and average partial effects have normal distributions when the two dimensions
of the panel grow large, but might suffer of incidental parameter bias. We show
how models with factor structures can also be applied to capture important
features of network data such as reciprocity, degree heterogeneity, homophily
in latent variables and clustering. We illustrate this applicability with an
empirical example to the estimation of a gravity equation of international
trade between countries using a Poisson model with multiple factors.
"
1,54,"  We study how long-lived rational agents learn from repeatedly observing a
private signal and each others' actions. With normal signals, a group of any
size learns more slowly than just four agents who directly observe each others'
private signals in each period. Similar results apply to general signal
structures. We identify rational groupthink---in which agents ignore their
private signals and choose the same action for long periods of time---as the
cause of this failure of information aggregation.
"
1,55,"  We propose new concepts of statistical depth, multivariate quantiles, ranks
and signs, based on canonical transportation maps between a distribution of
interest on $R^d$ and a reference distribution on the $d$-dimensional unit
ball. The new depth concept, called Monge-Kantorovich depth, specializes to
halfspace depth in the case of spherical distributions, but, for more general
distributions, differs from the latter in the ability for its contours to
account for non convex features of the distribution of interest. We propose
empirical counterparts to the population versions of those Monge-Kantorovich
depth contours, quantiles, ranks and signs, and show their consistency by
establishing a uniform convergence property for empirical transport maps, which
is of independent interest.
"
1,56,"  In our paper we analyze the relationship between the day-ahead electricity
price of the Energy Exchange Austria (EXAA) and other day-ahead electricity
prices in Europe. We focus on markets, which settle their prices after the
EXAA, which enables traders to include the EXAA price into their calculations.
For each market we employ econometric models to incorporate the EXAA price and
compare them with their counterparts without the price of the Austrian
exchange. By employing a forecasting study, we find that electricity price
models can be improved when EXAA prices are considered.
"
1,57,"  The frequentist method of simulated minimum distance (SMD) is widely used in
economics to estimate complex models with an intractable likelihood. In other
disciplines, a Bayesian approach known as Approximate Bayesian Computation
(ABC) is far more popular. This paper connects these two seemingly related
approaches to likelihood-free estimation by means of a Reverse Sampler that
uses both optimization and importance weighting to target the posterior
distribution. Its hybrid features enable us to analyze an ABC estimate from the
perspective of SMD. We show that an ideal ABC estimate can be obtained as a
weighted average of a sequence of SMD modes, each being the minimizer of the
deviations between the data and the model. This contrasts with the SMD, which
is the mode of the average deviations. Using stochastic expansions, we provide
a general characterization of frequentist estimators and those based on
Bayesian computations including Laplace-type estimators. Their differences are
illustrated using analytical examples and a simulation study of the dynamic
panel model.
"
1,58,"  In this note, we offer an approach to estimating causal/structural parameters
in the presence of many instruments and controls based on methods for
estimating sparse high-dimensional models. We use these high-dimensional
methods to select both which instruments and which control variables to use.
The approach we take extends BCCH2012, which covers selection of instruments
for IV models with a small number of controls, and extends BCH2014, which
covers selection of controls in models where the variable of interest is
exogenous conditional on observables, to accommodate both a large number of
controls and a large number of instruments. We illustrate the approach with a
simulation and an empirical example. Technical supporting material is available
in a supplementary online appendix.
"
1,59,"  Here we present an expository, general analysis of valid post-selection or
post-regularization inference about a low-dimensional target parameter,
$\alpha$, in the presence of a very high-dimensional nuisance parameter,
$\eta$, which is estimated using modern selection or regularization methods.
Our analysis relies on high-level, easy-to-interpret conditions that allow one
to clearly see the structures needed for achieving valid post-regularization
inference. Simple, readily verifiable sufficient conditions are provided for a
class of affine-quadratic models. We focus our discussion on estimation and
inference procedures based on using the empirical analog of theoretical
equations $$M(\alpha, \eta)=0$$ which identify $\alpha$. Within this structure,
we show that setting up such equations in a manner such that the
orthogonality/immunization condition $$\partial_\eta M(\alpha, \eta) = 0$$ at
the true parameter values is satisfied, coupled with plausible conditions on
the smoothness of $M$ and the quality of the estimator $\hat \eta$, guarantees
that inference on for the main parameter $\alpha$ based on testing or point
estimation methods discussed below will be regular despite selection or
regularization biases occurring in estimation of $\eta$. In particular, the
estimator of $\alpha$ will often be uniformly consistent at the root-$n$ rate
and uniformly asymptotically normal even though estimators $\hat \eta$ will
generally not be asymptotically linear and regular. The uniformity holds over
large classes of models that do not impose highly implausible ""beta-min""
conditions. We also show that inference can be carried out by inverting tests
formed from Neyman's $C(\alpha)$ (orthogonal score) statistics.
"
1,60,"  Common high-dimensional methods for prediction rely on having either a sparse
signal model, a model in which most parameters are zero and there are a small
number of non-zero parameters that are large in magnitude, or a dense signal
model, a model with no large parameters and very many small non-zero
parameters. We consider a generalization of these two basic models, termed here
a ""sparse+dense"" model, in which the signal is given by the sum of a sparse
signal and a dense signal. Such a structure poses problems for traditional
sparse estimators, such as the lasso, and for traditional dense estimation
methods, such as ridge estimation. We propose a new penalization-based method,
called lava, which is computationally efficient. With suitable choices of
penalty parameters, the proposed method strictly dominates both lasso and
ridge. We derive analytic expressions for the finite-sample risk function of
the lava estimator in the Gaussian sequence model. We also provide an deviation
bound for the prediction risk in the Gaussian regression model with fixed
design. In both cases, we provide Stein's unbiased estimator for lava's
prediction risk. A simulation example compares the performance of lava to
lasso, ridge, and elastic net in a regression example using feasible,
data-dependent penalty parameters and illustrates lava's improved performance
relative to these benchmarks.
"
1,61,"  We study Markov decision problems where the agent does not know the
transition probability function mapping current states and actions to future
states. The agent has a prior belief over a set of possible transition
functions and updates beliefs using Bayes' rule. We allow her to be
misspecified in the sense that the true transition probability function is not
in the support of her prior. This problem is relevant in many economic settings
but is usually not amenable to analysis by the researcher. We make the problem
tractable by studying asymptotic behavior. We propose an equilibrium notion and
provide conditions under which it characterizes steady state behavior. In the
special case where the problem is static, equilibrium coincides with the
single-agent version of Berk-Nash equilibrium (Esponda and Pouzo (2016)). We
also discuss subtle issues that arise exclusively in dynamic settings due to
the possibility of a negative value of experimentation.
"
1,62,"  In this paper we study the problems of estimating heterogeneity in causal
effects in experimental or observational studies and conducting inference about
the magnitude of the differences in treatment effects across subsets of the
population. In applications, our method provides a data-driven approach to
determine which subpopulations have large or small treatment effects and to
test hypotheses about the differences in these effects. For experiments, our
method allows researchers to identify heterogeneity in treatment effects that
was not specified in a pre-analysis plan, without concern about invalidating
inference due to multiple testing. In most of the literature on supervised
machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal
is to build a model of the relationship between a unit's attributes and an
observed outcome. A prominent role in these methods is played by
cross-validation which compares predictions to actual outcomes in test samples,
in order to select the level of complexity of the model that provides the best
predictive power. Our method is closely related, but it differs in that it is
tailored for predicting causal effects of a treatment rather than a unit's
outcome. The challenge is that the ""ground truth"" for a causal effect is not
observed for any individual unit: we observe the unit with the treatment, or
without the treatment, but not both at the same time. Thus, it is not obvious
how to use cross-validation to determine whether a causal effect has been
accurately predicted. We propose several novel cross-validation criteria for
this problem and demonstrate through simulations the conditions under which
they perform better than standard methods for the problem of causal effects. We
then apply the method to a large-scale field experiment re-ranking results on a
search engine.
"
1,63,"  Non-standard distributional approximations have received considerable
attention in recent years. They often provide more accurate approximations in
small samples, and theoretical improvements in some cases. This paper shows
that the seemingly unrelated ""many instruments asymptotics"" and ""small
bandwidth asymptotics"" share a common structure, where the object determining
the limiting distribution is a V-statistic with a remainder that is an
asymptotically normal degenerate U-statistic. We illustrate how this general
structure can be used to derive new results by obtaining a new asymptotic
distribution of a series estimator of the partially linear model when the
number of terms in the series approximation possibly grows as fast as the
sample size, which we call ""many terms asymptotics"".
"
1,64,"  Optimal behavior in (competitive) situation is traditionally determined with
the help of utility functions that measure the payoff of different actions.
Given an ordering on the space of revenues (payoffs), the classical axiomatic
approach of von Neumann and Morgenstern establishes the existence of suitable
utility functions, and yields to game-theory as the most prominent
materialization of a theory to determine optimal behavior. Although this
appears to be a most natural approach to risk management too, applications in
critical infrastructures often violate the implicit assumption of actions
leading to deterministic consequences. In that sense, the gameplay in a
critical infrastructure risk control competition is intrinsically random in the
sense of actions having uncertain consequences. Mathematically, this takes us
to utility functions that are probability-distribution-valued, in which case we
loose the canonic (in fact every possible) ordering on the space of payoffs,
and the original techniques of von Neumann and Morgenstern no longer apply.
  This work introduces a new kind of game in which uncertainty applies to the
payoff functions rather than the player's actions (a setting that has been
widely studied in the literature, yielding to celebrated notions like the
trembling hands equilibrium or the purification theorem). In detail, we show
how to fix the non-existence of a (canonic) ordering on the space of
probability distributions by only mildly restricting the full set to a subset
that can be totally ordered. Our vehicle to define the ordering and establish
basic game-theory is non-standard analysis and hyperreal numbers.
"
1,65,"  The linear regression model is widely used in empirical work in Economics,
Statistics, and many other disciplines. Researchers often include many
covariates in their linear model specification in an attempt to control for
confounders. We give inference methods that allow for many covariates and
heteroskedasticity. Our results are obtained using high-dimensional
approximations, where the number of included covariates are allowed to grow as
fast as the sample size. We find that all of the usual versions of Eicker-White
heteroskedasticity consistent standard error estimators for linear models are
inconsistent under this asymptotics. We then propose a new heteroskedasticity
consistent standard error formula that is fully automatic and robust to both
(conditional)\ heteroskedasticity of unknown form and the inclusion of possibly
many covariates. We apply our findings to three settings: parametric linear
models with many covariates, linear panel models with many fixed effects, and
semiparametric semi-linear models with many technical regressors. Simulation
evidence consistent with our theoretical results is also provided. The proposed
methods are also illustrated with an empirical application.
"
1,66,"  The ill-posedness of the inverse problem of recovering a regression function
in a nonparametric instrumental variable model leads to estimators that may
suffer from a very slow, logarithmic rate of convergence. In this paper, we
show that restricting the problem to models with monotone regression functions
and monotone instruments significantly weakens the ill-posedness of the
problem. In stark contrast to the existing literature, the presence of a
monotone instrument implies boundedness of our measure of ill-posedness when
restricted to the space of monotone functions. Based on this result we derive a
novel non-asymptotic error bound for the constrained estimator that imposes
monotonicity of the regression function. For a given sample size, the bound is
independent of the degree of ill-posedness as long as the regression function
is not too steep. As an implication, the bound allows us to show that the
constrained estimator converges at a fast, polynomial rate, independently of
the degree of ill-posedness, in a large, but slowly shrinking neighborhood of
constant functions. Our simulation study demonstrates significant finite-sample
performance gains from imposing monotonicity even when the regression function
is rather far from being a constant. We apply the constrained estimator to the
problem of estimating gasoline demand functions from U.S. data.
"
1,67,"  Nonparametric methods play a central role in modern empirical work. While
they provide inference procedures that are more robust to parametric
misspecification bias, they may be quite sensitive to tuning parameter choices.
We study the effects of bias correction on confidence interval coverage in the
context of kernel density and local polynomial regression estimation, and prove
that bias correction can be preferred to undersmoothing for minimizing coverage
error and increasing robustness to tuning parameter choice. This is achieved
using a novel, yet simple, Studentization, which leads to a new way of
constructing kernel-based bias-corrected confidence intervals. In addition, for
practical cases, we derive coverage error optimal bandwidths and discuss
easy-to-implement bandwidth selectors. For interior points, we show that the
MSE-optimal bandwidth for the original point estimator (before bias correction)
delivers the fastest coverage error decay rate after bias correction when
second-order (equivalent) kernels are employed, but is otherwise suboptimal
because it is too ""large"". Finally, for odd-degree local polynomial regression,
we show that, as with point estimation, coverage error adapts to boundary
points automatically when appropriate Studentization is used; however, the
MSE-optimal bandwidth for the original point estimator is suboptimal. All the
results are established using valid Edgeworth expansions and illustrated with
simulated data. Our findings have important consequences for empirical work as
they indicate that bias-corrected confidence intervals, coupled with
appropriate standard errors, have smaller coverage error and are less sensitive
to tuning parameter choices in practically relevant cases where additional
smoothness is available.
"
1,68,"  This paper makes several important contributions to the literature about
nonparametric instrumental variables (NPIV) estimation and inference on a
structural function $h_0$ and its functionals. First, we derive sup-norm
convergence rates for computationally simple sieve NPIV (series 2SLS)
estimators of $h_0$ and its derivatives. Second, we derive a lower bound that
describes the best possible (minimax) sup-norm rates of estimating $h_0$ and
its derivatives, and show that the sieve NPIV estimator can attain the minimax
rates when $h_0$ is approximated via a spline or wavelet sieve. Our optimal
sup-norm rates surprisingly coincide with the optimal root-mean-squared rates
for severely ill-posed problems, and are only a logarithmic factor slower than
the optimal root-mean-squared rates for mildly ill-posed problems. Third, we
use our sup-norm rates to establish the uniform Gaussian process strong
approximations and the score bootstrap uniform confidence bands (UCBs) for
collections of nonlinear functionals of $h_0$ under primitive conditions,
allowing for mildly and severely ill-posed problems. Fourth, as applications,
we obtain the first asymptotic pointwise and uniform inference results for
plug-in sieve t-statistics of exact consumer surplus (CS) and deadweight loss
(DL) welfare functionals under low-level conditions when demand is estimated
via sieve NPIV. Empiricists could read our real data application of UCBs for
exact CS and DL functionals of gasoline demand that reveals interesting
patterns and is applicable to other markets.
"
1,69,"  We consider a model of matching in trading networks in which firms can enter
into bilateral contracts. In trading networks, stable outcomes, which are
immune to deviations of arbitrary sets of firms, may not exist. We define a new
solution concept called trail stability. Trail-stable outcomes are immune to
consecutive, pairwise deviations between linked firms. We show that any trading
network with bilateral contracts has a trail-stable outcome whenever firms'
choice functions satisfy the full substitutability condition. For trail-stable
outcomes, we prove results on the lattice structure, the rural hospitals
theorem, strategy-proofness, and comparative statics of firm entry and exit. We
also introduce weak trail stability which is implied by trail stability under
full substitutability. We describe relationships between the solution concepts.
"
1,70,"  The first ever human vs. computer no-limit Texas hold 'em competition took
place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this
article I present my thoughts on the competition design, agent architecture,
and lessons learned.
"
1,71,"  Oversubscribed treatments are often allocated using randomized waiting lists.
Applicants are ranked randomly, and treatment offers are made following that
ranking until all seats are filled. To estimate causal effects, researchers
often compare applicants getting and not getting an offer. We show that those
two groups are not statistically comparable. Therefore, the estimator arising
from that comparison is inconsistent. We propose a new estimator, and show that
it is consistent. Finally, we revisit an application, and we show that using
our estimator can lead to sizably different results from those obtained using
the commonly used estimator.
"
1,72,"  The problem of endogeneity in statistics and econometrics is often handled by
introducing instrumental variables (IV) which fulfill the mean independence
assumption, i.e. the unobservable is mean independent of the instruments. When
full independence of IV's and the unobservable is assumed, nonparametric IV
regression models and nonparametric demand models lead to nonlinear integral
equations with unknown integral kernels. We prove convergence rates for the
mean integrated square error of the iteratively regularized Newton method
applied to these problems. Compared to related results we derive stronger
convergence results that rely on weaker nonlinearity restrictions. We
demonstrate in numerical simulations for a nonparametric IV regression that the
method produces better results than the standard model.
"
1,73,"  The game-theoretic risk management framework put forth in the precursor work
""Towards a Theory of Games with Payoffs that are Probability-Distributions""
(arXiv:1506.07368 [q-fin.EC]) is herein extended by algorithmic details on how
to compute equilibria in games where the payoffs are probability distributions.
Our approach is ""data driven"" in the sense that we assume empirical data
(measurements, simulation, etc.) to be available that can be compiled into
distribution models, which are suitable for efficient decisions about
preferences, and setting up and solving games using these as payoffs. While
preferences among distributions turn out to be quite simple if nonparametric
methods (kernel density estimates) are used, computing Nash-equilibria in games
using such models is discovered as inefficient (if not impossible). In fact, we
give a counterexample in which fictitious play fails to converge for the
(specifically unfortunate) choice of payoff distributions in the game, and
introduce a suitable tail approximation of the payoff densities to tackle the
issue. The overall procedure is essentially a modified version of fictitious
play, and is herein described for standard and multicriteria games, to
iteratively deliver an (approximate) Nash-equilibrium. An exact method using
linear programming is also given.
"
1,74,"  The partial (ceteris paribus) effects of interest in nonlinear and
interactive linear models are heterogeneous as they can vary dramatically with
the underlying observed or unobserved covariates. Despite the apparent
importance of heterogeneity, a common practice in modern empirical work is to
largely ignore it by reporting average partial effects (or, at best, average
effects for some groups). While average effects provide very convenient scalar
summaries of typical effects, by definition they fail to reflect the entire
variety of the heterogeneous effects. In order to discover these effects much
more fully, we propose to estimate and report sorted effects -- a collection of
estimated partial effects sorted in increasing order and indexed by
percentiles. By construction the sorted effect curves completely represent and
help visualize the range of the heterogeneous effects in one plot. They are as
convenient and easy to report in practice as the conventional average partial
effects. They also serve as a basis for classification analysis, where we
divide the observational units into most or least affected groups and summarize
their characteristics. We provide a quantification of uncertainty (standard
errors and confidence bands) for the estimated sorted effects and related
classification analysis, and provide confidence sets for the most and least
affected groups. The derived statistical results rely on establishing key, new
mathematical results on Hadamard differentiability of a multivariate sorting
operator and a related classification operator, which are of independent
interest. We apply the sorted effects method and classification analysis to
demonstrate several striking patterns in the gender wage gap.
"
1,75,"  We propose a general framework for regularization in M-estimation problems
under time dependent (absolutely regular-mixing) data which encompasses many of
the existing estimators. We derive non-asymptotic concentration bounds for the
regularized M-estimator. Our results exhibit a variance-bias trade-off, with
the variance term being governed by a novel measure of the complexity of the
parameter set. We also show that the mixing structure affect the variance term
by scaling the number of observations; depending on the decay rate of the
mixing coefficients, this scaling can even affect the asymptotic behavior.
Finally, we propose a data-driven method for choosing the tuning parameters of
the regularized estimator which yield the same (up to constants) concentration
bound as one that optimally balances the (squared) bias and variance terms. We
illustrate the results with several canonical examples.
"
1,76,"  We propose a bootstrap-based calibrated projection procedure to build
confidence intervals for single components and for smooth functions of a
partially identified parameter vector in moment (in)equality models. The method
controls asymptotic coverage uniformly over a large class of data generating
processes. The extreme points of the calibrated projection confidence interval
are obtained by extremizing the value of the function of interest subject to a
proper relaxation of studentized sample analogs of the moment (in)equality
conditions. The degree of relaxation, or critical level, is calibrated so that
the function of theta, not theta itself, is uniformly asymptotically covered
with prespecified probability. This calibration is based on repeatedly checking
feasibility of linear programming problems, rendering it computationally
attractive.
  Nonetheless, the program defining an extreme point of the confidence interval
is generally nonlinear and potentially intricate. We provide an algorithm,
based on the response surface method for global optimization, that approximates
the solution rapidly and accurately, and we establish its rate of convergence.
The algorithm is of independent interest for optimization problems with simple
objectives and complicated constraints. An empirical application estimating an
entry game illustrates the usefulness of the method. Monte Carlo simulations
confirm the accuracy of the solution algorithm, the good statistical as well as
computational performance of calibrated projection (including in comparison to
other methods), and the algorithm's potential to greatly accelerate computation
of other confidence intervals.
"
1,77,"  In this paper, we propose a doubly robust method to present the heterogeneity
of the average treatment effect with respect to observed covariates of
interest. We consider a situation where a large number of covariates are needed
for identifying the average treatment effect but the covariates of interest for
analyzing heterogeneity are of much lower dimension. Our proposed estimator is
doubly robust and avoids the curse of dimensionality. We propose a uniform
confidence band that is easy to compute, and we illustrate its usefulness via
Monte Carlo experiments and an application to the effects of smoking on birth
weights.
"
1,78,"  We discuss efficient Bayesian estimation of dynamic covariance matrices in
multivariate time series through a factor stochastic volatility model. In
particular, we propose two interweaving strategies (Yu and Meng, Journal of
Computational and Graphical Statistics, 20(3), 531-570, 2011) to substantially
accelerate convergence and mixing of standard MCMC approaches. Similar to
marginal data augmentation techniques, the proposed acceleration procedures
exploit non-identifiability issues which frequently arise in factor models. Our
new interweaving strategies are easy to implement and come at almost no extra
computational cost; nevertheless, they can boost estimation efficiency by
several orders of magnitude as is shown in extensive simulation studies. To
conclude, the application of our algorithm to a 26-dimensional exchange rate
data set illustrates the superior performance of the new approach for
real-world data.
"
1,79,"  Boosting is one of the most significant developments in machine learning.
This paper studies the rate of convergence of $L_2$Boosting, which is tailored
for regression, in a high-dimensional setting. Moreover, we introduce so-called
\textquotedblleft post-Boosting\textquotedblright. This is a post-selection
estimator which applies ordinary least squares to the variables selected in the
first stage by $L_2$Boosting. Another variant is \textquotedblleft Orthogonal
Boosting\textquotedblright\ where after each step an orthogonal projection is
conducted. We show that both post-$L_2$Boosting and the orthogonal boosting
achieve the same rate of convergence as LASSO in a sparse, high-dimensional
setting. We show that the rate of convergence of the classical $L_2$Boosting
depends on the design matrix described by a sparse eigenvalue constant. To show
the latter results, we derive new approximation results for the pure greedy
algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also
introduce feasible rules for early stopping, which can be easily implemented
and used in applied work. Our results also allow a direct comparison between
LASSO and boosting which has been missing from the literature. Finally, we
present simulation studies and applications to illustrate the relevance of our
theoretical results and to provide insights into the practical aspects of
boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms
LASSO.
"
1,80,"  In this paper, we consider a high-dimensional quantile regression model where
the sparsity structure may differ between two sub-populations. We develop
$\ell_1$-penalized estimators of both regression coefficients and the threshold
parameter. Our penalized estimators not only select covariates but also
discriminate between a model with homogeneous sparsity and a model with a
change point. As a result, it is not necessary to know or pretest whether the
change point is present, or where it occurs. Our estimator of the change point
achieves an oracle property in the sense that its asymptotic distribution is
the same as if the unknown active sets of regression coefficients were known.
Importantly, we establish this oracle property without a perfect covariate
selection, thereby avoiding the need for the minimum level condition on the
signals of active covariates. Dealing with high-dimensional quantile regression
with an unknown change point calls for a new proof technique since the quantile
loss function is non-smooth and furthermore the corresponding objective
function is non-convex with respect to the change point. The technique
developed in this paper is applicable to a general M-estimation framework with
a change point, which may be of independent interest. The proposed methods are
then illustrated via Monte Carlo experiments and an application to tipping in
the dynamics of racial segregation.
"
1,81,"  Behavior initiation is a form of leadership and is an important aspect of
social organization that affects the processes of group formation, dynamics,
and decision-making in human societies and other social animal species. In this
work, we formalize the ""Coordination Initiator Inference Problem"" and propose a
simple yet powerful framework for extracting periods of coordinated activity
and determining individuals who initiated this coordination, based solely on
the activity of individuals within a group during those periods. The proposed
approach, given arbitrary individual time series, automatically (1) identifies
times of coordinated group activity, (2) determines the identities of
initiators of those activities, and (3) classifies the likely mechanism by
which the group coordination occurred, all of which are novel computational
tasks. We demonstrate our framework on both simulated and real-world data:
trajectories tracking of animals as well as stock market data. Our method is
competitive with existing global leadership inference methods but provides the
first approaches for local leadership and coordination mechanism
classification. Our results are consistent with ground-truthed biological data
and the framework finds many known events in financial data which are not
otherwise reflected in the aggregate NASDAQ index. Our method is easily
generalizable to any coordinated time-series data from interacting entities.
"
1,82,"  The package High-dimensional Metrics (\Rpackage{hdm}) is an evolving
collection of statistical methods for estimation and quantification of
uncertainty in high-dimensional approximately sparse models. It focuses on
providing confidence intervals and significance testing for (possibly many)
low-dimensional subcomponents of the high-dimensional parameter vector.
Efficient estimators and uniformly valid confidence intervals for regression
coefficients on target variables (e.g., treatment or policy variable) in a
high-dimensional approximately sparse regression model, for average treatment
effect (ATE) and average treatment effect for the treated (ATET), as well for
extensions of these parameters to the endogenous setting are provided. Theory
grounded, data-driven methods for selecting the penalization parameter in Lasso
regressions under heteroscedastic and non-Gaussian errors are implemented.
Moreover, joint/ simultaneous confidence intervals for regression coefficients
of a high-dimensional sparse regression are implemented, including a joint
significance test for Lasso regression. Data sets which have been used in the
literature and might be useful for classroom demonstration and for testing new
estimators are included. \R and the package \Rpackage{hdm} are open-source
software projects and can be freely downloaded from CRAN:
\texttt{http://cran.r-project.org}.
"
1,83,"  In a randomized control trial, the precision of an average treatment effect
estimator can be improved either by collecting data on additional individuals,
or by collecting additional covariates that predict the outcome variable. We
propose the use of pre-experimental data such as a census, or a household
survey, to inform the choice of both the sample size and the covariates to be
collected. Our procedure seeks to minimize the resulting average treatment
effect estimator's mean squared error, subject to the researcher's budget
constraint. We rely on a modification of an orthogonal greedy algorithm that is
conceptually simple and easy to implement in the presence of a large number of
potential covariates, and does not require any tuning parameters. In two
empirical applications, we show that our procedure can lead to substantial
gains of up to 58%, measured either in terms of reductions in data collection
costs or in terms of improvements in the precision of the treatment effect
estimator.
"
1,84,"  We study factor models augmented by observed covariates that have explanatory
powers on the unknown factors. In financial factor models, the unknown factors
can be reasonably well explained by a few observable proxies, such as the
Fama-French factors. In diffusion index forecasts, identified factors are
strongly related to several directly measurable economic variables such as
consumption-wealth variable, financial ratios, and term spread. With those
covariates, both the factors and loadings are identifiable up to a rotation
matrix even only with a finite dimension. To incorporate the explanatory power
of these covariates, we propose a smoothed principal component analysis (PCA):
(i) regress the data onto the observed covariates, and (ii) take the principal
components of the fitted data to estimate the loadings and factors. This allows
us to accurately estimate the percentage of both explained and unexplained
components in factors and thus to assess the explanatory power of covariates.
We show that both the estimated factors and loadings can be estimated with
improved rates of convergence compared to the benchmark method. The degree of
improvement depends on the strength of the signals, representing the
explanatory power of the covariates on the factors. The proposed estimator is
robust to possibly heavy-tailed distributions. We apply the model to forecast
US bond risk premia, and find that the observed macroeconomic characteristics
contain strong explanatory powers of the factors. The gain of forecast is more
substantial when the characteristics are incorporated to estimate the common
factors than directly used for forecasts.
"
1,85,"  Estimating the long-term effects of treatments is of interest in many fields.
A common challenge in estimating such treatment effects is that long-term
outcomes are unobserved in the time frame needed to make policy decisions. One
approach to overcome this missing data problem is to analyze treatments effects
on an intermediate outcome, often called a statistical surrogate, if it
satisfies the condition that treatment and outcome are independent conditional
on the statistical surrogate. The validity of the surrogacy condition is often
controversial. Here we exploit that fact that in modern datasets, researchers
often observe a large number, possibly hundreds or thousands, of intermediate
outcomes, thought to lie on or close to the causal chain between the treatment
and the long-term outcome of interest. Even if none of the individual proxies
satisfies the statistical surrogacy criterion by itself, using multiple proxies
can be useful in causal inference. We focus primarily on a setting with two
samples, an experimental sample containing data about the treatment indicator
and the surrogates and an observational sample containing information about the
surrogates and the primary outcome. We state assumptions under which the
average treatment effect be identified and estimated with a high-dimensional
vector of proxies that collectively satisfy the surrogacy assumption, and
derive the bias from violations of the surrogacy assumption, and show that even
if the primary outcome is also observed in the experimental sample, there is
still information to be gained from using surrogates.
"
1,86,"  In this paper, a mathematical model based on the one-parameter Mittag-Leffler
function is proposed to be used for the first time to describe the relation
between unemployment rate and inflation rate, also known as the Phillips curve.
The Phillips curve is in the literature often represented by an
exponential-like shape. On the other hand, Phillips in his fundamental paper
used a power function in the model definition. Considering that the ordinary as
well as generalised Mittag-Leffler function behaves between a purely
exponential function and a power function it is natural to implement it in the
definition of the model used to describe the relation between the data
representing the Phillips curve. For the modelling purposes the data of two
different European economies, France and Switzerland, were used and an
""out-of-sample"" forecast was done to compare the performance of the
Mittag-Leffler model to the performance of the power-type and exponential-type
model. The results demonstrate that the ability of the Mittag-Leffler function
to fit data that manifest signs of stretched exponentials, oscillations or even
damped oscillations can be of use when describing economic relations and
phenomenons, such as the Phillips curve.
"
1,87,"  In a unified framework, we provide estimators and confidence bands for a
variety of treatment effects when the outcome of interest, typically a
duration, is subjected to right censoring. Our methodology accommodates
average, distributional, and quantile treatment effects under different
identifying assumptions including unconfoundedness, local treatment effects,
and nonlinear differences-in-differences. The proposed estimators are easy to
implement, have close-form representation, are fully data-driven upon
estimation of nuisance parameters, and do not rely on parametric distributional
assumptions, shape restrictions, or on restricting the potential treatment
effect heterogeneity across different subpopulations. These treatment effects
results are obtained as a consequence of more general results on two-step
Kaplan-Meier estimators that are of independent interest: we provide conditions
for applying (i) uniform law of large numbers, (ii) functional central limit
theorems, and (iii) we prove the validity of the ordinary nonparametric
bootstrap in a two-step estimation procedure where the outcome of interest may
be randomly censored.
"
1,88,"  There are many settings where researchers are interested in estimating
average treatment effects and are willing to rely on the unconfoundedness
assumption, which requires that the treatment assignment be as good as random
conditional on pre-treatment variables. The unconfoundedness assumption is
often more plausible if a large number of pre-treatment variables are included
in the analysis, but this can worsen the performance of standard approaches to
treatment effect estimation. In this paper, we develop a method for de-biasing
penalized regression adjustments to allow sparse regression methods like the
lasso to be used for sqrt{n}-consistent inference of average treatment effects
in high-dimensional linear models. Given linearity, we do not need to assume
that the treatment propensities are estimable, or that the average treatment
effect is a sparse contrast of the outcome model parameters. Rather, in
addition standard assumptions used to make lasso regression on the outcome
model consistent under 1-norm error, we only require overlap, i.e., that the
propensity score be uniformly bounded away from 0 and 1. Procedurally, our
method combines balancing weights with a regularized regression adjustment.
"
1,89,"  In complicated/nonlinear parametric models, it is generally hard to know
whether the model parameters are point identified. We provide computationally
attractive procedures to construct confidence sets (CSs) for identified sets of
full parameters and of subvectors in models defined through a likelihood or a
vector of moment equalities or inequalities. These CSs are based on level sets
of optimal sample criterion functions (such as likelihood or optimally-weighted
or continuously-updated GMM criterions). The level sets are constructed using
cutoffs that are computed via Monte Carlo (MC) simulations directly from the
quasi-posterior distributions of the criterions. We establish new Bernstein-von
Mises (or Bayesian Wilks) type theorems for the quasi-posterior distributions
of the quasi-likelihood ratio (QLR) and profile QLR in partially-identified
regular models and some non-regular models. These results imply that our MC CSs
have exact asymptotic frequentist coverage for identified sets of full
parameters and of subvectors in partially-identified regular models, and have
valid but potentially conservative coverage in models with reduced-form
parameters on the boundary. Our MC CSs for identified sets of subvectors are
shown to have exact asymptotic coverage in models with singularities. We also
provide results on uniform validity of our CSs over classes of DGPs that
include point and partially identified models. We demonstrate good
finite-sample coverage properties of our procedures in two simulation
experiments. Finally, our procedures are applied to two non-trivial empirical
examples: an airline entry game and a model of trade flows.
"
1,90,"  By borrowing methods from complex system analysis, in this paper we analyze
the features of the complex relationship that links the development and the
industrialization of a country to economic inequality. In order to do this, we
identify industrialization as a combination of a monetary index, the GDP per
capita, and a recently introduced measure of the complexity of an economy, the
Fitness. At first we explore these relations on a global scale over the time
period 1990--2008 focusing on two different dimensions of inequality: the
capital share of income and a Theil measure of wage inequality. In both cases,
the movement of inequality follows a pattern similar to the one theorized by
Kuznets in the fifties. We then narrow down the object of study ad we
concentrate on wage inequality within the United States. By employing data on
wages and employment on the approximately 3100 US counties for the time
interval 1990--2014, we generalize the Fitness-Complexity algorithm for
counties and NAICS sectors, and we investigate wage inequality between
industrial sectors within counties. At this scale, in the early nineties we
recover a behavior similar to the global one. While, in more recent years, we
uncover a trend reversal: wage inequality monotonically increases as
industrialization levels grow. Hence at a county level, at net of the social
and institutional factors that differ among countries, we not only observe an
upturn in inequality but also a change in the structure of the relation between
wage inequality and development.
"
1,91,"  The issue addressed in this paper is that of testing for common breaks across
or within equations of a multivariate system. Our framework is very general and
allows integrated regressors and trends as well as stationary regressors. The
null hypothesis is that breaks in different parameters occur at common
locations and are separated by some positive fraction of the sample size unless
they occur across different equations. Under the alternative hypothesis, the
break dates across parameters are not the same and also need not be separated
by a positive fraction of the sample size whether within or across equations.
The test considered is the quasi-likelihood ratio test assuming normal errors,
though as usual the limit distribution of the test remains valid with
non-normal errors. Of independent interest, we provide results about the rate
of convergence of the estimates when searching over all possible partitions
subject only to the requirement that each regime contains at least as many
observations as some positive fraction of the sample size, allowing break dates
not separated by a positive fraction of the sample size across equations.
Simulations show that the test has good finite sample properties. We also
provide an application to issues related to level shifts and persistence for
various measures of inflation to illustrate its usefulness.
"
1,92,"  For many application areas A/B testing, which partitions users of a system
into an A (control) and B (treatment) group to experiment between several
application designs, enables Internet companies to optimize their services to
the behavioral patterns of their users. Unfortunately, the A/B testing
framework cannot be applied in a straightforward manner to applications like
auctions where the users (a.k.a., bidders) submit bids before the partitioning
into the A and B groups is made. This paper combines auction theoretic modeling
with the A/B testing framework to develop methodology for A/B testing auctions.
The accuracy of our method %, assuming the auction is directly comparable to
ideal A/B testing where there is no interference between A and B. Our results
are based on an extension and improved analysis of the inference method of
Chawla et al. (2014).
"
1,93,"  This paper develops and implements a nonparametric test of Random Utility
Models. The motivating application is to test the null hypothesis that a sample
of cross-sectional demand distributions was generated by a population of
rational consumers. We test a necessary and sufficient condition for this that
does not rely on any restriction on unobserved heterogeneity or the number of
goods. We also propose and implement a control function approach to account for
endogenous expenditure. An econometric result of independent interest is a test
for linear inequality constraints when these are represented as the vertices of
a polyhedron rather than its faces. An empirical application to the U.K.
Household Expenditure Survey illustrates computational feasibility of the
method in demand problems with 5 goods.
"
1,94,"  We propose two types of Quantile Graphical Models (QGMs) --- Conditional
Independence Quantile Graphical Models (CIQGMs) and Prediction Quantile
Graphical Models (PQGMs). CIQGMs characterize the conditional independence of
distributions by evaluating the distributional dependence structure at each
quantile index. As such, CIQGMs can be used for validation of the graph
structure in the causal graphical models (\cite{pearl2009causality,
robins1986new, heckman2015causal}). One main advantage of these models is that
we can apply them to large collections of variables driven by non-Gaussian and
non-separable shocks. PQGMs characterize the statistical dependencies through
the graphs of the best linear predictors under asymmetric loss functions. PQGMs
make weaker assumptions than CIQGMs as they allow for misspecification. Because
of QGMs' ability to handle large collections of variables and focus on specific
parts of the distributions, we could apply them to quantify tail
interdependence. The resulting tail risk network can be used for measuring
systemic risk contributions that help make inroads in understanding
international financial contagion and dependence structures of returns under
downside market movements.
  We develop estimation and inference methods for QGMs focusing on the
high-dimensional case, where the number of variables in the graph is large
compared to the number of observations. For CIQGMs, these methods and results
include valid simultaneous choices of penalty functions, uniform rates of
convergence, and confidence regions that are simultaneously valid. We also
derive analogous results for PQGMs, which include new results for penalized
quantile regressions in high-dimensional settings to handle misspecification,
many controls, and a continuum of additional conditioning events.
"
1,95,"  Bayesian and frequentist criteria are fundamentally different, but often
posterior and sampling distributions are asymptotically equivalent (e.g.,
Gaussian). For the corresponding limit experiment, we characterize the
frequentist size of a certain Bayesian hypothesis test of (possibly nonlinear)
inequalities. If the null hypothesis is that the (possibly
infinite-dimensional) parameter lies in a certain half-space, then the Bayesian
test's size is $\alpha$; if the null hypothesis is a subset of a half-space,
then size is above $\alpha$ (sometimes strictly); and in other cases, size may
be above, below, or equal to $\alpha$. Two examples illustrate our results:
testing stochastic dominance and testing curvature of a translog cost function.
"
1,96,"  In this review, we present econometric and statistical methods for analyzing
randomized experiments. For basic experiments we stress randomization-based
inference as opposed to sampling-based inference. In randomization-based
inference, uncertainty in estimates arises naturally from the random assignment
of the treatments, rather than from hypothesized sampling from a large
population. We show how this perspective relates to regression analyses for
randomized experiments. We discuss the analyses of stratified, paired, and
clustered randomized experiments, and we stress the general efficiency gains
from stratification. We also discuss complications in randomized experiments
such as non-compliance. In the presence of non-compliance we contrast
intention-to-treat analyses with instrumental variables analyses allowing for
general treatment effect heterogeneity. We consider in detail estimation and
inference for heterogeneous treatment effects in settings with (possibly many)
covariates. These methods allow researchers to explore heterogeneity by
identifying subpopulations with different treatment effects while maintaining
the ability to construct valid confidence intervals. We also discuss optimal
assignment to treatment based on covariates in such settings. Finally, we
discuss estimation and inference in experiments in settings with interactions
between units, both in general network settings and in settings where the
population is partitioned into groups with all interactions contained within
these groups.
"
1,97,"  In this paper we discuss recent developments in econometrics that we view as
important for empirical researchers working on policy evaluation questions. We
focus on three main areas, where in each case we highlight recommendations for
applied work. First, we discuss new research on identification strategies in
program evaluation, with particular focus on synthetic control methods,
regression discontinuity, external validity, and the causal interpretation of
regression methods. Second, we discuss various forms of supplementary analyses
to make the identification strategies more credible. These include placebo
analyses as well as sensitivity and robustness analyses. Third, we discuss
recent advances in machine learning methods for causal effects. These advances
include methods to adjust for differences between treated and control units in
high-dimensional settings, and methods for identifying and estimating
heterogeneous treatment effects.
"
1,98,"  This paper proposes a straightforward algorithm to carry out inference in
large time-varying parameter vector autoregressions (TVP-VARs) with mixture
innovation components for each coefficient in the system. We significantly
decrease the computational burden by approximating the latent indicators that
drive the time-variation in the coefficients with a latent threshold process
that depends on the absolute size of the shocks. The merits of our approach are
illustrated with two applications. First, we forecast the US term structure of
interest rates and demonstrate forecast gains of the proposed mixture
innovation model relative to other benchmark models. Second, we apply our
approach to US macroeconomic data and find significant evidence for
time-varying effects of a monetary policy tightening.
"
1,99,"  We report the first ex post study of the economic impact of sea level rise.
We apply two econometric approaches to estimate the past effects of sea level
rise on the economy of the USA, viz. Barro type growth regressions adjusted for
spatial patterns and a matching estimator. Unit of analysis is 3063 counties of
the USA. We fit growth regressions for 13 time periods and we estimated
numerous varieties and robustness tests for both growth regressions and
matching estimator. Although there is some evidence that sea level rise has a
positive effect on economic growth, in most specifications the estimated
effects are insignificant. We therefore conclude that there is no stable,
significant effect of sea level rise on economic growth. This finding
contradicts previous ex ante studies.
"
1,100,"  We study the problem of Bayesian learning in a dynamical system involving
strategic agents with asymmetric information. In a series of seminal papers in
the literature, this problem has been investigated under a simplifying model
where myopically selfish players appear sequentially and act once in the game,
based on private noisy observations of the system state and public observation
of past players' actions. It has been shown that there exist information
cascades where users discard their private information and mimic the action of
their predecessor. In this paper, we provide a framework for studying Bayesian
learning dynamics in a more general setting than the one described above. In
particular, our model incorporates cases where players are non-myopic and
strategically participate for the whole duration of the game, and cases where
an endogenous process selects which subset of players will act at each time
instance. The proposed framework hinges on a sequential decomposition
methodology for finding structured perfect Bayesian equilibria (PBE) of a
general class of dynamic games with asymmetric information, where user-specific
states evolve as conditionally independent Markov processes and users make
independent noisy observations of their states. Using this methodology, we
study a specific dynamic learning model where players make decisions about
public investment based on their estimates of everyone's types. We characterize
a set of informational cascades for this problem where learning stops for the
team as a whole. We show that in such cascades, all players' estimates of other
players' types freeze even though each individual player asymptotically learns
its own true type.
"
1,101,"  Many economic and causal parameters depend on nonparametric or high
dimensional first steps. We give a general construction of locally
robust/orthogonal moment functions for GMM, where moment conditions have zero
derivative with respect to first steps. We show that orthogonal moment
functions can be constructed by adding to identifying moments the nonparametric
influence function for the effect of the first step on identifying moments.
Orthogonal moments reduce model selection and regularization bias, as is very
important in many applications, especially for machine learning first steps.
  We give debiased machine learning estimators of functionals of high
dimensional conditional quantiles and of dynamic discrete choice parameters
with high dimensional state variables. We show that adding to identifying
moments the nonparametric influence function provides a general construction of
orthogonal moments, including regularity conditions, and show that the
nonparametric influence function is robust to additional unknown functions on
which it depends. We give a general approach to estimating the unknown
functions in the nonparametric influence function and use it to automatically
debias estimators of functionals of high dimensional conditional location
learners. We give a variety of new doubly robust moment equations and
characterize double robustness. We give general and simple regularity
conditions and apply these for asymptotic inference on functionals of high
dimensional regression quantiles and dynamic discrete choice parameters with
high dimensional state variables.
"
1,102,"  Most modern supervised statistical/machine learning (ML) methods are
explicitly designed to solve prediction problems very well. Achieving this goal
does not imply that these methods automatically deliver good estimators of
causal parameters. Examples of such parameters include individual regression
coefficients, average treatment effects, average lifts, and demand or supply
elasticities. In fact, estimates of such causal parameters obtained via naively
plugging ML estimators into estimating equations for such parameters can behave
very poorly due to the regularization bias. Fortunately, this regularization
bias can be removed by solving auxiliary prediction problems via ML tools.
Specifically, we can form an orthogonal score for the target low-dimensional
parameter by combining auxiliary and main ML predictions. The score is then
used to build a de-biased estimator of the target parameter which typically
will converge at the fastest possible 1/root(n) rate and be approximately
unbiased and normal, and from which valid confidence intervals for these
parameters of interest may be constructed. The resulting method thus could be
called a ""double ML"" method because it relies on estimating primary and
auxiliary predictive models. In order to avoid overfitting, our construction
also makes use of the K-fold sample splitting, which we call cross-fitting.
This allows us to use a very broad set of ML predictive methods in solving the
auxiliary and main prediction problems, such as random forest, lasso, ridge,
deep neural nets, boosted trees, as well as various hybrids and aggregators of
these methods.
"
1,103,"  In this article the package High-dimensional Metrics (\texttt{hdm}) is
introduced. It is a collection of statistical methods for estimation and
quantification of uncertainty in high-dimensional approximately sparse models.
It focuses on providing confidence intervals and significance testing for
(possibly many) low-dimensional subcomponents of the high-dimensional parameter
vector. Efficient estimators and uniformly valid confidence intervals for
regression coefficients on target variables (e.g., treatment or policy
variable) in a high-dimensional approximately sparse regression model, for
average treatment effect (ATE) and average treatment effect for the treated
(ATET), as well for extensions of these parameters to the endogenous setting
are provided. Theory grounded, data-driven methods for selecting the
penalization parameter in Lasso regressions under heteroscedastic and
non-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence
intervals for regression coefficients of a high-dimensional sparse regression
are implemented. Data sets which have been used in the literature and might be
useful for classroom demonstration and for testing new estimators are included.
"
1,104,"  This paper considers inference on fixed effects in a linear regression model
estimated from network data. An important special case of our setup is the
two-way regression model. This is a workhorse technique in the analysis of
matched data sets, such as employer-employee or student-teacher panel data. We
formalize how the structure of the network affects the accuracy with which the
fixed effects can be estimated. This allows us to derive sufficient conditions
on the network for consistent estimation and asymptotically-valid inference to
be possible. Estimation of moments is also considered. We allow for general
networks and our setup covers both the dense and sparse case. We provide
numerical results for the estimation of teacher value-added models and
regressions with occupational dummies.
"
1,105,"  Quantile and quantile effect functions are important tools for descriptive
and causal analyses due to their natural and intuitive interpretation. Existing
inference methods for these functions do not apply to discrete random
variables. This paper offers a simple, practical construction of simultaneous
confidence bands for quantile and quantile effect functions of possibly
discrete random variables. It is based on a natural transformation of
simultaneous confidence bands for distribution functions, which are readily
available for many problems. The construction is generic and does not depend on
the nature of the underlying problem. It works in conjunction with parametric,
semiparametric, and nonparametric modeling methods for observed and
counterfactual distributions, and does not depend on the sampling scheme. We
apply our method to characterize the distributional impact of insurance
coverage on health care utilization and obtain the distributional decomposition
of the racial test score gap. We find that universal insurance coverage
increases the number of doctor visits across the entire distribution, and that
the racial test score gap is small at early ages but grows with age due to
socio economic factors affecting child development especially at the top of the
distribution. These are new, interesting empirical findings that complement
previous analyses that focused on mean effects only. In both applications, the
outcomes of interest are discrete rendering existing inference methods invalid
for obtaining uniform confidence bands for observed and counterfactual quantile
functions and for their difference -- the quantile effects functions.
"
1,106,"  First-best climate policy is a uniform carbon tax which gradually rises over
time. Civil servants have complicated climate policy to expand bureaucracies,
politicians to create rents. Environmentalists have exaggerated climate change
to gain influence, other activists have joined the climate bandwagon. Opponents
to climate policy have attacked the weaknesses in climate research. The climate
debate is convoluted and polarized as a result, and climate policy complex.
Climate policy should become easier and more rational as the Paris Agreement
has shifted climate policy back towards national governments. Changing
political priorities, austerity, and a maturing bureaucracy should lead to a
more constructive climate debate.
"
1,107,"  We address the curse of dimensionality in dynamic covariance estimation by
modeling the underlying co-volatility dynamics of a time series vector through
latent time-varying stochastic factors. The use of a global-local shrinkage
prior for the elements of the factor loadings matrix pulls loadings on
superfluous factors towards zero. To demonstrate the merits of the proposed
framework, the model is applied to simulated data as well as to daily
log-returns of 300 S&P 500 members. Our approach yields precise correlation
estimates, strong implied minimum variance portfolio performance and superior
forecasting accuracy in terms of log predictive scores when compared to typical
benchmarks.
"
1,108,"  This paper studies a penalized statistical decision rule for the treatment
assignment problem. Consider the setting of a utilitarian policy maker who must
use sample data to allocate a binary treatment to members of a population,
based on their observable characteristics. We model this problem as a
statistical decision problem where the policy maker must choose a subset of the
covariate space to assign to treatment, out of a class of potential subsets. We
focus on settings in which the policy maker may want to select amongst a
collection of constrained subset classes: examples include choosing the number
of covariates over which to perform best-subset selection, and model selection
when approximating a complicated class via a sieve. We adapt and extend results
from statistical learning to develop the Penalized Welfare Maximization (PWM)
rule. We establish an oracle inequality for the regret of the PWM rule which
shows that it is able to perform model selection over the collection of
available classes. We then use this oracle inequality to derive relevant bounds
on maximum regret for PWM. An important consequence of our results is that we
are able to formalize model-selection using a ""hold-out"" procedure, where the
policy maker would first estimate various policies using half of the data, and
then select the policy which performs the best when evaluated on the other half
of the data.
"
1,109,"  The moment conditions or estimating equations for instrumental variables
quantile regression involve the discontinuous indicator function. We instead
use smoothed estimating equations (SEE), with bandwidth $h$. We show that the
mean squared error (MSE) of the vector of the SEE is minimized for some $h>0$,
leading to smaller asymptotic MSE of the estimating equations and associated
parameter estimators. The same MSE-optimal $h$ also minimizes the higher-order
type I error of a SEE-based $\chi^2$ test and increases size-adjusted power in
large samples. Computation of the SEE estimator also becomes simpler and more
reliable, especially with (more) endogenous regressors. Monte Carlo simulations
demonstrate all of these superior properties in finite samples, and we apply
our estimator to JTPA data. Smoothing the estimating equations is not just a
technical operation for establishing Edgeworth expansions and bootstrap
refinements; it also brings the real benefits of having more precise estimators
and more powerful tests. Code for the estimator, simulations, and empirical
examples is available from the first author's website.
"
1,110,"  Using and extending fractional order statistic theory, we characterize the
$O(n^{-1})$ coverage probability error of the previously proposed confidence
intervals for population quantiles using $L$-statistics as endpoints in Hutson
(1999). We derive an analytic expression for the $n^{-1}$ term, which may be
used to calibrate the nominal coverage level to get
$O\bigl(n^{-3/2}[\log(n)]^3\bigr)$ coverage error. Asymptotic power is shown to
be optimal. Using kernel smoothing, we propose a related method for
nonparametric inference on conditional quantiles. This new method compares
favorably with asymptotic normality and bootstrap methods in theory and in
simulations. Code is available from the second author's website for both
unconditional and conditional methods, simulations, and empirical examples.
"
1,111,"  We propose generalized random forests, a method for non-parametric
statistical estimation based on random forests (Breiman, 2001) that can be used
to fit any quantity of interest identified as the solution to a set of local
moment equations. Following the literature on local maximum likelihood
estimation, our method considers a weighted set of nearby training examples;
however, instead of using classical kernel weighting functions that are prone
to a strong curse of dimensionality, we use an adaptive weighting function
derived from a forest designed to express heterogeneity in the specified
quantity of interest. We propose a flexible, computationally efficient
algorithm for growing generalized random forests, develop a large sample theory
for our method showing that our estimates are consistent and asymptotically
Gaussian, and provide an estimator for their asymptotic variance that enables
valid confidence intervals. We use our approach to develop new methods for
three statistical tasks: non-parametric quantile regression, conditional
average partial effect estimation, and heterogeneous treatment effect
estimation via instrumental variables. A software implementation, grf for R and
C++, is available from CRAN.
"
1,112,"  We consider a variable selection problem for the prediction of binary
outcomes. We study the best subset selection procedure by which the covariates
are chosen by maximizing Manski (1975, 1985)'s maximum score objective function
subject to a constraint on the maximal number of selected variables. We show
that this procedure can be equivalently reformulated as solving a mixed integer
optimization problem, which enables computation of the exact or an approximate
solution with a definite approximation error bound. In terms of theoretical
results, we obtain non-asymptotic upper and lower risk bounds when the
dimension of potential covariates is possibly much larger than the sample size.
Our upper and lower risk bounds are minimax rate-optimal when the maximal
number of selected variables is fixed and does not increase with the sample
size. We illustrate usefulness of the best subset binary prediction approach
via Monte Carlo simulations and an empirical application of the work-trip
transportation mode choice.
"
1,113,"  We present the Stata commands probitfe and logitfe, which estimate probit and
logit panel data models with individual and/or time unobserved effects. Fixed
effect panel data methods that estimate the unobserved effects can be severely
biased because of the incidental parameter problem (Neyman and Scott, 1948). We
tackle this problem by using the analytical and jackknife bias corrections
derived in Fernandez-Val and Weidner (2016) for panels where the two dimensions
($N$ and $T$) are moderately large. We illustrate the commands with an
empirical application to international trade and a Monte Carlo simulation
calibrated to this application.
"
1,114,"  The Counterfactual package implements the estimation and inference methods of
Chernozhukov, Fern\'andez-Val and Melly (2013) for counterfactual analysis. The
counterfactual distributions considered are the result of changing either the
marginal distribution of covariates related to the outcome variable of
interest, or the conditional distribution of the outcome given the covariates.
They can be applied to estimate quantile treatment effects and wage
decompositions. This paper serves as an introduction to the package and
displays basic functionality of the commands contained within.
"
1,115,"  The R package quantreg.nonpar implements nonparametric quantile regression
methods to estimate and make inference on partially linear quantile models.
quantreg.nonpar obtains point estimates of the conditional quantile function
and its derivatives based on series approximations to the nonparametric part of
the model. It also provides pointwise and uniform confidence intervals over a
region of covariate values and/or quantile indices for the same functions using
analytical and resampling methods. This paper serves as an introduction to the
package and displays basic functionality of the functions contained within.
"
1,116,"  This paper develops inferential methods for a very general class of ill-posed
models in econometrics encompassing the nonparametric instrumental variable
regression, various functional regressions, and the density deconvolution. We
focus on uniform confidence sets for the parameter of interest estimated with
Tikhonov regularization, as in Darolles, Fan, Florens, and Renault (2011).
Since it is impossible to have inferential methods based on the central limit
theorem, we develop two alternative approaches relying on the concentration
inequality and bootstrap approximations. We show that expected diameters and
coverage properties of resulting sets have uniform validity over a large class
of models, i.e., constructed confidence sets are honest. Monte Carlo
experiments illustrate that introduced confidence sets have reasonable width
and coverage properties. Using U.S. data, we provide uniform confidence sets
for Engel curves for various commodities.
"
1,117,"  This paper proposes new nonparametric diagnostic tools to assess the
asymptotic validity of different treatment effects estimators that rely on the
correct specification of the propensity score. We derive a particular
restriction relating the propensity score distribution of treated and control
groups, and develop specification tests based upon it. The resulting tests do
not suffer from the ""curse of dimensionality"" when the vector of covariates is
high-dimensional, are fully data-driven, do not require tuning parameters such
as bandwidths, and are able to detect a broad class of local alternatives
converging to the null at the parametric rate $n^{-1/2}$, with $n$ the sample
size. We show that the use of an orthogonal projection on the tangent space of
nuisance parameters facilitates the simulation of critical values by means of a
multiplier bootstrap procedure, and can lead to power gains. The finite sample
performance of the tests is examined by means of a Monte Carlo experiment and
an empirical application. Open-source software is available for implementing
the proposed tests.
"
1,118,"  We consider inference about coefficients on a small number of variables of
interest in a linear panel data model with additive unobserved individual and
time specific effects and a large number of additional time-varying confounding
variables. We allow the number of these additional confounding variables to be
larger than the sample size, and suppose that, in addition to unrestricted time
and individual specific effects, these confounding variables are generated by a
small number of common factors and high-dimensional weakly-dependent
disturbances. We allow that both the factors and the disturbances are related
to the outcome variable and other variables of interest. To make informative
inference feasible, we impose that the contribution of the part of the
confounding variables not captured by time specific effects, individual
specific effects, or the common factors can be captured by a relatively small
number of terms whose identities are unknown. Within this framework, we provide
a convenient computational algorithm based on factor extraction followed by
lasso regression for inference about parameters of interest and show that the
resulting procedure has good asymptotic properties. We also provide a simple
k-step bootstrap procedure that may be used to construct inferential statements
about parameters of interest and prove its asymptotic validity. The proposed
bootstrap may be of substantive independent interest outside of the present
context as the proposed bootstrap may readily be adapted to other contexts
involving inference after lasso variable selection and the proof of its
validity requires some new technical arguments. We also provide simulation
evidence about performance of our procedure and illustrate its use in two
empirical applications.
"
1,119,"  This article proposes different tests for treatment effect heterogeneity when
the outcome of interest, typically a duration variable, may be right-censored.
The proposed tests study whether a policy 1) has zero distributional (average)
effect for all subpopulations defined by covariate values, and 2) has
homogeneous average effect across different subpopulations. The proposed tests
are based on two-step Kaplan-Meier integrals and do not rely on parametric
distributional assumptions, shape restrictions, or on restricting the potential
treatment effect heterogeneity across different subpopulations. Our framework
is suitable not only to exogenous treatment allocation but can also account for
treatment noncompliance - an important feature in many applications. The
proposed tests are consistent against fixed alternatives, and can detect
nonparametric alternatives converging to the null at the parametric
$n^{-1/2}$-rate, $n$ being the sample size. Critical values are computed with
the assistance of a multiplier bootstrap. The finite sample properties of the
proposed tests are examined by means of a Monte Carlo study and an application
about the effect of labor market programs on unemployment duration. Open-source
software is available for implementing all proposed tests.
"
1,120,"  This paper considers maximum likelihood (ML) estimation in a large class of
models with hidden Markov regimes. We investigate consistency of the ML
estimator and local asymptotic normality for the models under general
conditions which allow for autoregressive dynamics in the observable process,
Markov regime sequences with covariate-dependent transition matrices, and
possible model misspecification. A Monte Carlo study examines the finite-sample
properties of the ML estimator in correctly specified and misspecified models.
An empirical application is also discussed.
"
1,121,"  Extremal quantile regression, i.e. quantile regression applied to the tails
of the conditional distribution, counts with an increasing number of economic
and financial applications such as value-at-risk, production frontiers,
determinants of low infant birth weights, and auction models. This chapter
provides an overview of recent developments in the theory and empirics of
extremal quantile regression. The advances in the theory have relied on the use
of extreme value approximations to the law of the Koenker and Bassett (1978)
quantile regression estimator. Extreme value laws not only have been shown to
provide more accurate approximations than Gaussian laws at the tails, but also
have served as the basis to develop bias corrected estimators and inference
methods using simulation and suitable variations of bootstrap and subsampling.
The applicability of these methods is illustrated with two empirical examples
on conditional value-at-risk and financial contagion.
"
1,122,"  The Fisher Ideal index, developed to measure price inflation, is applied to
define a population-weighted temperature trend. This method has the advantages
that the trend is representative for the population distribution throughout the
sample but without conflating the trend in the population distribution and the
trend in the temperature. I show that the trend in the global area-weighted
average surface air temperature is different in key details from the
population-weighted trend. I extend the index to include urbanization and the
urban heat island effect. This substantially changes the trend again. I further
extend the index to include international migration, but this has a minor
impact on the trend.
"
1,123,"  We extend the Granger-Johansen representation theorems for I(1) and I(2)
vector autoregressive processes to accommodate processes that take values in an
arbitrary complex separable Hilbert space. This more general setting is of
central relevance for statistical applications involving functional time
series. We first obtain a range of necessary and sufficient conditions for a
pole in the inverse of a holomorphic index-zero Fredholm operator pencil to be
of first or second order. Those conditions form the basis for our development
of I(1) and I(2) representations of autoregressive Hilbertian processes.
Cointegrating and attractor subspaces are characterized in terms of the
behavior of the autoregressive operator pencil in a neighborhood of one.
"
1,124,"  There is a large literature on semiparametric estimation of average treatment
effects under unconfounded treatment assignment in settings with a fixed number
of covariates. More recently attention has focused on settings with a large
number of covariates. In this paper we extend lessons from the earlier
literature to this new setting. We propose that in addition to reporting point
estimates and standard errors, researchers report results from a number of
supplementary analyses to assist in assessing the credibility of their
estimates.
"
1,125,"  Which equilibria will arise in signaling games depends on how the receiver
interprets deviations from the path of play. We develop a micro-foundation for
these off-path beliefs, and an associated equilibrium refinement, in a model
where equilibrium arises through non-equilibrium learning by populations of
patient and long-lived senders and receivers. In our model, young senders are
uncertain about the prevailing distribution of play, so they rationally send
out-of-equilibrium signals as experiments to learn about the behavior of the
population of receivers. Differences in the payoff functions of the types of
senders generate different incentives for these experiments. Using the Gittins
index (Gittins, 1979), we characterize which sender types use each signal more
often, leading to a constraint on the receiver's off-path beliefs based on
""type compatibility"" and hence a learning-based equilibrium selection.
"
1,126,"  In many areas, practitioners seek to use observational data to learn a
treatment assignment policy that satisfies application-specific constraints,
such as budget, fairness, simplicity, or other functional form constraints. For
example, policies may be restricted to take the form of decision trees based on
a limited set of easily observable individual characteristics. We propose a new
approach to this problem motivated by the theory of semiparametrically
efficient estimation. Our method can be used to optimize either binary
treatments or infinitesimal nudges to continuous treatments, and can leverage
observational data where causal effects are identified using a variety of
strategies, including selection on observables and instrumental variables.
Given a doubly robust estimator of the causal effect of assigning everyone to
treatment, we develop an algorithm for choosing whom to treat, and establish
strong guarantees for the asymptotic utilitarian regret of the resulting
policy.
"
1,127,"  In the recent years more and more high-dimensional data sets, where the
number of parameters $p$ is high compared to the number of observations $n$ or
even larger, are available for applied researchers. Boosting algorithms
represent one of the major advances in machine learning and statistics in
recent years and are suitable for the analysis of such data sets. While Lasso
has been applied very successfully for high-dimensional data sets in Economics,
boosting has been underutilized in this field, although it has been proven very
powerful in fields like Biostatistics and Pattern Recognition. We attribute
this to missing theoretical results for boosting. The goal of this paper is to
fill this gap and show that boosting is a competitive method for inference of a
treatment effect or instrumental variable (IV) estimation in a high-dimensional
setting. First, we present the $L_2$Boosting with componentwise least squares
algorithm and variants which are tailored for regression problems which are the
workhorse for most Econometric problems. Then we show how $L_2$Boosting can be
used for estimation of treatment effects and IV estimation. We highlight the
methods and illustrate them with simulations and empirical examples. For
further results and technical details we refer to Luo and Spindler (2016, 2017)
and to the online supplement of the paper.
"
1,128,"  Interbank lending and borrowing occur when financial institutions seek to
settle and refinance their mutual positions over time and circumstances. This
interactive process involves money creation at the aggregate level.
Coordination mismatch on interbank credit may trigger systemic crises. This
happened when, since summer 2007, interbank credit coordination did not longer
work smoothly across financial institutions, eventually requiring exceptional
monetary policies by central banks, and guarantee and bailout interventions by
governments. Our article develops an interacting heterogeneous agents-based
model of interbank credit coordination under minimal institutions. First, we
explore the link between interbank credit coordination and the money generation
process. Contrary to received understanding, interbank credit has the capacity
to make the monetary system unbound. Second, we develop simulation analysis on
imperfect interbank credit coordination, studying impact of interbank dynamics
on financial stability and resilience at individual and aggregate levels.
Systemically destabilizing forces prove to be related to the working of the
banking system over time, especially interbank coordination conditions and
circumstances.
"
1,129,"  We study a sequential-learning model featuring a network of naive agents with
Gaussian information structures. Agents apply a heuristic rule to aggregate
predecessors' actions. They weigh these actions according the strengths of
their social connections to different predecessors. We show this rule arises
endogenously when agents wrongly believe others act solely on private
information and thus neglect redundancies among observations. We provide a
simple linear formula expressing agents' actions in terms of network paths and
use this formula to characterize the set of networks where naive agents
eventually learn correctly. This characterization implies that, on all networks
where later agents observe more than one neighbor, there exist
disproportionately influential early agents who can cause herding on incorrect
actions. Going beyond existing social-learning results, we compute the
probability of such mislearning exactly. This allows us to compare likelihoods
of incorrect herding, and hence expected welfare losses, across network
structures. The probability of mislearning increases when link densities are
higher and when networks are more integrated. In partially segregated networks,
divergent early signals can lead to persistent disagreement between groups.
"
1,130,"  This paper examines the limit properties of information criteria (such as
AIC, BIC, HQIC) for distinguishing between the unit root model and the various
kinds of explosive models. The explosive models include the local-to-unit-root
model, the mildly explosive model and the regular explosive model. Initial
conditions with different order of magnitude are considered. Both the OLS
estimator and the indirect inference estimator are studied. It is found that
BIC and HQIC, but not AIC, consistently select the unit root model when data
come from the unit root model. When data come from the local-to-unit-root
model, both BIC and HQIC select the wrong model with probability approaching 1
while AIC has a positive probability of selecting the right model in the limit.
When data come from the regular explosive model or from the mildly explosive
model in the form of $1+n^{\alpha }/n$ with $\alpha \in (0,1)$, all three
information criteria consistently select the true model. Indirect inference
estimation can increase or decrease the probability for information criteria to
select the right model asymptotically relative to OLS, depending on the
information criteria and the true model. Simulation results confirm our
asymptotic results in finite sample.
"
1,131,"  The random coefficients model is an extension of the linear regression model
that allows for unobserved heterogeneity in the population by modeling the
regression coefficients as random variables. Given data from this model, the
statistical challenge is to recover information about the joint density of the
random coefficients which is a multivariate and ill-posed problem. Because of
the curse of dimensionality and the ill-posedness, pointwise nonparametric
estimation of the joint density is difficult and suffers from slow convergence
rates. Larger features, such as an increase of the density along some direction
or a well-accentuated mode can, however, be much easier detected from data by
means of statistical tests. In this article, we follow this strategy and
construct tests and confidence statements for qualitative features of the joint
density, such as increases, decreases and modes. We propose a multiple testing
approach based on aggregating single tests which are designed to extract shape
information on fixed scales and directions. Using recent tools for Gaussian
approximations of multivariate empirical processes, we derive expressions for
the critical value. We apply our method to simulated and real data.
"
1,132,"  This paper is an axiomatic study of consistent approval-based multi-winner
rules, i.e., voting rules that select a fixed-size group of candidates based on
approval ballots. We introduce the class of counting rules and provide an
axiomatic characterization of this class based on the consistency axiom.
Building upon this result, we axiomatically characterize three important
consistent multi-winner rules: Proportional Approval Voting, Multi-Winner
Approval Voting and the Approval Chamberlin--Courant rule. Our results
demonstrate the variety of multi-winner rules and illustrate three different,
orthogonal principles that multi-winner voting rules may represent: individual
excellence, diversity, and proportionality.
"
1,133,"  We develop a Bayesian vector autoregressive (VAR) model with multivariate
stochastic volatility that is capable of handling vast dimensional information
sets. Three features are introduced to permit reliable estimation of the model.
First, we assume that the reduced-form errors in the VAR feature a factor
stochastic volatility structure, allowing for conditional equation-by-equation
estimation. Second, we apply recently developed global-local shrinkage priors
to the VAR coefficients to cure the curse of dimensionality. Third, we utilize
recent innovations to efficiently sample from high-dimensional multivariate
Gaussian distributions. This makes simulation-based fully Bayesian inference
feasible when the dimensionality is large but the time series length is
moderate. We demonstrate the merits of our approach in an extensive simulation
study and apply the model to US macroeconomic data to evaluate its forecasting
capabilities.
"
1,134,"  This paper proposes a valid bootstrap-based distributional approximation for
M-estimators exhibiting a Chernoff (1964)-type limiting distribution. For
estimators of this kind, the standard nonparametric bootstrap is inconsistent.
The method proposed herein is based on the nonparametric bootstrap, but
restores consistency by altering the shape of the criterion function defining
the estimator whose distribution we seek to approximate. This modification
leads to a generic and easy-to-implement resampling method for inference that
is conceptually distinct from other available distributional approximations. We
illustrate the applicability of our results with four examples in econometrics
and machine learning.
"
1,135,"  This paper uses model symmetries in the instrumental variable (IV) regression
to derive an invariant test for the causal structural parameter. Contrary to
popular belief, we show that there exist model symmetries when equation errors
are heteroskedastic and autocorrelated (HAC). Our theory is consistent with
existing results for the homoskedastic model (Andrews, Moreira, and Stock
(2006) and Chamberlain (2007)). We use these symmetries to propose the
conditional integrated likelihood (CIL) test for the causality parameter in the
over-identified model. Theoretical and numerical findings show that the CIL
test performs well compared to other tests in terms of power and
implementation. We recommend that practitioners use the Anderson-Rubin (AR)
test in the just-identified model, and the CIL test in the over-identified
model.
"
1,136,"  It is common to assume in empirical research that observables and
unobservables are additively separable, especially, when the former are
endogenous. This is done because it is widely recognized that identification
and estimation challenges arise when interactions between the two are allowed
for. Starting from a nonseparable IV model, where the instrumental variable is
independent of unobservables, we develop a novel nonparametric test of
separability of unobservables. The large-sample distribution of the test
statistics is nonstandard and relies on a novel Donsker-type central limit
theorem for the empirical distribution of nonparametric IV residuals, which may
be of independent interest. Using a dataset drawn from the 2015 US Consumer
Expenditure Survey, we find that the test rejects the separability in Engel
curves for most of the commodities.
"
1,137,"  Given a set of baseline assumptions, a breakdown frontier is the boundary
between the set of assumptions which lead to a specific conclusion and those
which do not. In a potential outcomes model with a binary treatment, we
consider two conclusions: First, that ATE is at least a specific value (e.g.,
nonnegative) and second that the proportion of units who benefit from treatment
is at least a specific value (e.g., at least 50\%). For these conclusions, we
derive the breakdown frontier for two kinds of assumptions: one which indexes
relaxations of the baseline random assignment of treatment assumption, and one
which indexes relaxations of the baseline rank invariance assumption. These
classes of assumptions nest both the point identifying assumptions of random
assignment and rank invariance and the opposite end of no constraints on
treatment selection or the dependence structure between potential outcomes.
This frontier provides a quantitative measure of robustness of conclusions to
relaxations of the baseline point identifying assumptions. We derive
$\sqrt{N}$-consistent sample analog estimators for these frontiers. We then
provide two asymptotically valid bootstrap procedures for constructing lower
uniform confidence bands for the breakdown frontier. As a measure of
robustness, estimated breakdown frontiers and their corresponding confidence
bands can be presented alongside traditional point estimates and confidence
intervals obtained under point identifying assumptions. We illustrate this
approach in an empirical application to the effect of child soldiering on
wages. We find that sufficiently weak conclusions are robust to simultaneous
failures of rank invariance and random assignment, while some stronger
conclusions are fairly robust to failures of rank invariance but not
necessarily to relaxations of random assignment.
"
1,138,"  We present a simple continuous-time model of clearing in financial networks.
Financial firms are represented as ""tanks"" filled with fluid (money), flowing
in and out. Once ""pipes"" connecting ""tanks"" are open, the system reaches the
clearing payment vector in finite time. This approach provides a simple
recursive solution to a classical static model of financial clearing in
bankruptcy, and suggests a practical payment mechanism. With sufficient
resources, a system of mutual obligations can be restructured into an
equivalent system that has a cascade structure: there is a group of banks that
paid off their debts, another group that owes money only to banks in the first
group, and so on. Technically, we use the machinery of Markov chains to analyze
evolution of a deterministic dynamical system.
"
1,139,"  In treatment allocation problems the individuals to be treated often arrive
sequentially. We study a problem in which the policy maker is not only
interested in the expected cumulative welfare but is also concerned about the
uncertainty/risk of the treatment outcomes. At the outset, the total number of
treatment assignments to be made may even be unknown. A sequential treatment
policy which attains the minimax optimal regret is proposed. We also
demonstrate that the expected number of suboptimal treatments only grows slowly
in the number of treatments. Finally, we study a setting where outcomes are
only observed with delay.
"
1,140,"  Consider a researcher estimating the parameters of a regression function
based on data for all 50 states in the United States or on data for all visits
to a website. What is the interpretation of the estimated parameters and the
standard errors? In practice, researchers typically assume that the sample is
randomly drawn from a large population of interest and report standard errors
that are designed to capture sampling variation. This is common even in
applications where it is difficult to articulate what that population of
interest is, and how it differs from the sample. In this article, we explore an
alternative approach to inference, which is partly design-based. In a
design-based setting, the values of some of the regressors can be manipulated,
perhaps through a policy intervention. Design-based uncertainty emanates from
lack of knowledge about the values that the regression outcome would have taken
under alternative interventions. We derive standard errors that account for
design-based uncertainty instead of, or in addition to, sampling-based
uncertainty. We show that our standard errors in general are smaller than the
usual infinite-population sampling-based standard errors and provide conditions
under which they coincide.
"
1,141,"  The Machina thought experiments pose to major non-expected utility models
challenges that are similar to those posed by the Ellsberg thought experiments
to subjective expected utility theory (SEUT). We test human choices in the
`Ellsberg three-color example', confirming typical ambiguity aversion patterns,
and the `Machina 50/51 and reflection examples', partially confirming the
preferences hypothesized by Machina. Then, we show that a quantum-theoretic
framework for decision-making under uncertainty recently elaborated by some of
us allows faithful modeling of all data on the Ellsberg and Machina paradox
situations. In the quantum-theoretic framework subjective probabilities are
represented by quantum probabilities, while quantum state transformations
enable representations of ambiguity aversion and subjective attitudes toward
it.
"
1,142,"  Bayesian inference for stochastic volatility models using MCMC methods highly
depends on actual parameter values in terms of sampling efficiency. While draws
from the posterior utilizing the standard centered parameterization break down
when the volatility of volatility parameter in the latent state equation is
small, non-centered versions of the model show deficiencies for highly
persistent latent variable series. The novel approach of
ancillarity-sufficiency interweaving has recently been shown to aid in
overcoming these issues for a broad class of multilevel models. In this paper,
we demonstrate how such an interweaving strategy can be applied to stochastic
volatility models in order to greatly improve sampling efficiency for all
parameters and throughout the entire parameter range. Moreover, this method of
""combining best of different worlds"" allows for inference for parameter
constellations that have previously been infeasible to estimate without the
need to select a particular parameterization beforehand.
"
1,143,"  Structural econometric methods are often criticized for being sensitive to
functional form assumptions. We study parametric estimators of the local
average treatment effect (LATE) derived from a widely used class of latent
threshold crossing models and show they yield LATE estimates algebraically
equivalent to the instrumental variables (IV) estimator. Our leading example is
Heckman's (1979) two-step (""Heckit"") control function estimator which, with
two-sided non-compliance, can be used to compute estimates of a variety of
causal parameters. Equivalence with IV is established for a semi-parametric
family of control function estimators and shown to hold at interior solutions
for a class of maximum likelihood estimators. Our results suggest differences
between structural and IV estimates often stem from disagreements about the
target parameter rather than from functional form assumptions per se. In cases
where equivalence fails, reporting structural estimates of LATE alongside IV
provides a simple means of assessing the credibility of structural
extrapolation exercises.
"
1,144,"  Multinomial choice models are fundamental for empirical modeling of economic
choices among discrete alternatives. We analyze identification of binary and
multinomial choice models when the choice utilities are nonseparable in
observed attributes and multidimensional unobserved heterogeneity with
cross-section and panel data. We show that derivatives of choice probabilities
with respect to continuous attributes are weighted averages of utility
derivatives in cross-section models with exogenous heterogeneity. In the
special case of random coefficient models with an independent additive effect,
we further characterize that the probability derivative at zero is proportional
to the population mean of the coefficients. We extend the identification
results to models with endogenous heterogeneity using either a control function
or panel data. In time stationary panel models with two periods, we find that
differences over time of derivatives of choice probabilities identify utility
derivatives ""on the diagonal,"" i.e. when the observed attributes take the same
values in the two periods. We also show that time stationarity does not
identify structural derivatives ""off the diagonal"" both in continuous and
multinomial choice panel models.
"
1,145,"  In this paper we present tools for applied researchers that re-purpose
off-the-shelf methods from the computer-science field of machine learning to
create a ""discovery engine"" for data from randomized controlled trials (RCTs).
The applied problem we seek to solve is that economists invest vast resources
into carrying out RCTs, including the collection of a rich set of candidate
outcome measures. But given concerns about inference in the presence of
multiple testing, economists usually wind up exploring just a small subset of
the hypotheses that the available data could be used to test. This prevents us
from extracting as much information as possible from each RCT, which in turn
impairs our ability to develop new theories or strengthen the design of policy
interventions. Our proposed solution combines the basic intuition of reverse
regression, where the dependent variable of interest now becomes treatment
assignment itself, with methods from machine learning that use the data
themselves to flexibly identify whether there is any function of the outcomes
that predicts (or has signal about) treatment group status. This leads to
correctly-sized tests with appropriate $p$-values, which also have the
important virtue of being easy to implement in practice. One open challenge
that remains with our work is how to meaningfully interpret the signal that
these methods find.
"
1,146,"  In the classical herding literature, agents receive a private signal
regarding a binary state of nature, and sequentially choose an action, after
observing the actions of their predecessors. When the informativeness of
private signals is unbounded, it is known that agents converge to the correct
action and correct belief. We study how quickly convergence occurs, and show
that it happens more slowly than it does when agents observe signals. However,
we also show that the speed of learning from actions can be arbitrarily close
to the speed of learning from signals. In particular, the expected time until
the agents stop taking the wrong action can be either finite or infinite,
depending on the private signal distribution. In the canonical case of Gaussian
private signals we calculate the speed of convergence precisely, and show
explicitly that, in this case, learning from actions is significantly slower
than learning from signals.
"
1,147,"  This paper develops theory for feasible estimators of finite-dimensional
parameters identified by general conditional quantile restrictions, under much
weaker assumptions than previously seen in the literature. This includes
instrumental variables nonlinear quantile regression as a special case. More
specifically, we consider a set of unconditional moments implied by the
conditional quantile restrictions, providing conditions for local
identification. Since estimators based on the sample moments are generally
impossible to compute numerically in practice, we study feasible estimators
based on smoothed sample moments. We propose a method of moments estimator for
exactly identified models, as well as a generalized method of moments estimator
for over-identified models. We establish consistency and asymptotic normality
of both estimators under general conditions that allow for weakly dependent
data and nonlinear structural models. Simulations illustrate the finite-sample
properties of the methods. Our in-depth empirical application concerns the
consumption Euler equation derived from quantile utility maximization.
Advantages of the quantile Euler equation include robustness to fat tails,
decoupling of risk attitude from the elasticity of intertemporal substitution,
and log-linearization without any approximation error. For the four countries
we examine, the quantile estimates of discount factor and elasticity of
intertemporal substitution are economically reasonable for a range of quantiles
above the median, even when two-stage least squares estimates are not
reasonable.
"
1,148,"  This paper examines signalling when the sender exerts effort and receives
benefits over time. Receivers only observe a noisy public signal about the
effort, which has no intrinsic value.
  The modelling of signalling in a dynamic context gives rise to novel
equilibrium outcomes. In some equilibria, a sender with a higher cost of effort
exerts strictly more effort than his low-cost counterpart. The low-cost type
can compensate later for initial low effort, but this is not worthwhile for a
high-cost type. The interpretation of a given signal switches endogenously over
time, depending on which type the receivers expect to send it.
  JEL classification: D82, D83, C73.
  Keywords: Dynamic games, signalling , incomplete information
"
1,149,"  We show that the space in which scientific, technological and economic
developments interplay with each other can be mathematically shaped using
pioneering multilayer network and complexity techniques. We build the
tri-layered network of human activities (scientific production, patenting, and
industrial production) and study the interactions among them, also taking into
account the possible time delays. Within this construction we can identify
which capabilities and prerequisites are needed to be competitive in a given
activity, and even measure how much time is needed to transform, for instance,
the technological know-how into economic wealth and scientific innovation,
being able to make predictions with a very long time horizon. Quite
unexpectedly, we find empirical evidence that the naive knowledge flow from
science, to patents, to products is not supported by data, being instead
technology the best predictor for industrial and scientific production for the
next decades.
"
1,150,"  Conditional independence of treatment assignment from potential outcomes is a
commonly used but nonrefutable assumption. We derive identified sets for
various treatment effect parameters under nonparametric deviations from this
conditional independence assumption. These deviations are defined via a
conditional treatment assignment probability, which makes it straightforward to
interpret. Our results can be used to assess the robustness of empirical
conclusions obtained under the baseline conditional independence assumption.
"
1,151,"  We analyse the autocatalytic structure of technological networks and evaluate
its significance for the dynamics of innovation patenting. To this aim, we
define a directed network of technological fields based on the International
Patents Classification, in which a source node is connected to a receiver node
via a link if patenting activity in the source field anticipates patents in the
receiver field in the same region more frequently than we would expect at
random. We show that the evolution of the technology network is compatible with
the presence of a growing autocatalytic structure, i.e. a portion of the
network in which technological fields mutually benefit from being connected to
one another. We further show that technological fields in the core of the
autocatalytic set display greater fitness, i.e. they tend to appear in a
greater number of patents, thus suggesting the presence of positive spillovers
as well as positive reinforcement. Finally, we observe that core shifts take
place whereby different groups of technology fields alternate within the
autocatalytic structure; this points to the importance of recombinant
innovation taking place between close as well as distant fields of the
hierarchical classification of technological fields.
"
1,152,"  When comparing two distributions, it is often helpful to learn at which
quantiles or values there is a statistically significant difference. This
provides more information than the binary ""reject"" or ""do not reject"" decision
of a global goodness-of-fit test. Framing our question as multiple testing
across the continuum of quantiles $\tau\in(0,1)$ or values $r\in\mathbb{R}$, we
show that the Kolmogorov--Smirnov test (interpreted as a multiple testing
procedure) achieves strong control of the familywise error rate. However, its
well-known flaw of low sensitivity in the tails remains. We provide an
alternative method that retains such strong control of familywise error rate
while also having even sensitivity, i.e., equal pointwise type I error rates at
each of $n\to\infty$ order statistics across the distribution. Our one-sample
method computes instantly, using our new formula that also instantly computes
goodness-of-fit $p$-values and uniform confidence bands. To improve power, we
also propose stepdown and pre-test procedures that maintain control of the
asymptotic familywise error rate. One-sample and two-sample cases are
considered, as well as extensions to regression discontinuity designs and
conditional distributions. Simulations, empirical examples, and code are
provided.
"
1,153,"  The memory-type control charts, such as EWMA and CUSUM, are powerful tools
for detecting small quality changes in univariate and multivariate processes.
Many papers on economic design of these control charts use the formula proposed
by Lorenzen and Vance (1986) [Lorenzen, T. J., & Vance, L. C. (1986). The
economic design of control charts: A unified approach. Technometrics, 28(1),
3-10, DOI: 10.2307/1269598]. This paper shows that this formula is not correct
for memory-type control charts and its values can significantly deviate from
the original values even if the ARL values used in this formula are accurately
computed. Consequently, the use of this formula can result in charts that are
not economically optimal. The formula is corrected for memory-type control
charts, but unfortunately the modified formula is not a helpful tool from a
computational perspective. We show that simulation-based optimization is a
possible alternative method.
"
1,154,"  Shrinkage estimation usually reduces variance at the cost of bias. But when
we care only about some parameters of a model, I show that we can reduce
variance without incurring bias if we have additional information about the
distribution of covariates. In a linear regression model with homoscedastic
Normal noise, I consider shrinkage estimation of the nuisance parameters
associated with control variables. For at least three control variables and
exogenous treatment, I establish that the standard least-squares estimator is
dominated with respect to squared-error loss in the treatment effect even among
unbiased estimators and even when the target parameter is low-dimensional. I
construct the dominating estimator by a variant of James-Stein shrinkage in a
high-dimensional Normal-means problem. It can be interpreted as an invariant
generalized Bayes estimator with an uninformative (improper) Jeffreys prior in
the target parameter.
"
1,155,"  The two-stage least-squares (2SLS) estimator is known to be biased when its
first-stage fit is poor. I show that better first-stage prediction can
alleviate this bias. In a two-stage linear regression model with Normal noise,
I consider shrinkage in the estimation of the first-stage instrumental variable
coefficients. For at least four instrumental variables and a single endogenous
regressor, I establish that the standard 2SLS estimator is dominated with
respect to bias. The dominating IV estimator applies James-Stein type shrinkage
in a first-stage high-dimensional Normal-means problem followed by a
control-function approach in the second stage. It preserves invariances of the
structural instrumental variable equations.
"
1,156,"  Econometrics and machine learning seem to have one common goal: to construct
a predictive model, for a variable of interest, using explanatory variables (or
features). However, these two fields developed in parallel, thus creating two
different cultures, to paraphrase Breiman (2001). The first was to build
probabilistic models to describe economic phenomena. The second uses algorithms
that will learn from their mistakes, with the aim, most often to classify
(sounds, images, etc.). Recently, however, learning models have proven to be
more effective than traditional econometric techniques (with a price to pay
less explanatory power), and above all, they manage to manage much larger data.
In this context, it becomes necessary for econometricians to understand what
these two cultures are, what opposes them and especially what brings them
closer together, in order to appropriate tools developed by the statistical
learning community to integrate them into Econometric models.
"
1,157,"  It is known that the common factors in a large panel of data can be
consistently estimated by the method of principal components, and principal
components can be constructed by iterative least squares regressions. Replacing
least squares with ridge regressions turns out to have the effect of shrinking
the singular values of the common component and possibly reducing its rank. The
method is used in the machine learning literature to recover low-rank matrices.
We study the procedure from the perspective of estimating a minimum-rank
approximate factor model. We show that the constrained factor estimates are
biased but can be more efficient in terms of mean-squared errors. Rank
consideration suggests a data-dependent penalty for selecting the number of
factors. The new criterion is more conservative in cases when the nominal
number of factors is inflated by the presence of weak factors or large
measurement noise. The framework is extended to incorporate a priori linear
constraints on the loadings. We provide asymptotic results that can be used to
test economic hypotheses.
"
1,158,"  We add the assumption that players know their opponents' payoff functions and
rationality to a model of non-equilibrium learning in signaling games. Agents
are born into player roles and play against random opponents every period.
Inexperienced agents are uncertain about the prevailing distribution of
opponents' play, but believe that opponents never choose conditionally
dominated strategies. Agents engage in active learning and update beliefs based
on personal observations. Payoff information can refine or expand learning
predictions, since patient young senders' experimentation incentives depend on
which receiver responses they deem plausible. We show that with payoff
knowledge, the limiting set of long-run learning outcomes is bounded above by
rationality-compatible equilibria (RCE), and bounded below by uniform RCE. RCE
refine the Intuitive Criterion (Cho and Kreps, 1987) and include all divine
equilibria (Banks and Sobel, 1987). Uniform RCE sometimes but not always
exists, and implies universally divine equilibrium.
"
1,159,"  We show that estimators based on spectral regularization converge to the best
approximation of a structural parameter in a class of nonidentified linear
ill-posed inverse models. Importantly, this convergence holds in the uniform
and Hilbert space norms. We describe several circumstances when the best
approximation coincides with a structural parameter, or at least reasonably
approximates it, and discuss how our results can be useful in the partial
identification setting. Lastly, we document that identification failures have
important implications for the asymptotic distribution of a linear functional
of regularized estimators, which can have a weighted chi-squared component. The
theory is illustrated for various high-dimensional and nonparametric IV
regressions.
"
1,160,"  This note is a contribution to the debate about the optimal algorithm for
Economic Complexity that recently appeared on ArXiv [1, 2] . The authors of [2]
eventually agree that the ECI+ algorithm [1] consists just in a renaming of the
Fitness algorithm we introduced in 2012, as we explicitly showed in [3].
However, they omit any comment on the fact that their extensive numerical tests
claimed to demonstrate that the same algorithm works well if they name it ECI+,
but not if its name is Fitness. They should realize that this eliminates any
credibility to their numerical methods and therefore also to their new
analysis, in which they consider many algorithms [2]. Since by their own
admission the best algorithm is the Fitness one, their new claim became that
the search for the best algorithm is pointless and all algorithms are alike.
This is exactly the opposite of what they claimed a few days ago and it does
not deserve much comments. After these clarifications we also present a
constructive analysis of the status of Economic Complexity, its algorithms, its
successes and its perspectives. For us the discussion closes here, we will not
reply to further comments.
"
1,161,"  A counterparty credit limit (CCL) is a limit that is imposed by a financial
institution to cap its maximum possible exposure to a specified counterparty.
CCLs help institutions to mitigate counterparty credit risk via selective
diversification of their exposures. In this paper, we analyze how CCLs impact
the prices that institutions pay for their trades during everyday trading. We
study a high-quality data set from a large electronic trading platform in the
foreign exchange spot market, which enables institutions to apply CCLs. We find
empirically that CCLs had little impact on the vast majority of trades in this
data. We also study the impact of CCLs using a new model of trading. By
simulating our model with different underlying CCL networks, we highlight that
CCLs can have a major impact in some situations.
"
1,162,"  This article reviews recent advances in fixed effect estimation of panel data
models for long panels, where the number of time periods is relatively large.
We focus on semiparametric models with unobserved individual and time effects,
where the distribution of the outcome variable conditional on covariates and
unobserved effects is specified parametrically, while the distribution of the
unobserved effects is left unrestricted. Compared to existing reviews on long
panels (Arellano and Hahn 2007; a section in Arellano and Bonhomme 2011) we
discuss models with both individual and time effects, split-panel Jackknife
bias corrections, unbalanced panels, distribution and quantile effects, and
other extensions. Understanding and correcting the incidental parameter bias
caused by the estimation of many fixed effects is our main focus, and the
unifying theme is that the order of this bias is given by the simple formula
p/n for all models discussed, with p the number of estimated parameters and n
the total sample size.
"
1,163,"  This paper considers the identification of treatment effects on conditional
transition probabilities. We show that even under random assignment only the
instantaneous average treatment effect is point identified. Since treated and
control units drop out at different rates, randomization only ensures the
comparability of treatment and controls at the time of randomization, so that
long-run average treatment effects are not point identified. Instead we derive
informative bounds on these average treatment effects. Our bounds do not impose
(semi)parametric restrictions, for example, proportional hazards. We also
explore various assumptions such as monotone treatment response, common shocks
and positively correlated outcomes that tighten the bounds.
"
1,164,"  We propose an inference procedure for estimators defined by mathematical
programming problems, focusing on the important special cases of linear
programming (LP) and quadratic programming (QP). In these settings, the
coefficients in both the objective function and the constraints of the
mathematical programming problem may be estimated from data and hence involve
sampling error. Our inference approach exploits the characterization of the
solutions to these programming problems by complementarity conditions; by doing
so, we can transform the problem of doing inference on the solution of a
constrained optimization problem (a non-standard inference problem) into one
involving inference based on a set of inequalities with pre-estimated
coefficients, which is much better understood. We evaluate the performance of
our procedure in several Monte Carlo simulations and an empirical application
to the classic portfolio selection problem in finance.
"
1,165,"  This paper establishes a general equivalence between discrete choice and
rational inattention models. Matejka and McKay (2015, AER) showed that when
information costs are modelled using the Shannon entropy function, the
resulting choice probabilities in the rational inattention model take the
multinomial logit form. By exploiting convex-analytic properties of the
discrete choice model, we show that when information costs are modelled using a
class of generalized entropy functions, the choice probabilities in any
rational inattention model are observationally equivalent to some additive
random utility discrete choice model and vice versa. Thus any additive random
utility model can be given an interpretation in terms of boundedly rational
behavior. This includes empirically relevant specifications such as the probit
and nested logit models.
"
1,166,"  We analyze the empirical content of the Roy model, stripped down to its
essential features, namely sector specific unobserved heterogeneity and
self-selection on the basis of potential outcomes. We characterize sharp bounds
on the joint distribution of potential outcomes and testable implications of
the Roy self-selection model under an instrumental constraint on the joint
distribution of potential outcomes we call stochastically monotone instrumental
variable (SMIV). We show that testing the Roy model selection is equivalent to
testing stochastic monotonicity of observed outcomes relative to the
instrument. We apply our sharp bounds to the derivation of a measure of
departure from Roy self-selection to identify values of observable
characteristics that induce the most costly misallocation of talent and sector
and are therefore prime targets for intervention. Special emphasis is put on
the case of binary outcomes, which has received little attention in the
literature to date. For richer sets of outcomes, we emphasize the distinction
between pointwise sharp bounds and functional sharp bounds, and its importance,
when constructing sharp bounds on functional features, such as inequality
measures. We analyze a Roy model of college major choice in Canada and Germany
within this framework, and we take a new look at the under-representation of
women in~STEM.
"
1,167,"  The ongoing net neutrality debate has generated a lot of heated discussions
on whether or not monetary interactions should be regulated between content and
access providers. Among the several topics discussed, `differential pricing'
has recently received attention due to `zero-rating' platforms proposed by some
service providers. In the differential pricing scheme, Internet Service
Providers (ISPs) can exempt data access charges for on content from certain CPs
(zero-rated) while no exemption is on content from other CPs. This allows the
possibility for Content Providers (CPs) to make `sponsorship' agreements to
zero-rate their content and attract more user traffic. In this paper, we study
the effect of differential pricing on various players in the Internet. We first
consider a model with a monopolistic ISP and multiple CPs where users select
CPs based on the quality of service (QoS) and data access charges. We show that
in a differential pricing regime 1) a CP offering low QoS can make have higher
surplus than a CP offering better QoS through sponsorships. 2) Overall QoS
(mean delay) for end users can degrade under differential pricing schemes. In
the oligopolistic market with multiple ISPs, users tend to select the ISP with
lowest ISP resulting in same type of conclusions as in the monopolistic market.
We then study how differential pricing effects the revenue of ISPs.
"
1,168,"  This paper presents an intertemporal bimodal network to analyze the evolution
of the semantic content of a scientific field within the framework of topic
modeling, namely using the Latent Dirichlet Allocation (LDA). The main
contribution is the conceptualization of the topic dynamics and its
formalization and codification into an algorithm. To benchmark the
effectiveness of this approach, we propose three indexes which track the
transformation of topics over time, their rate of birth and death, and the
novelty of their content. Applying the LDA, we test the algorithm both on a
controlled experiment and on a corpus of several thousands of scientific papers
over a period of more than 100 years which account for the history of the
economic thought.
"
1,169,"  This paper derives conditions under which preferences and technology are
nonparametrically identified in hedonic equilibrium models, where products are
differentiated along more than one dimension and agents are characterized by
several dimensions of unobserved heterogeneity. With products differentiated
along a quality index and agents characterized by scalar unobserved
heterogeneity, single crossing conditions on preferences and technology provide
identifying restrictions in Ekeland, Heckman and Nesheim (2004) and Heckman,
Matzkin and Nesheim (2010). We develop similar shape restrictions in the
multi-attribute case. These shape restrictions, which are based on optimal
transport theory and generalized convexity, allow us to identify preferences
for goods differentiated along multiple dimensions, from the observation of a
single market. We thereby derive nonparametric identification results for
nonseparable simultaneous equations and multi-attribute hedonic equilibrium
models with (possibly) multiple dimensions of unobserved heterogeneity. One of
our results is a proof of absolute continuity of the distribution of
endogenously traded qualities, which is of independent interest.
"
1,170,"  In many macroeconomic applications, confidence intervals for impulse
responses are constructed by estimating VAR models in levels - ignoring
cointegration rank uncertainty. We investigate the consequences of ignoring
this uncertainty. We adapt several methods for handling model uncertainty and
highlight their shortcomings. We propose a new method -
Weighted-Inference-by-Model-Plausibility (WIMP) - that takes rank uncertainty
into account in a data-driven way. In simulations the WIMP outperforms all
other methods considered, delivering intervals that are robust to rank
uncertainty, yet not overly conservative. We also study potential ramifications
of rank uncertainty on applied macroeconomic analysis by re-assessing the
effects of fiscal policy shocks.
"
1,171,"  The uncertainty and robustness of Computable General Equilibrium models can
be assessed by conducting a Systematic Sensitivity Analysis. Different methods
have been used in the literature for SSA of CGE models such as Gaussian
Quadrature and Monte Carlo methods. This paper explores the use of Quasi-random
Monte Carlo methods based on the Halton and Sobol' sequences as means to
improve the efficiency over regular Monte Carlo SSA, thus reducing the
computational requirements of the SSA. The findings suggest that by using
low-discrepancy sequences, the number of simulations required by the regular MC
SSA methods can be notably reduced, hence lowering the computational time
required for SSA of CGE models.
"
1,172,"  We propose a method of estimating the linear-in-means model of peer effects
in which the peer group, defined by a social network, is endogenous in the
outcome equation for peer effects. Endogeneity is due to unobservable
individual characteristics that influence both link formation in the network
and the outcome of interest. We propose two estimators of the peer effect
equation that control for the endogeneity of the social connections using a
control function approach. We leave the functional form of the control function
unspecified and treat it as unknown. To estimate the model, we use a sieve
semiparametric approach, and we establish asymptotics of the semiparametric
estimator.
"
1,173,"  Gaussian graphical models are recently used in economics to obtain networks
of dependence among agents. A widely-used estimator is the Graphical Lasso
(GLASSO), which amounts to a maximum likelihood estimation regularized using
the $L_{1,1}$ matrix norm on the precision matrix $\Omega$. The $L_{1,1}$ norm
is a lasso penalty that controls for sparsity, or the number of zeros in
$\Omega$. We propose a new estimator called Structured Graphical Lasso
(SGLASSO) that uses the $L_{1,2}$ mixed norm. The use of the $L_{1,2}$ penalty
controls for the structure of the sparsity in $\Omega$. We show that when the
network size is fixed, SGLASSO is asymptotically equivalent to an infeasible
GLASSO problem which prioritizes the sparsity-recovery of high-degree nodes.
Monte Carlo simulation shows that SGLASSO outperforms GLASSO in terms of
estimating the overall precision matrix and in terms of estimating the
structure of the graphical model. In an empirical illustration using a classic
firms' investment dataset, we obtain a network of firms' dependence that
exhibits the core-periphery structure, with General Motors, General Electric
and U.S. Steel forming the core group of firms.
"
1,174,"  This paper considers the problem of forecasting a collection of short time
series using cross sectional information in panel data. We construct point
predictors using Tweedie's formula for the posterior mean of heterogeneous
coefficients under a correlated random effects distribution. This formula
utilizes cross-sectional information to transform the unit-specific (quasi)
maximum likelihood estimator into an approximation of the posterior mean under
a prior distribution that equals the population distribution of the random
coefficients. We show that the risk of a predictor based on a non-parametric
estimate of the Tweedie correction is asymptotically equivalent to the risk of
a predictor that treats the correlated-random-effects distribution as known
(ratio-optimality). Our empirical Bayes predictor performs well compared to
various competitors in a Monte Carlo study. In an empirical application we use
the predictor to forecast revenues for a large panel of bank holding companies
and compare forecasts that condition on actual and severely adverse
macroeconomic conditions.
"
1,175,"  There is a fast growing literature that set-identifies structural vector
autoregressions (SVARs) by imposing sign restrictions on the responses of a
subset of the endogenous variables to a particular structural shock
(sign-restricted SVARs). Most methods that have been used to construct
pointwise coverage bands for impulse responses of sign-restricted SVARs are
justified only from a Bayesian perspective. This paper demonstrates how to
formulate the inference problem for sign-restricted SVARs within a
moment-inequality framework. In particular, it develops methods of constructing
confidence bands for impulse response functions of sign-restricted SVARs that
are valid from a frequentist perspective. The paper also provides a comparison
of frequentist and Bayesian coverage bands in the context of an empirical
application - the former can be substantially wider than the latter.
"
1,176,"  We systematically investigate the effect heterogeneity of job search
programmes for unemployed workers. To investigate possibly heterogeneous
employment effects, we combine non-experimental causal empirical models with
Lasso-type estimators. The empirical analyses are based on rich administrative
data from Swiss social security records. We find considerable heterogeneities
only during the first six months after the start of training. Consistent with
previous results of the literature, unemployed persons with fewer employment
opportunities profit more from participating in these programmes. Furthermore,
we also document heterogeneous employment effects by residence status. Finally,
we show the potential of easy-to-implement programme participation rules for
improving average employment effects of these active labour market programmes.
"
1,177,"  Dynamic contracts with multiple agents is a classical decentralized
decision-making problem with asymmetric information. In this paper, we extend
the single-agent dynamic incentive contract model in continuous-time to a
multi-agent scheme in finite horizon and allow the terminal reward to be
dependent on the history of actions and incentives. We first derive a set of
sufficient conditions for the existence of optimal contracts in the most
general setting and conditions under which they form a Nash equilibrium. Then
we show that the principal's problem can be converted to solving
Hamilton-Jacobi-Bellman (HJB) equation requiring a static Nash equilibrium.
Finally, we provide a framework to solve this problem by solving partial
differential equations (PDE) derived from backward stochastic differential
equations (BSDE).
"
1,178,"  To quantify uncertainty around point estimates of conditional objects such as
conditional means or variances, parameter uncertainty has to be taken into
account. Attempts to incorporate parameter uncertainty are typically based on
the unrealistic assumption of observing two independent processes, where one is
used for parameter estimation, and the other for conditioning upon. Such
unrealistic foundation raises the question whether these intervals are
theoretically justified in a realistic setting. This paper presents an
asymptotic justification for this type of intervals that does not require such
an unrealistic assumption, but relies on a sample-split approach instead. By
showing that our sample-split intervals coincide asymptotically with the
standard intervals, we provide a novel, and realistic, justification for
confidence intervals of conditional objects. The analysis is carried out for a
rich class of time series models.
"
1,179,"  This paper presents a new estimator of the intercept of a linear regression
model in cases where the outcome varaible is observed subject to a selection
rule. The intercept is often in this context of inherent interest; for example,
in a program evaluation context, the difference between the intercepts in
outcome equations for participants and non-participants can be interpreted as
the difference in average outcomes of participants and their counterfactual
average outcomes if they had chosen not to participate. The new estimator can
under mild conditions exhibit a rate of convergence in probability equal to
$n^{-p/(2p+1)}$, where $p\ge 2$ is an integer that indexes the strength of
certain smoothness assumptions. This rate of convergence is shown in this
context to be the optimal rate of convergence for estimation of the intercept
parameter in terms of a minimax criterion. The new estimator, unlike other
proposals in the literature, is under mild conditions consistent and
asymptotically normal with a rate of convergence that is the same regardless of
the degree to which selection depends on unobservables in the outcome equation.
Simulation evidence and an empirical example are included.
"
1,180,"  Identification of the parameters of stable linear dynamical systems is a
well-studied problem in the literature, both in the low and high-dimensional
settings. However, there are hardly any results for the unstable case,
especially regarding finite time bounds. For this setting, classical results on
least-squares estimation of the dynamics parameters are not applicable and
therefore new concepts and technical approaches need to be developed to address
the issue. Unstable linear systems arise in key real applications in control
theory, econometrics, and finance. This study establishes finite time bounds
for the identification error of the least-squares estimates for a fairly large
class of heavy-tailed noise distributions, and transition matrices of such
systems. The results relate the time length (samples) required for estimation
to a function of the problem dimension and key characteristics of the true
underlying transition matrix and the noise distribution. To establish them,
appropriate concentration inequalities for random matrices and for sequences of
martingale differences are leveraged.
"
1,181,"  Gale, Kuhn and Tucker (1950) introduced two ways to reduce a zero-sum game by
packaging some strategies with respect to a probability distribution on them.
In terms of value, they gave conditions for a desirable reduction. We show that
a probability distribution for a desirable reduction relies on optimal
strategies in the original game. Also, we correct an improper example given by
them to show that the reverse of a theorem does not hold.
"
1,182,"  In empirical work it is common to estimate parameters of models and report
associated standard errors that account for ""clustering"" of units, where
clusters are defined by factors such as geography. Clustering adjustments are
typically motivated by the concern that unobserved components of outcomes for
units within clusters are correlated. However, this motivation does not provide
guidance about questions such as: (i) Why should we adjust standard errors for
clustering in some situations but not others? How can we justify the common
practice of clustering in observational studies but not randomized experiments,
or clustering by state but not by gender? (ii) Why is conventional clustering a
potentially conservative ""all-or-nothing"" adjustment, and are there alternative
methods that respond to data and are less conservative? (iii) In what settings
does the choice of whether and how to cluster make a difference? We address
these questions using a framework of sampling and design inference. We argue
that clustering can be needed to address sampling issues if sampling follows a
two stage process where in the first stage, a subset of clusters are sampled
from a population of clusters, and in the second stage, units are sampled from
the sampled clusters. Then, clustered standard errors account for the existence
of clusters in the population that we do not see in the sample. Clustering can
be needed to account for design issues if treatment assignment is correlated
with membership in a cluster. We propose new variance estimators to deal with
intermediate settings where conventional cluster standard errors are
unnecessarily conservative and robust standard errors are too small.
"
1,183,"  In this paper, a unified approach is proposed to derive the exact local
asymptotic power for panel unit root tests, which is one of the most important
issues in nonstationary panel data literature. Two most widely used panel unit
root tests known as Levin-Lin-Chu (LLC, Levin, Lin and Chu (2002)) and
Im-Pesaran-Shin (IPS, Im, Pesaran and Shin (2003)) tests are systematically
studied for various situations to illustrate our method. Our approach is
characteristic function based, and can be used directly in deriving the moments
of the asymptotic distributions of these test statistics under the null and the
local-to-unity alternatives. For the LLC test, the approach provides an
alternative way to obtain the results that can be derived by the existing
method. For the IPS test, the new results are obtained, which fills the gap in
the literature where few results exist, since the IPS test is non-admissible.
Moreover, our approach has the advantage in deriving Edgeworth expansions of
these tests, which are also given in the paper. The simulations are presented
to illustrate our theoretical findings.
"
1,184,"  With the advent of Big Data, nowadays in many applications databases
containing large quantities of similar time series are available. Forecasting
time series in these domains with traditional univariate forecasting procedures
leaves great potentials for producing accurate forecasts untapped. Recurrent
neural networks (RNNs), and in particular Long Short-Term Memory (LSTM)
networks, have proven recently that they are able to outperform
state-of-the-art univariate time series forecasting methods in this context
when trained across all available time series. However, if the time series
database is heterogeneous, accuracy may degenerate, so that on the way towards
fully automatic forecasting methods in this space, a notion of similarity
between the time series needs to be built into the methods. To this end, we
present a prediction model that can be used with different types of RNN models
on subgroups of similar time series, which are identified by time series
clustering techniques. We assess our proposed methodology using LSTM networks,
a widely popular RNN variant. Our method achieves competitive results on
benchmarking datasets under competition evaluation procedures. In particular,
in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM
model and outperforms all other methods on the CIF2016 forecasting competition
dataset.
"
1,185,"  Given a sample of bids from independent auctions, this paper examines the
question of inference on auction fundamentals (e.g. valuation distributions,
welfare measures) under weak assumptions on information structure. The question
is important as it allows us to learn about the valuation distribution in a
robust way, i.e., without assuming that a particular information structure
holds across observations. We leverage the recent contributions of
\cite{Bergemann2013} in the robust mechanism design literature that exploit the
link between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in
incomplete information games to construct an econometrics framework for
learning about auction fundamentals using observed data on bids. We showcase
our construction of identified sets in private value and common value auctions.
Our approach for constructing these sets inherits the computational simplicity
of solving for correlated equilibria: checking whether a particular valuation
distribution belongs to the identified set is as simple as determining whether
a {\it linear} program is feasible. A similar linear program can be used to
construct the identified set on various welfare measures and counterfactual
objects. For inference and to summarize statistical uncertainty, we propose
novel finite sample methods using tail inequalities that are used to construct
confidence regions on sets. We also highlight methods based on Bayesian
bootstrap and subsampling. A set of Monte Carlo experiments show adequate
finite sample properties of our inference procedures. We illustrate our methods
using data from OCS auctions.
"
1,186,"  This paper examines and proposes several attribution modeling methods that
quantify how revenue should be attributed to online advertising inputs. We
adopt and further develop relative importance method, which is based on
regression models that have been extensively studied and utilized to
investigate the relationship between advertising efforts and market reaction
(revenue). Relative importance method aims at decomposing and allocating
marginal contributions to the coefficient of determination (R^2) of regression
models as attribution values. In particular, we adopt two alternative
submethods to perform this decomposition: dominance analysis and relative
weight analysis. Moreover, we demonstrate an extension of the decomposition
methods from standard linear model to additive model. We claim that our new
approaches are more flexible and accurate in modeling the underlying
relationship and calculating the attribution values. We use simulation examples
to demonstrate the superior performance of our new approaches over traditional
methods. We further illustrate the value of our proposed approaches using a
real advertising campaign dataset.
"
1,187,"  This paper characterizes the minimax linear estimator of the value of an
unknown function at a boundary point of its domain in a Gaussian white noise
model under the restriction that the first-order derivative of the unknown
function is Lipschitz continuous (the second-order H\""{o}lder class). The
result is then applied to construct the minimax optimal estimator for the
regression discontinuity design model, where the parameter of interest involves
function values at boundary points.
"
1,188,"  We review recent advances in modal regression studies using kernel density
estimation. Modal regression is an alternative approach for investigating
relationship between a response variable and its covariates. Specifically,
modal regression summarizes the interactions between the response variable and
covariates using the conditional mode or local modes. We first describe the
underlying model of modal regression and its estimators based on kernel density
estimation. We then review the asymptotic properties of the estimators and
strategies for choosing the smoothing bandwidth. We also discuss useful
algorithms and similar alternative approaches for modal regression, and propose
future direction in this field.
"
1,189,"  The recent research report of U.S. Department of Energy prompts us to
re-examine the pricing theories applied in electricity market design. The
theory of spot pricing is the basis of electricity market design in many
countries, but it has two major drawbacks: one is that it is still based on the
traditional hourly scheduling/dispatch model, ignores the crucial time
continuity in electric power production and consumption and does not treat the
inter-temporal constraints seriously; the second is that it assumes that the
electricity products are homogeneous in the same dispatch period and cannot
distinguish the base, intermediate and peak power with obviously different
technical and economic characteristics. To overcome the shortcomings, this
paper presents a continuous time commodity model of electricity, including spot
pricing model and load duration model. The market optimization models under the
two pricing mechanisms are established with the Riemann and Lebesgue integrals
respectively and the functional optimization problem are solved by the
Euler-Lagrange equation to obtain the market equilibria. The feasibility of
pricing according to load duration is proved by strict mathematical derivation.
Simulation results show that load duration pricing can correctly identify and
value different attributes of generators, reduce the total electricity
purchasing cost, and distribute profits among the power plants more equitably.
The theory and methods proposed in this paper will provide new ideas and
theoretical foundation for the development of electric power markets.
"
1,190,"  We generalize the approach of Carlier (2001) and provide an existence proof
for the multidimensional screening problem with general nonlinear preferences.
We first formulate the principal's problem as a maximization problem with
$G$-convexity constraints and then use $G$-convex analysis to prove existence.
"
1,191,"  This study proposes a simple technique for propensity score matching for
multiple treatment levels under the strong unconfoundedness assumption with the
help of the Aitchison distance proposed in the field of compositional data
analysis (CODA).
"
1,192,"  Binary classification is highly used in credit scoring in the estimation of
probability of default. The validation of such predictive models is based both
on rank ability, and also on calibration (i.e. how accurately the probabilities
output by the model map to the observed probabilities). In this study we cover
the current best practices regarding calibration for binary classification, and
explore how different approaches yield different results on real world credit
scoring data. The limitations of evaluating credit scoring models using only
rank ability metrics are explored. A benchmark is run on 18 real world
datasets, and results compared. The calibration techniques used are Platt
Scaling and Isotonic Regression. Also, different machine learning models are
used: Logistic Regression, Random Forest Classifiers, and Gradient Boosting
Classifiers. Results show that when the dataset is treated as a time series,
the use of re-calibration with Isotonic Regression is able to improve the long
term calibration better than the alternative methods. Using re-calibration, the
non-parametric models are able to outperform the Logistic Regression on Brier
Score Loss.
"
1,193,"  Ratio of medians or other suitable quantiles of two distributions is widely
used in medical research to compare treatment and control groups or in
economics to compare various economic variables when repeated cross-sectional
data are available. Inspired by the so-called growth incidence curves
introduced in poverty research, we argue that the ratio of quantile functions
is a more appropriate and informative tool to compare two distributions. We
present an estimator for the ratio of quantile functions and develop
corresponding simultaneous confidence bands, which allow to assess significance
of certain features of the quantile functions ratio. Derived simultaneous
confidence bands rely on the asymptotic distribution of the quantile functions
ratio and do not require re-sampling techniques. The performance of the
simultaneous confidence bands is demonstrated in simulations. Analysis of the
expenditure data from Uganda in years 1999, 2002 and 2005 illustrates the
relevance of our approach.
"
1,194,"  Constraining the maximum likelihood density estimator to satisfy a
sufficiently strong constraint, $\log-$concavity being a common example, has
the effect of restoring consistency without requiring additional parameters.
Since many results in economics require densities to satisfy a regularity
condition, these estimators are also attractive for the structural estimation
of economic models. In all of the examples of regularity conditions provided by
Bagnoli and Bergstrom (2005) and Ewerhart (2013), $\log-$concavity is
sufficient to ensure that the density satisfies the required conditions.
However, in many cases $\log-$concavity is far from necessary, and it has the
unfortunate side effect of ruling out sub-exponential tail behavior.
  In this paper, we use optimal transport to formulate a shape constrained
density estimator. We initially describe the estimator using a $\rho-$concavity
constraint. In this setting we provide results on consistency, asymptotic
distribution, convexity of the optimization problem defining the estimator, and
formulate a test for the null hypothesis that the population density satisfies
a shape constraint. Afterward, we provide sufficient conditions for these
results to hold using an arbitrary shape constraint. This generalization is
used to explore whether the California Department of Transportation's decision
to award construction contracts with the use of a first price auction is cost
minimizing. We estimate the marginal costs of construction firms subject to
Myerson's (1981) regularity condition, which is a requirement for the first
price reverse auction to be cost minimizing. The proposed test fails to reject
that the regularity condition is satisfied.
"
1,195,"  We present the calibrated-projection MATLAB package implementing the method
to construct confidence intervals proposed by Kaido, Molinari and Stoye (2017).
This manual provides details on how to use the package for inference on
projections of partially identified parameters. It also explains how to use the
MATLAB functions we developed to compute confidence intervals on solutions of
nonlinear optimization problems with estimated constraints.
"
1,196,"  In this paper we study methods for estimating causal effects in settings with
panel data, where some units are exposed to a treatment during some periods and
the goal is estimating counterfactual (untreated) outcomes for the treated
unit/period combinations. We propose a class of matrix completion estimators
that uses the observed elements of the matrix of control outcomes corresponding
to untreated unit/periods to impute the ""missing"" elements of the control
outcome matrix, corresponding to treated units/periods. This leads to a matrix
that well-approximates the original (incomplete) matrix, but has lower
complexity according to the nuclear norm for matrices. We generalize results
from the matrix completion literature by allowing the patterns of missing data
to have a time series dependency structure that is common in social science
applications. We present novel insights concerning the connections between the
matrix completion literature, the literature on interactive fixed effects
models and the literatures on program evaluation under unconfoundedness and
synthetic control methods. We show that all these estimators can be viewed as
focusing on the same objective function. They differ solely in the way they
deal with identification, in some cases solely through regularization (our
proposed nuclear norm matrix completion estimator) and in other cases primarily
through imposing hard restrictions (the unconfoundedness and synthetic control
approaches). The proposed method outperforms unconfoundedness-based or
synthetic control estimators in simulations based on real data.
"
1,197,"  Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, but understanding and explaining AI remain challenging. This
paper clarifies the connections between machine-learning algorithms to develop
AIs and the econometrics of dynamic structural models through the case studies
of three famous game AIs. Chess-playing Deep Blue is a calibrated value
function, whereas shogi-playing Bonanza is an estimated value function via
Rust's (1987) nested fixed-point method. AlphaGo's ""supervised-learning policy
network"" is a deep neural network implementation of Hotz and Miller's (1993)
conditional choice probability estimation; its ""reinforcement-learning value
network"" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional
choice simulation method. Relaxing these AIs' implicit econometric assumptions
would improve their structural interpretability.
"
1,198,"  We consider an index model of dyadic link formation with a homophily effect
index and a degree heterogeneity index. We provide nonparametric identification
results in a single large network setting for the potentially nonparametric
homophily effect function, the realizations of unobserved individual fixed
effects and the unknown distribution of idiosyncratic pairwise shocks, up to
normalization, for each possible true value of the unknown parameters. We
propose a novel form of scale normalization on an arbitrary interquantile
range, which is not only theoretically robust but also proves particularly
convenient for the identification analysis, as quantiles provide direct
linkages between the observable conditional probabilities and the unknown index
values. We then use an inductive ""in-fill and out-expansion"" algorithm to
establish our main results, and consider extensions to more general settings
that allow nonseparable dependence between homophily and degree heterogeneity,
as well as certain extents of network sparsity and weaker assumptions on the
support of unobserved heterogeneity. As a byproduct, we also propose a concept
called ""modeling equivalence"" as a refinement of ""observational equivalence"",
and use it to provide a formal discussion about normalization, identification
and their interplay with counterfactuals.
"
1,199,"  Peer-to-peer (P2P) lending is a fast growing financial technology (FinTech)
trend that is displacing traditional retail banking. Studies on P2P lending
have focused on predicting individual interest rates or default probabilities.
However, the relationship between aggregated P2P interest rates and the general
economy will be of interest to investors and borrowers as the P2P credit market
matures. We show that the variation in P2P interest rates across grade types
are determined by three macroeconomic latent factors formed by Canonical
Correlation Analysis (CCA) - macro default, investor uncertainty, and the
fundamental value of the market. However, the variation in P2P interest rates
across term types cannot be explained by the general economy.
"
1,200,"  In this paper, we provide some new results for the Weibull-R family of
distributions (Alzaghal, Ghosh and Alzaatreh (2016)). We derive some new
structural properties of the Weibull-R family of distributions. We provide
various characterizations of the family via conditional moments, some functions
of order statistics and via record values.
"
1,201,"  Double machine learning provides $\sqrt{n}$-consistent estimates of
parameters of interest even when high-dimensional or nonparametric nuisance
parameters are estimated at an $n^{-1/4}$ rate. The key is to employ
Neyman-orthogonal moment equations which are first-order insensitive to
perturbations in the nuisance parameters. We show that the $n^{-1/4}$
requirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order
notion of orthogonality that grants robustness to more complex or
higher-dimensional nuisance parameters. In the partially linear regression
setting popular in causal inference, we show that we can construct second-order
orthogonal moments if and only if the treatment residual is not normally
distributed. Our proof relies on Stein's lemma and may be of independent
interest. We conclude by demonstrating the robustness benefits of an explicit
doubly-orthogonal estimation procedure for treatment effect.
"
1,202,"  We assess the relationship between model size and complexity in the
time-varying parameter VAR framework via thorough predictive exercises for the
Euro Area, the United Kingdom and the United States. It turns out that
sophisticated dynamics through drifting coefficients are important in small
data sets while simpler models tend to perform better in sizeable data sets. To
combine best of both worlds, novel shrinkage priors help to mitigate the curse
of dimensionality, resulting in competitive forecasts for all scenarios
considered. Furthermore, we discuss dynamic model selection to improve upon the
best performing individual model for each point in time.
"
1,203,"  Startups have become in less than 50 years a major component of innovation
and economic growth. Silicon Valley has been the place where the startup
phenomenon was the most obvious and Stanford University was a major component
of that success. Companies such as Google, Yahoo, Sun Microsystems, Cisco,
Hewlett Packard had very strong links with Stanford but even these vary famous
success stories cannot fully describe the richness and diversity of the
Stanford entrepreneurial activity. This report explores the dynamics of more
than 5000 companies founded by Stanford University alumni and staff, through
their value creation, their field of activities, their growth patterns and
more. The report also explores some features of the founders of these companies
such as their academic background or the number of years between their Stanford
experience and their company creation.
"
1,204,"  Startups have become in less than 50 years a major component of innovation
and economic growth. An important feature of the startup phenomenon has been
the wealth created through equity in startups to all stakeholders. These
include the startup founders, the investors, and also the employees through the
stock-option mechanism and universities through licenses of intellectual
property. In the employee group, the allocation to important managers like the
chief executive, vice-presidents and other officers, and independent board
members is also analyzed. This report analyzes how equity was allocated in more
than 400 startups, most of which had filed for an initial public offering. The
author has the ambition of informing a general audience about best practice in
equity split, in particular in Silicon Valley, the central place for startup
innovation.
"
1,205,"  I propose a treatment selection model that introduces unobserved
heterogeneity in both choice sets and preferences to evaluate the average
effects of a program offer. I show how to exploit the model structure to define
parameters capturing these effects and then computationally characterize their
identified sets under instrumental variable variation in choice sets. I
illustrate these tools by analyzing the effects of providing an offer to the
Head Start preschool program using data from the Head Start Impact Study. I
find that such a policy affects a large number of children who take up the
offer, and that they subsequently have positive effects on test scores. These
effects arise from children who do not have any preschool as an outside option.
A cost-benefit analysis reveals that the earning benefits associated with the
test score gains can be large and outweigh the net costs associated with offer
take up.
"
1,206,"  Triangular systems with nonadditively separable unobserved heterogeneity
provide a theoretically appealing framework for the modelling of complex
structural relationships. However, they are not commonly used in practice due
to the need for exogenous variables with large support for identification, the
curse of dimensionality in estimation, and the lack of inferential tools. This
paper introduces two classes of semiparametric nonseparable triangular models
that address these limitations. They are based on distribution and quantile
regression modelling of the reduced form conditional distributions of the
endogenous variables. We show that average, distribution and quantile
structural functions are identified in these systems through a control function
approach that does not require a large support condition. We propose a
computationally attractive three-stage procedure to estimate the structural
functions where the first two stages consist of quantile or distribution
regressions. We provide asymptotic theory and uniform inference methods for
each stage. In particular, we derive functional central limit theorems and
bootstrap functional central limit theorems for the distribution regression
estimators of the structural functions. These results establish the validity of
the bootstrap for three-stage estimators of structural functions, and lead to
simple inference algorithms. We illustrate the implementation and applicability
of all our methods with numerical simulations and an empirical application to
demand analysis.
"
1,207,"  In this paper we extend the work by Ryuzo Sato devoted to the development of
economic growth models within the framework of the Lie group theory. We propose
a new growth model based on the assumption of logistic growth in factors. It is
employed to derive new production functions and introduce a new notion of wage
share. In the process it is shown that the new functions compare reasonably
well against relevant economic data. The corresponding problem of maximization
of profit under conditions of perfect competition is solved with the aid of one
of these functions. In addition, it is explained in reasonably rigorous
mathematical terms why Bowley's law no longer holds true in post-1960 data.
"
1,208,"  I study the measurement of scientists' influence using bibliographic data.
The main result is an axiomatic characterization of the family of
citation-counting indices, a broad class of influence measures which includes
the renowned h-index. The result highlights several limitations of these
indices: they are not suitable to compare scientists across different fields,
and they cannot account for indirect influence. I explore how these limitations
can be overcome by using richer bibliographic information.
"
1,209,"  I study identification, estimation and inference for spillover effects in
experiments where units' outcomes may depend on the treatment assignments of
other units within a group. I show that the commonly-used reduced-form
linear-in-means regression identifies a weighted sum of spillover effects with
some negative weights, and that the difference in means between treated and
controls identifies a combination of direct and spillover effects entering with
different signs. I propose nonparametric estimators for average direct and
spillover effects that overcome these issues and are consistent and
asymptotically normal under a precise relationship between the number of
parameters of interest, the total sample size and the treatment assignment
mechanism. These findings are illustrated using data from a conditional cash
transfer program and with simulations. The empirical results reveal the
potential pitfalls of failing to flexibly account for spillover effects in
policy evaluation: the estimated difference in means and the reduced-form
linear-in-means coefficients are all close to zero and statistically
insignificant, whereas the nonparametric estimators I propose reveal large,
nonlinear and significant spillover effects.
"
1,210,"  Futures market contracts with varying maturities are traded concurrently and
the speed at which they process information is of value in understanding the
pricing discovery process. Using price discovery measures, including Putnins
(2013) information leadership share and intraday data, we quantify the
proportional contribution of price discovery between nearby and deferred
contracts in the corn and live cattle futures markets. Price discovery is more
systematic in the corn than in the live cattle market. On average, nearby
contracts lead all deferred contracts in price discovery in the corn market,
but have a relatively less dominant role in the live cattle market. In both
markets, the nearby contract loses dominance when its relative volume share
dips below 50%, which occurs about 2-3 weeks before expiration in corn and 5-6
weeks before expiration in live cattle. Regression results indicate that the
share of price discovery is most closely linked to trading volume but is also
affected, to far less degree, by time to expiration, backwardation, USDA
announcements and market crashes. The effects of these other factors vary
between the markets which likely reflect the difference in storability as well
as other market-related characteristics.
"
1,211,"  We develop SHOPPER, a sequential probabilistic model of shopping data.
SHOPPER uses interpretable components to model the forces that drive how a
customer chooses products; in particular, we designed SHOPPER to capture how
items interact with other items. We develop an efficient posterior inference
algorithm to estimate these forces from large-scale data, and we analyze a
large dataset from a major chain grocery store. We are interested in answering
counterfactual queries about changes in prices. We found that SHOPPER provides
accurate predictions even under price interventions, and that it helps identify
complementary and substitutable pairs of products.
"
1,212,"  Testing for regime switching when the regime switching probabilities are
specified either as constants (`mixture models') or are governed by a
finite-state Markov chain (`Markov switching models') are long-standing
problems that have also attracted recent interest. This paper considers testing
for regime switching when the regime switching probabilities are time-varying
and depend on observed data (`observation-dependent regime switching').
Specifically, we consider the likelihood ratio test for observation-dependent
regime switching in mixture autoregressive models. The testing problem is
highly nonstandard, involving unidentified nuisance parameters under the null,
parameters on the boundary, singular information matrices, and higher-order
approximations of the log-likelihood. We derive the asymptotic null
distribution of the likelihood ratio test statistic in a general mixture
autoregressive setting using high-level conditions that allow for various forms
of dependence of the regime switching probabilities on past observations, and
we illustrate the theory using two particular mixture autoregressive models.
The likelihood ratio test has a nonstandard asymptotic distribution that can
easily be simulated, and Monte Carlo studies show the test to have satisfactory
finite sample size and power properties.
"
1,213,"  It is well known that sequential decision making may lead to information
cascades. That is, when agents make decisions based on their private
information, as well as observing the actions of those before them, then it
might be rational to ignore their private signal and imitate the action of
previous individuals. If the individuals are choosing between a right and a
wrong state, and the initial actions are wrong, then the whole cascade will be
wrong. This issue is due to the fact that cascades can be based on very little
information.
  We show that if agents occasionally disregard the actions of others and base
their action only on their private information, then wrong cascades can be
avoided. Moreover, we study the optimal asymptotic rate at which the error
probability at time $t$ can go to zero. The optimal policy is for the player at
time $t$ to follow their private information with probability $p_{t} = c/t$,
leading to a learning rate of $c'/t$, where the constants $c$ and $c'$ are
explicit.
"
1,214,"  We consider continuous-time models with a large panel of moment conditions,
where the structural parameter depends on a set of characteristics, whose
effects are of interest. The leading example is the linear factor model in
financial economics where factor betas depend on observed characteristics such
as firm specific instruments and macroeconomic variables, and their effects
pick up long-run time-varying beta fluctuations. We specify the factor betas as
the sum of characteristic effects and an orthogonal idiosyncratic parameter
that captures high-frequency movements. It is often the case that researchers
do not know whether or not the latter exists, or its strengths, and thus the
inference about the characteristic effects should be valid uniformly over a
broad class of data generating processes for idiosyncratic parameters. We
construct our estimation and inference in a two-step continuous-time GMM
framework. It is found that the limiting distribution of the estimated
characteristic effects has a discontinuity when the variance of the
idiosyncratic parameter is near the boundary (zero), which makes the usual
""plug-in"" method using the estimated asymptotic variance only valid pointwise
and may produce either over- or under- coveraging probabilities. We show that
the uniformity can be achieved by cross-sectional bootstrap. Our procedure
allows both known and estimated factors, and also features a bias correction
for the effect of estimating unknown factors.
"
1,215,"  Given additional distributional information in the form of moment
restrictions, kernel density and distribution function estimators with implied
generalised empirical likelihood probabilities as weights achieve a reduction
in variance due to the systematic use of this extra information. The particular
interest here is the estimation of densities or distributions of (generalised)
residuals in semi-parametric models defined by a finite number of moment
restrictions. Such estimates are of great practical interest, being potentially
of use for diagnostic purposes, including tests of parametric assumptions on an
error distribution, goodness-of-fit tests or tests of overidentifying moment
restrictions. The paper gives conditions for the consistency and describes the
asymptotic mean squared error properties of the kernel density and distribution
estimators proposed in the paper. A simulation study evaluates the small sample
performance of these estimators. Supplements provide analytic examples to
illustrate situations where kernel weighting provides a reduction in variance
together with proofs of the results in the paper.
"
1,216,"  We study the out-of-sample properties of robust empirical optimization
problems with smooth $\phi$-divergence penalties and smooth concave objective
functions, and develop a theory for data-driven calibration of the non-negative
""robustness parameter"" $\delta$ that controls the size of the deviations from
the nominal model. Building on the intuition that robust optimization reduces
the sensitivity of the expected reward to errors in the model by controlling
the spread of the reward distribution, we show that the first-order benefit of
``little bit of robustness"" (i.e., $\delta$ small, positive) is a significant
reduction in the variance of the out-of-sample reward while the corresponding
impact on the mean is almost an order of magnitude smaller. One implication is
that substantial variance (sensitivity) reduction is possible at little cost if
the robustness parameter is properly calibrated. To this end, we introduce the
notion of a robust mean-variance frontier to select the robustness parameter
and show that it can be approximated using resampling methods like the
bootstrap. Our examples show that robust solutions resulting from ""open loop""
calibration methods (e.g., selecting a $90\%$ confidence level regardless of
the data and objective function) can be very conservative out-of-sample, while
those corresponding to the robustness parameter that optimizes an estimate of
the out-of-sample expected reward (e.g., via the bootstrap) with no regard for
the variance are often insufficiently robust.
"
1,217,"  We present a robust generalization of the synthetic control method for
comparative case studies. Like the classical method, we present an algorithm to
estimate the unobservable counterfactual of a treatment unit. A distinguishing
feature of our algorithm is that of de-noising the data matrix via singular
value thresholding, which renders our approach robust in multiple facets: it
automatically identifies a good subset of donors, overcomes the challenges of
missing data, and continues to work well in settings where covariate
information may not be provided. To begin, we establish the condition under
which the fundamental assumption in synthetic control-like approaches holds,
i.e. when the linear relationship between the treatment unit and the donor pool
prevails in both the pre- and post-intervention periods. We provide the first
finite sample analysis for a broader class of models, the Latent Variable
Model, in contrast to Factor Models previously considered in the literature.
Further, we show that our de-noising procedure accurately imputes missing
entries, producing a consistent estimator of the underlying signal matrix
provided $p = \Omega( T^{-1 + \zeta})$ for some $\zeta > 0$; here, $p$ is the
fraction of observed data and $T$ is the time interval of interest. Under the
same setting, we prove that the mean-squared-error (MSE) in our prediction
estimation scales as $O(\sigma^2/p + 1/\sqrt{T})$, where $\sigma^2$ is the
noise variance. Using a data aggregation method, we show that the MSE can be
made as small as $O(T^{-1/2+\gamma})$ for any $\gamma \in (0, 1/2)$, leading to
a consistent estimator. We also introduce a Bayesian framework to quantify the
model uncertainty through posterior probabilities. Our experiments, using both
real-world and synthetic datasets, demonstrate that our robust generalization
yields an improvement over the classical synthetic control method.
"
1,218,"  Contextual bandit algorithms are sensitive to the estimation method of the
outcome model as well as the exploration method used, particularly in the
presence of rich heterogeneity or complex outcome models, which can lead to
difficult estimation problems along the path of learning. We study a
consideration for the exploration vs. exploitation framework that does not
arise in multi-armed bandits but is crucial in contextual bandits; the way
exploration and exploitation is conducted in the present affects the bias and
variance in the potential outcome model estimation in subsequent stages of
learning. We develop parametric and non-parametric contextual bandits that
integrate balancing methods from the causal inference literature in their
estimation to make it less prone to problems of estimation bias. We provide the
first regret bound analyses for contextual bandits with balancing in the domain
of linear contextual bandits that match the state of the art regret bounds. We
demonstrate the strong practical advantage of balanced contextual bandits on a
large number of supervised learning datasets and on a synthetic example that
simulates model mis-specification and prejudice in the initial training data.
Additionally, we develop contextual bandits with simpler assignment policies by
leveraging sparse model estimation methods from the econometrics literature and
demonstrate empirically that in the early stages they can improve the rate of
learning and decrease regret.
"
1,219,"  Economic complexity reflects the amount of knowledge that is embedded in the
productive structure of an economy. It resides on the premise of hidden
capabilities - fundamental endowments underlying the productive structure. In
general, measuring the capabilities behind economic complexity directly is
difficult, and indirect measures have been suggested which exploit the fact
that the presence of the capabilities is expressed in a country's mix of
products. We complement these studies by introducing a probabilistic framework
which leverages Bayesian non-parametric techniques to extract the dominant
features behind the comparative advantage in exported products. Based on
economic evidence and trade data, we place a restricted Indian Buffet Process
on the distribution of countries' capability endowment, appealing to a culinary
metaphor to model the process of capability acquisition. The approach comes
with a unique level of interpretability, as it produces a concise and
economically plausible description of the instantiated capabilities.
"
1,220,"  Two network measures known as the Economic Complexity Index (ECI) and Product
Complexity Index (PCI) have provided important insights into patterns of
economic development. We show that the ECI and PCI are equivalent to a spectral
clustering algorithm that partitions a similarity graph into two parts. The
measures are also related to various dimensionality reduction methods and can
be interpreted as vectors that determine distances between nodes based on their
similarity. Our results shed a new light on the ECI's empirical success in
explaining cross-country differences in GDP/capita and economic growth, which
is often linked to the diversity of country export baskets. In fact, countries
with high (low) ECI tend to specialize in high (low) PCI products. We also find
that the ECI and PCI uncover economically informative specialization patterns
across US states and UK regions.
"
1,221,"  This study briefly introduces the development of Shantou Special Economic
Zone under Reform and Opening-Up Policy from 1980 through 2016 with a focus on
policy making issues and its influences on local economy. This paper is divided
into two parts, 1980 to 1991, 1992 to 2016 in accordance with the separation of
the original Shantou District into three cities: Shantou, Chaozhou and Jieyang
in the end of 1991. This study analyzes the policy making issues in the
separation of the original Shantou District, the influences of the policy on
Shantou's economy after separation, the possibility of merging the three cities
into one big new economic district in the future and reasons that lead to the
stagnant development of Shantou in recent 20 years. This paper uses statistical
longitudinal analysis in analyzing economic problems with applications of
non-parametric statistics through generalized additive model and time series
forecasting methods. The paper is authored by Bowen Cai solely, who is the
graduate student in the PhD program of Applied and Computational Mathematics
and Statistics at the University of Notre Dame with concentration in big data
analysis.
"
1,222,"  This paper presents the identification of heterogeneous elasticities in the
Cobb-Douglas production function. The identification is constructive with
closed-form formulas for the elasticity with respect to each input for each
firm. We propose that the flexible input cost ratio plays the role of a control
function under ""non-collinear heterogeneity"" between elasticities with respect
to two flexible inputs. The ex ante flexible input cost share can be used to
identify the elasticities with respect to flexible inputs for each firm. The
elasticities with respect to labor and capital can be subsequently identified
for each firm under the timing assumption admitting the functional
independence.
"
1,223,"  Some empirical results are more likely to be published than others. Such
selective publication leads to biased estimates and distorted inference. This
paper proposes two approaches for identifying the conditional probability of
publication as a function of a study's results, the first based on systematic
replication studies and the second based on meta-studies. For known conditional
publication probabilities, we propose median-unbiased estimators and associated
confidence sets that correct for selective publication. We apply our methods to
recent large-scale replication studies in experimental economics and
psychology, and to meta-studies of the effects of minimum wages and de-worming
programs.
"
1,224,"  Research on growing American political polarization and antipathy primarily
studies public institutions and political processes, ignoring private effects
including strained family ties. Using anonymized smartphone-location data and
precinct-level voting, we show that Thanksgiving dinners attended by
opposing-party precinct residents were 30-50 minutes shorter than same-party
dinners. This decline from a mean of 257 minutes survives extensive spatial and
demographic controls. Dinner reductions in 2016 tripled for travelers from
media markets with heavy political advertising --- an effect not observed in
2015 --- implying a relationship to election-related behavior. Effects appear
asymmetric: while fewer Democratic-precinct residents traveled in 2016 than
2015, political differences shortened Thanksgiving dinners more among
Republican-precinct residents. Nationwide, 34 million person-hours of
cross-partisan Thanksgiving discourse were lost in 2016 to partisan effects.
"
1,225,"  This article contains new tools for studying the shape of the stationary
distribution of sizes in a dynamic economic system in which units experience
random multiplicative shocks and are occasionally reset. Each unit has a
Markov-switching type which influences their growth rate and reset probability.
We show that the size distribution has a Pareto upper tail, with exponent equal
to the unique positive solution to an equation involving the spectral radius of
a certain matrix-valued function. Under a non-lattice condition on growth
rates, an eigenvector associated with the Pareto exponent provides the
distribution of types in the upper tail of the size distribution.
"
1,226,"  We develop a machine learning based tool for accurate prediction of
socio-economic indicators from daytime satellite imagery. The diverse set of
indicators are often not intuitively related to observable features in
satellite images, and are not even always well correlated with each other. Our
predictive tool is more accurate than using night light as a proxy, and can be
used to predict missing data, smooth out noise in surveys, monitor development
progress of a region, and flag potential anomalies. Finally, we use predicted
variables to do robustness analysis of a regression study of high rate of
stunting in India.
"
1,227,"  Gift giving is a ubiquitous social phenomenon, and red packets have been used
as monetary gifts in Asian countries for thousands of years. In recent years,
online red packets have become widespread in China through the WeChat platform.
Exploiting a unique dataset consisting of 61 million group red packets and
seven million users, we conduct a large-scale, data-driven study to understand
the spread of red packets and the effect of red packets on group activity. We
find that the cash flows between provinces are largely consistent with
provincial GDP rankings, e.g., red packets are sent from users in the south to
those in the north. By distinguishing spontaneous from reciprocal red packets,
we reveal the behavioral patterns in sending red packets: males, seniors, and
people with more in-group friends are more inclined to spontaneously send red
packets, while red packets from females, youths, and people with less in-group
friends are more reciprocal. Furthermore, we use propensity score matching to
study the external effects of red packets on group dynamics. We show that red
packets increase group participation and strengthen in-group relationships,
which partly explain the benefits and motivations for sending red packets.
"
1,228,"  We consider the scaling laws, second-order statistics and entropy of the
consumed energy of metropolis cities which are hybrid complex systems
comprising social networks, engineering systems, agricultural output, economic
activity and energy components. We abstract a city in terms of two fundamental
variables; $s$ resource cells (of unit area) that represent energy-consuming
geographic or spatial zones (e.g. land, housing or infrastructure etc.) and a
population comprising $n$ mobile units that can migrate between these cells. We
show that with a constant metropolis area (fixed $s$), the variance and entropy
of consumed energy initially increase with $n$, reach a maximum and then
eventually diminish to zero as saturation is reached. These metrics are
indicators of the spatial mobility of the population. Under certain situations,
the variance is bounded as a quadratic function of the mean consumed energy of
the metropolis. However, when population and metropolis area are endogenous,
growth in the latter is arrested when $n\leq\frac{s}{2}\log(s)$ due to
diminished population density. Conversely, the population growth reaches
equilibrium when $n\geq {s}\log{n}$ or equivalently when the aggregate of both
over-populated and under-populated areas is large. Moreover, we also draw the
relationship between our approach and multi-scalar information, when economic
dependency between a metropolis's sub-regions is based on the entropy of
consumed energy. Finally, if the city's economic size (domestic product etc.)
is proportional to the consumed energy, then for a constant population density,
we show that the economy scales linearly with the surface area (or $s$).
"
1,229,"  Web search data are a valuable source of business and economic information.
Previous studies have utilized Google Trends web search data for economic
forecasting. We expand this work by providing algorithms to combine and
aggregate search volume data, so that the resulting data is both consistent
over time and consistent between data series. We give a brand equity example,
where Google Trends is used to analyze shopping data for 100 top ranked brands
and these data are used to nowcast economic variables. We describe the
importance of out of sample prediction and show how principal component
analysis (PCA) can be used to improve the signal to noise ratio and prevent
overfitting in nowcasting models. We give a finance example, where exploratory
data analysis and classification is used to analyze the relationship between
Google Trends searches and stock prices.
"
1,230,"  This paper illustrates how one can deduce preference from observed choices
when attention is not only limited but also random. In contrast to earlier
approaches, we introduce a Random Attention Model (RAM) where we abstain from
any particular attention formation, and instead consider a large class of
nonparametric random attention rules. Our model imposes one intuitive
condition, termed Monotonic Attention, which captures the idea that each
consideration set competes for the decision-maker's attention. We then develop
revealed preference theory within RAM and obtain precise testable implications
for observable choice probabilities. Based on these theoretical findings, we
propose econometric methods for identification, estimation, and inference of
the decision maker's preferences. To illustrate the applicability of our
results and their concrete empirical content in specific settings, we also
develop revealed preference theory and accompanying econometric methods under
additional nonparametric assumptions on the consideration set for binary choice
problems. Finally, we provide general purpose software implementation of our
estimation and inference results, and showcase their performance using
simulations.
"
1,231,"  This paper proposes a method for estimating the effect of a policy
intervention on an outcome over time. We train recurrent neural networks (RNNs)
on the history of control unit outcomes to learn a useful representation for
predicting future outcomes. The learned representation of control units is then
applied to the treated units for predicting counterfactual outcomes. RNNs are
specifically structured to exploit temporal dependencies in panel data, and are
able to learn negative and nonlinear interactions between control unit
outcomes. We apply the method to the problem of estimating the long-run impact
of U.S. homestead policy on public school spending.
"
1,232,"  We propose a new inferential methodology for dynamic economies that is robust
to misspecification of the mechanism generating frictions. Economies with
frictions are treated as perturbations of a frictionless economy that are
consistent with a variety of mechanisms. We derive a representation for the law
of motion for such economies and we characterize parameter set identification.
We derive a link from model aggregate predictions to distributional information
contained in qualitative survey data and specify conditions under which the
identified set is refined. The latter is used to semi-parametrically estimate
distortions due to frictions in macroeconomic variables. Based on these
estimates, we propose a novel test for complete models. Using consumer and
business survey data collected by the European Commission, we apply our method
to estimate distortions due to financial frictions in the Spanish economy. We
investigate the implications of these estimates for the adequacy of the
standard model of financial frictions SW-BGG (Smets and Wouters (2007),
Bernanke, Gertler, and Gilchrist (1999)).
"
1,233,"  In economics we often face a system, which intrinsically imposes a structure
of hierarchy of its components, i.e., in modelling trade accounts related to
foreign exchange or in optimization of regional air protection policy.
  A problem of reconciliation of forecasts obtained on different levels of
hierarchy has been addressed in the statistical and econometric literature for
many times and concerns bringing together forecasts obtained independently at
different levels of hierarchy.
  This paper deals with this issue in case of a hierarchical functional time
series. We present and critically discuss a state of art and indicate
opportunities of an application of these methods to a certain environment
protection problem. We critically compare the best predictor known from the
literature with our own original proposal. Within the paper we study a
macromodel describing a day and night air pollution in Silesia region divided
into five subregions.
"
1,234,"  In accordance with ""Democracy's Effect on Development: More Questions than
Answers"", we seek to carry out a study in following the description in the
'Questions for Further Study.' To that end, we studied 33 countries in the
Sub-Saharan Africa region, who all went through an election which should signal
a ""step-up"" for their democracy, one in which previously homogenous regimes
transfer power to an opposition party that fairly won the election. After doing
so, liberal-democracy indicators and democracy indicators were evaluated in the
five years prior to and after the election took place, and over that ten-year
period, we examine the data for trends. If we see positive or negative trends
over this time horizon, we are able to conclude that it was the recent increase
in the quality of their democracy which led to it. Having investigated examples
of this in depth, there seem to be three main archetypes which drive the
results. Countries with positive results to their democracy from the election
have generally positive effects on their development, countries with more
""plateau"" like results also did well, but countries for whom the descent to
authoritarianism was continued by this election found more negative results.
"
1,235,"  We consider estimation and inference on average treatment effects under
unconfoundedness conditional on the realizations of the treatment variable and
covariates. Given nonparametric smoothness and/or shape restrictions on the
conditional mean of the outcome variable, we derive estimators and confidence
intervals (CIs) that are optimal in finite samples when the regression errors
are normal with known variance. In contrast to conventional CIs, our CIs use a
larger critical value that explicitly takes into account the potential bias of
the estimator. When the error distribution is unknown, feasible versions of our
CIs are valid asymptotically, even when $\sqrt{n}$-inference is not possible
due to lack of overlap, or low smoothness of the conditional mean. We also
derive the minimum smoothness conditions on the conditional mean that are
necessary for $\sqrt{n}$-inference. When the conditional mean is restricted to
be Lipschitz with a large enough bound on the Lipschitz constant, the optimal
estimator reduces to a matching estimator with the number of matches set to
one. We illustrate our methods in an application to the National Supported Work
Demonstration.
"
1,236,"  We propose strategies to estimate and make inference on key features of
heterogeneous effects in randomized experiments. These key features include
best linear predictors of the effects using machine learning proxies, average
effects sorted by impact groups, and average characteristics of most and least
impacted units. The approach is valid in high dimensional settings, where the
effects are proxied (but not necessarily consistently estimated) by predictive
and causal machine learning methods. We post-process these proxies into
estimates of the key features. Our approach is generic, it can be used in
conjunction with penalized methods, neural networks, random forests, boosted
trees, and ensemble methods, both predictive and causal. Estimation and
inference are based on repeated data splitting to avoid overfitting and achieve
validity. We use quantile aggregation of the results across many potential
splits, in particular taking medians of p-values and medians and other
quantiles of confidence intervals. We show that quantile aggregation lowers
estimation risks over a single split procedure, and establish its principal
inferential properties. Finally, our analysis reveals ways to build provably
better machine learning proxies through causal learning: we can use the
objective functions that we develop to construct the best linear predictors of
the effects, to obtain better machine learning proxies in the initial step. We
illustrate the use of both inferential tools and causal learners with a
randomized field experiment that evaluates a combination of nudges to stimulate
demand for immunization in India.
"
1,237,"  Flexible estimation of heterogeneous treatment effects lies at the heart of
many statistical challenges, such as personalized medicine and optimal resource
allocation. In this paper, we develop a general class of two-step algorithms
for heterogeneous treatment effect estimation in observational studies. We
first estimate marginal effects and treatment propensities in order to form an
objective function that isolates the causal component of the signal. Then, we
optimize this data-adaptive objective function. Our approach has several
advantages over existing methods. From a practical perspective, our method is
flexible and easy to use: In both steps, we can use any loss-minimization
method, e.g., penalized regression, deep neural networks, or boosting;
moreover, these methods can be fine-tuned by cross validation. Meanwhile, in
the case of penalized kernel regression, we show that our method has a
quasi-oracle property: Even if the pilot estimates for marginal effects and
treatment propensities are not particularly accurate, we achieve the same error
bounds as an oracle who has a priori knowledge of these two nuisance
components. We implement variants of our approach based on penalized
regression, kernel ridge regression, and boosting in a variety of simulation
setups, and find promising performance relative to existing baselines.
"
1,238,"  We analyze Assessment Voting, a new two-round voting procedure that can be
applied to binary decisions in democratic societies. In the first round, a
randomly-selected number of citizens cast their vote on one of the two
alternatives at hand, thereby irrevocably exercising their right to vote. In
the second round, after the results of the first round have been published, the
remaining citizens decide whether to vote for one alternative or to ab- stain.
The votes from both rounds are aggregated, and the final outcome is obtained by
applying the majority rule, with ties being broken by fair randomization.
Within a costly voting framework, we show that large elec- torates will choose
the preferred alternative of the majority with high prob- ability, and that
average costs will be low. This result is in contrast with the literature on
one-round voting, which predicts either higher voting costs (when voting is
compulsory) or decisions that often do not represent the preferences of the
majority (when voting is voluntary).
"
1,239,"  We present a general framework for studying regularized estimators; such
estimators are pervasive in estimation problems wherein ""plug-in"" type
estimators are either ill-defined or ill-behaved. Within this framework, we
derive, under primitive conditions, consistency and a generalization of the
asymptotic linearity property. We also provide data-driven methods for choosing
tuning parameters that, under some conditions, achieve the aforementioned
properties. We illustrate the scope of our approach by presenting a wide range
of applications.
"
1,240,"  Transformation models are a very important tool for applied statisticians and
econometricians. In many applications, the dependent variable is transformed so
that homogeneity or normal distribution of the error holds. In this paper, we
analyze transformation models in a high-dimensional setting, where the set of
potential covariates is large. We propose an estimator for the transformation
parameter and we show that it is asymptotically normally distributed using an
orthogonalized moment condition where the nuisance functions depend on the
target parameter. In a simulation study, we show that the proposed estimator
works well in small samples. A common practice in labor economics is to
transform wage with the log-function. In this study, we test if this
transformation holds in CPS data from the United States.
"
1,241,"  This paper defines the class of $\mathcal{H}$-valued autoregressive (AR)
processes with a unit root of finite type, where $\mathcal{H}$ is an infinite
dimensional separable Hilbert space, and derives a generalization of the
Granger-Johansen Representation Theorem valid for any integration order
$d=1,2,\dots$. An existence theorem shows that the solution of an AR with a
unit root of finite type is necessarily integrated of some finite integer $d$
and displays a common trends representation with a finite number of common
stochastic trends of the type of (cumulated) bilateral random walks and an
infinite dimensional cointegrating space. A characterization theorem clarifies
the connections between the structure of the AR operators and $(i)$ the order
of integration, $(ii)$ the structure of the attractor space and the
cointegrating space, $(iii)$ the expression of the cointegrating relations, and
$(iv)$ the Triangular representation of the process. Except for the fact that
the number of cointegrating relations that are integrated of order 0 is
infinite, the representation of $\mathcal{H}$-valued ARs with a unit root of
finite type coincides with that of usual finite dimensional VARs, which
corresponds to the special case $\mathcal{H}=\mathbb{R}^p$.
"
1,242,"  Most long memory forecasting studies assume that the memory is generated by
the fractional difference operator. We argue that the most cited theoretical
arguments for the presence of long memory do not imply the fractional
difference operator, and assess the performance of the autoregressive
fractionally integrated moving average $(ARFIMA)$ model when forecasting series
with long memory generated by nonfractional processes. We find that high-order
autoregressive $(AR)$ models produce similar or superior forecast performance
than $ARFIMA$ models at short horizons. Nonetheless, as the forecast horizon
increases, the $ARFIMA$ models tend to dominate in forecast performance. Hence,
$ARFIMA$ models are well suited for forecasts of long memory processes
regardless of the long memory generating mechanism, particularly for medium and
long forecast horizons. Additionally, we analyse the forecasting performance of
the heterogeneous autoregressive ($HAR$) model which imposes restrictions on
high-order $AR$ models. We find that the structure imposed by the $HAR$ model
produces better long horizon forecasts than $AR$ models of the same order, at
the price of inferior short horizon forecasts in some cases. Our results have
implications for, among others, Climate Econometrics and Financial Econometrics
models dealing with long memory series at different forecast horizons. We show
in an example that while a short memory autoregressive moving average $(ARMA)$
model gives the best performance when forecasting the Realized Variance of the
S\&P 500 up to a month ahead, the $ARFIMA$ model gives the best performance for
longer forecast horizons.
"
1,243,"  High-dimensional linear models with endogenous variables play an increasingly
important role in recent econometric literature. In this work we allow for
models with many endogenous variables and many instrument variables to achieve
identification. Because of the high-dimensionality in the second stage,
constructing honest confidence regions with asymptotically correct coverage is
non-trivial. Our main contribution is to propose estimators and confidence
regions that would achieve that. The approach relies on moment conditions that
have an additional orthogonal property with respect to nuisance parameters.
Moreover, estimation of high-dimension nuisance parameters is carried out via
new pivotal procedures. In order to achieve simultaneously valid confidence
regions we use a multiplier bootstrap procedure to compute critical values and
establish its validity.
"
1,244,"  Player-Compatible Equilibrium (PCE) imposes cross-player restrictions on the
magnitudes of the players' ""trembles"" onto different strategies. These
restrictions capture the idea that trembles correspond to deliberate
experiments by agents who are unsure of the prevailing distribution of play.
PCE selects intuitive equilibria in a number of examples where trembling-hand
perfect equilibrium (Selten, 1975) and proper equilibrium (Myerson, 1978) have
no bite. We show that rational learning and weighted fictitious play imply our
compatibility restrictions in a steady-state setting.
"
1,245,"  Separate selling of two independent goods is shown to yield at least 62% of
the optimal revenue, and at least 73% when the goods satisfy the Myerson
regularity condition. This improves the 50% result of Hart and Nisan (2017,
originally circulated in 2012).
"
1,246,"  We introduce new inference procedures for counterfactual and synthetic
control methods for policy evaluation. We recast the causal inference problem
as a counterfactual prediction and a structural breaks testing problem. This
allows us to exploit insights from conformal prediction and structural breaks
testing to develop permutation inference procedures that accommodate modern
high-dimensional estimators, are valid under weak and easy-to-verify
conditions, and are provably robust against misspecification. Our methods work
in conjunction with many different approaches for predicting counterfactual
mean outcomes in the absence of the policy intervention. Examples include
synthetic controls, difference-in-differences, factor and matrix completion
models, and (fused) time series panel data models. Our approach demonstrates an
excellent small-sample performance in simulations and is taken to a data
application where we re-evaluate the consequences of decriminalizing indoor
prostitution. Open-source software for implementing our conformal inference
methods is available.
"
1,247,"  We propose a new variational Bayes estimator for high-dimensional copulas
with discrete, or a combination of discrete and continuous, margins. The method
is based on a variational approximation to a tractable augmented posterior, and
is faster than previous likelihood-based approaches. We use it to estimate
drawable vine copulas for univariate and multivariate Markov ordinal and mixed
time series. These have dimension $rT$, where $T$ is the number of observations
and $r$ is the number of series, and are difficult to estimate using previous
methods. The vine pair-copulas are carefully selected to allow for
heteroskedasticity, which is a feature of most ordinal time series data. When
combined with flexible margins, the resulting time series models also allow for
other common features of ordinal data, such as zero inflation, multiple modes
and under- or over-dispersion. Using six example series, we illustrate both the
flexibility of the time series copula models, and the efficacy of the
variational Bayes estimator for copulas of up to 792 dimensions and 60
parameters. This far exceeds the size and complexity of copula models for
discrete data that can be estimated using previous methods.
"
1,248,"  This paper provides estimation and inference methods for an identified set's
boundary (i.e., support function) where the selection among a very large number
of covariates is based on modern regularized tools. I characterize the boundary
using a semiparametric moment equation. Combining Neyman-orthogonality and
sample splitting ideas, I construct a root-N consistent, uniformly
asymptotically Gaussian estimator of the boundary and propose a multiplier
bootstrap procedure to conduct inference. I apply this result to the partially
linear model, the partially linear IV model and the average partial derivative
with an interval-valued outcome.
"
1,249,"  Our confidence set quantifies the statistical uncertainty from data-driven
group assignments in grouped panel models. It covers the true group memberships
jointly for all units with pre-specified probability and is constructed by
inverting many simultaneous unit-specific one-sided tests for group membership.
We justify our approach under $N, T \to \infty$ asymptotics using tools from
high-dimensional statistics, some of which we extend in this paper. We provide
Monte Carlo evidence that the confidence set has adequate coverage in finite
samples.An empirical application illustrates the use of our confidence set.
"
1,250,"  Empirical researchers are increasingly faced with rich data sets containing
many controls or instrumental variables, making it essential to choose an
appropriate approach to variable selection. In this paper, we provide results
for valid inference after post- or orthogonal $L_2$-Boosting is used for
variable selection. We consider treatment effects after selecting among many
control variables and instrumental variable models with potentially many
instruments. To achieve this, we establish new results for the rate of
convergence of iterated post-$L_2$-Boosting and orthogonal $L_2$-Boosting in a
high-dimensional setting similar to Lasso, i.e., under approximate sparsity
without assuming the beta-min condition. These results are extended to the 2SLS
framework and valid inference is provided for treatment effect analysis. We
give extensive simulation results for the proposed methods and compare them
with Lasso. In an empirical application, we construct efficient IVs with our
proposed methods to estimate the effect of pre-merger overlap of bank branch
networks in the US on the post-merger stock returns of the acquirer bank.
"
1,251,"  This paper investigates the impacts of major natural resource discoveries
since 1960 on life expectancy in the nations that they were resource poor prior
to the discoveries. Previous literature explains the relation between nations
wealth and life expectancy, but it has been silent about the impacts of
resource discoveries on life expectancy. We attempt to fill this gap in this
study. An important advantage of this study is that as the previous researchers
argued resource discovery could be an exogenous variable. We use longitudinal
data from 1960 to 2014 and we apply three modern empirical methods including
Difference-in-Differences, Event studies, and Synthetic Control approach, to
investigate the main question of the research which is 'how resource
discoveries affect life expectancy?'. The findings show that resource
discoveries in Ecuador, Yemen, Oman, and Equatorial Guinea have positive and
significant impacts on life expectancy, but the effects for the European
countries are mostly negative.
"
1,252,"  This document collects the lecture notes from my mini-course ""Complexity
Theory, Game Theory, and Economics,"" taught at the Bellairs Research Institute
of McGill University, Holetown, Barbados, February 19--23, 2017, as the 29th
McGill Invitational Workshop on Computational Complexity.
  The goal of this mini-course is twofold: (i) to explain how complexity theory
has helped illuminate several barriers in economics and game theory; and (ii)
to illustrate how game-theoretic questions have led to new and interesting
complexity theory, including recent several breakthroughs. It consists of two
five-lecture sequences: the Solar Lectures, focusing on the communication and
computational complexity of computing equilibria; and the Lunar Lectures,
focusing on applications of complexity theory in game theory and economics. No
background in game theory is assumed.
"
1,253,"  In this paper, a new and convenient $\chi^2$ wald test based on MCMC outputs
is proposed for hypothesis testing. The new statistic can be explained as MCMC
version of Wald test and has several important advantages that make it very
convenient in practical applications. First, it is well-defined under improper
prior distributions and avoids Jeffrey-Lindley's paradox. Second, it's
asymptotic distribution can be proved to follow the $\chi^2$ distribution so
that the threshold values can be easily calibrated from this distribution.
Third, it's statistical error can be derived using the Markov chain Monte Carlo
(MCMC) approach. Fourth, most importantly, it is only based on the posterior
MCMC random samples drawn from the posterior distribution. Hence, it is only
the by-product of the posterior outputs and very easy to compute. In addition,
when the prior information is available, the finite sample theory is derived
for the proposed test statistic. At last, the usefulness of the test is
illustrated with several applications to latent variable models widely used in
economics and finance.
"
1,254,"  This paper compares alternative univariate versus multivariate models,
frequentist versus Bayesian autoregressive and vector autoregressive
specifications, for hourly day-ahead electricity prices, both with and without
renewable energy sources. The accuracy of point and density forecasts are
inspected in four main European markets (Germany, Denmark, Italy and Spain)
characterized by different levels of renewable energy power generation. Our
results show that the Bayesian VAR specifications with exogenous variables
dominate other multivariate and univariate specifications, in terms of both
point and density forecasting.
"
1,255,"  An intensive research sprang up for stochastic methods in insurance during
the past years. To meet all future claims rising from policies, it is requisite
to quantify the outstanding loss liabilities. Loss reserving methods based on
aggregated data from run-off triangles are predominantly used to calculate the
claims reserves. Conventional reserving techniques have some disadvantages:
loss of information from the policy and the claim's development due to the
aggregation, zero or negative cells in the triangle; usually small number of
observations in the triangle; only few observations for recent accident years;
and sensitivity to the most recent paid claims.
  To overcome these dilemmas, granular loss reserving methods for individual
claim-by-claim data will be derived. Reserves' estimation is a crucial part of
the risk valuation process, which is now a front burner in economics. Since
there is a growing demand for prediction of total reserves for different types
of claims or even multiple lines of business, a time-varying copula framework
for granular reserving will be established.
"
1,256,"  We introduce the simulation tool SABCEMM (Simulator for Agent-Based
Computational Economic Market Models) for agent-based computational economic
market (ABCEM) models. Our simulation tool is implemented in C++ and we can
easily run ABCEM models with several million agents. The object-oriented
software design enables the isolated implementation of building blocks for
ABCEM models, such as agent types and market mechanisms. The user can design
and compare ABCEM models in a unified environment by recombining existing
building blocks using the XML-based SABCEMM configuration file. We introduce an
abstract ABCEM model class which our simulation tool is built upon.
Furthermore, we present the software architecture as well as computational
aspects of SABCEMM. Here, we focus on the efficiency of SABCEMM with respect to
the run time of our simulations. We show the great impact of different random
number generators on the run time of ABCEM models. The code and documentation
is published on GitHub at https://github.com/SABCEMM/SABCEMM, such that all
results can be reproduced by the reader.
"
1,257,"  The purpose of this article is to propose a new ""theory,"" the Strategic
Analysis of Financial Markets (SAFM) theory, that explains the operation of
financial markets using the analytical perspective of an enlightened gambler.
The gambler understands that all opportunities for superior performance arise
from suboptimal decisions by humans, but understands also that knowledge of
human decision making alone is not enough to understand market behavior --- one
must still model how those decisions lead to market prices. Thus are there
three parts to the model: gambling theory, human decision making, and strategic
problem solving. A new theory is necessary because at this writing in 2017,
there is no theory of financial markets acceptable to both practitioners and
theorists. Theorists' efficient market theory, for example, cannot explain
bubbles and crashes nor the exceptional returns of famous investors and
speculators such as Warren Buffett and George Soros. At the same time, a new
theory must be sufficiently quantitative, explain market ""anomalies"" and
provide predictions in order to satisfy theorists. It is hoped that the SAFM
framework will meet these requirements.
"
1,258,"  Agents learn about a changing state using private signals and their
neighbors' past estimates of the state. We present a model in which Bayesian
agents in equilibrium use neighbors' estimates simply by taking weighted sums
with time-invariant weights. The dynamics thus parallel those of the tractable
DeGroot model of learning in networks, but arise as an equilibrium outcome
rather than a behavioral assumption. We examine whether information aggregation
is nearly optimal as neighborhoods grow large. A key condition for this is
signal diversity: each individual's neighbors have private signals that not
only contain independent information, but also have sufficiently different
distributions. Without signal diversity $\unicode{x2013}$ e.g., if private
signals are i.i.d. $\unicode{x2013}$ learning is suboptimal in all networks and
highly inefficient in some. Turning to social influence, we find it is much
more sensitive to one's signal quality than to one's number of neighbors, in
contrast to standard models with exogenous updating rules.
"
1,259,"  This paper studies the problem of stochastic dynamic pricing and energy
management policy for electric vehicle (EV) charging service providers. In the
presence of renewable energy integration and energy storage system, EV charging
service providers must deal with multiple uncertainties --- charging demand
volatility, inherent intermittency of renewable energy generation, and
wholesale electricity price fluctuation. The motivation behind our work is to
offer guidelines for charging service providers to determine proper charging
prices and manage electricity to balance the competing objectives of improving
profitability, enhancing customer satisfaction, and reducing impact on power
grid in spite of these uncertainties. We propose a new metric to assess the
impact on power grid without solving complete power flow equations. To protect
service providers from severe financial losses, a safeguard of profit is
incorporated in the model. Two algorithms --- stochastic dynamic programming
(SDP) algorithm and greedy algorithm (benchmark algorithm) --- are applied to
derive the pricing and electricity procurement policy. A Pareto front of the
multiobjective optimization is derived. Simulation results show that using SDP
algorithm can achieve up to 7% profit gain over using greedy algorithm.
Additionally, we observe that the charging service provider is able to reshape
spatial-temporal charging demands to reduce the impact on power grid via
pricing signals.
"
1,260,"  This paper studies the problem of multi-stage placement of electric vehicle
(EV) charging stations with incremental EV penetration rates. A nested logit
model is employed to analyze the charging preference of the individual consumer
(EV owner), and predict the aggregated charging demand at the charging
stations. The EV charging industry is modeled as an oligopoly where the entire
market is dominated by a few charging service providers (oligopolists). At the
beginning of each planning stage, an optimal placement policy for each service
provider is obtained through analyzing strategic interactions in a Bayesian
game. To derive the optimal placement policy, we consider both the
transportation network graph and the electric power network graph. A simulation
software --- The EV Virtual City 1.0 --- is developed using Java to investigate
the interactions among the consumers (EV owner), the transportation network
graph, the electric power network graph, and the charging stations. Through a
series of experiments using the geographic and demographic data from the city
of San Pedro District of Los Angeles, we show that the charging station
placement is highly consistent with the heatmap of the traffic flow. In
addition, we observe a spatial economic phenomenon that service providers
prefer clustering instead of separation in the EV charging market.
"
1,261,"  This paper presents a multi-stage approach to the placement of charging
stations under the scenarios of different electric vehicle (EV) penetration
rates. The EV charging market is modeled as the oligopoly. A consumer behavior
based approach is applied to forecast the charging demand of the charging
stations using a nested logit model. The impacts of both the urban road network
and the power grid network on charging station planning are also considered. At
each planning stage, the optimal station placement strategy is derived through
solving a Bayesian game among the service providers. To investigate the
interplay of the travel pattern, the consumer behavior, urban road network,
power grid network, and the charging station placement, a simulation platform
(The EV Virtual City 1.0) is developed using Java on Repast.We conduct a case
study in the San Pedro District of Los Angeles by importing the geographic and
demographic data of that region into the platform. The simulation results
demonstrate a strong consistency between the charging station placement and the
traffic flow of EVs. The results also reveal an interesting phenomenon that
service providers prefer clustering instead of spatial separation in this
oligopoly market.
"
1,262,"  To determine the welfare implications of price changes in demand data, we
introduce a revealed preference relation over prices. We show that the absence
of cycles in this relation characterizes a consumer who trades off the utility
of consumption against the disutility of expenditure. Our model can be applied
whenever a consumer's demand over a strict subset of all available goods is
being analyzed; it can also be extended to settings with discrete goods and
nonlinear prices. To illustrate its use, we apply our model to a single-agent
data set and to a data set with repeated cross-sections. We develop a novel
test of linear hypotheses on partially identified parameters to estimate the
proportion of the population who are revealed better off due to a price change
in the latter application. This new technique can be used for nonparametric
counterfactual analysis more broadly.
"
1,263,"  This paper presents a dynamic pricing and energy management framework for
electric vehicle (EV) charging service providers. To set the charging prices,
the service providers faces three uncertainties: the volatility of wholesale
electricity price, intermittent renewable energy generation, and
spatial-temporal EV charging demand. The main objective of our work here is to
help charging service providers to improve their total profits while enhancing
customer satisfaction and maintaining power grid stability, taking into account
those uncertainties. We employ a linear regression model to estimate the EV
charging demand at each charging station, and introduce a quantitative measure
for customer satisfaction. Both the greedy algorithm and the dynamic
programming (DP) algorithm are employed to derive the optimal charging prices
and determine how much electricity to be purchased from the wholesale market in
each planning horizon. Simulation results show that DP algorithm achieves an
increased profit (up to 9%) compared to the greedy algorithm (the benchmark
algorithm) under certain scenarios. Additionally, we observe that the
integration of a low-cost energy storage into the system can not only improve
the profit, but also smooth out the charging price fluctuation, protecting the
end customers from the volatile wholesale market.
"
1,264,"  In this paper we estimate a Bayesian vector autoregressive model with factor
stochastic volatility in the error term to assess the effects of an uncertainty
shock in the Euro area. This allows us to treat macroeconomic uncertainty as a
latent quantity during estimation. Only a limited number of contributions to
the literature estimate uncertainty and its macroeconomic consequences jointly,
and most are based on single country models. We analyze the special case of a
shock restricted to the Euro area, where member states are highly related by
construction. We find significant results of a decrease in real activity for
all countries over a period of roughly a year following an uncertainty shock.
Moreover, equity prices, short-term interest rates and exports tend to decline,
while unemployment levels increase. Dynamic responses across countries differ
slightly in magnitude and duration, with Ireland, Slovakia and Greece
exhibiting different reactions for some macroeconomic fundamentals.
"
1,265,"  We report a new result on lotteries --- that a well-funded syndicate has a
purely mechanical strategy to achieve expected returns of 10\% to 25\% in an
equiprobable lottery with no take and no carryover pool. We prove that an
optimal strategy (Nash equilibrium) in a game between the syndicate and other
players consists of betting one of each ticket (the ""trump ticket""), and extend
that result to proportional ticket selection in non-equiprobable lotteries. The
strategy can be adjusted to accommodate lottery taxes and carryover pools. No
""irrationality"" need be involved for the strategy to succeed --- it requires
only that a large group of non-syndicate bettors each choose a few tickets
independently.
"
1,266,"  Despite its unusual payout structure, the Canadian 6/49 Lotto is one of the
few government sponsored lotteries that has the potential for a favorable
strategy we call ""buying the pot."" By buying the pot we mean that a syndicate
buys each ticket in the lottery, ensuring that it holds a jackpot winner. We
assume that the other bettors independently buy small numbers of tickets. This
paper presents (1) a formula for the syndicate's expected return, (2)
conditions under which buying the pot produces a significant positive expected
return, and (3) the implications of these findings for lottery design.
"
1,267,"  This article is a prologue to the article ""Why Markets are Inefficient: A
Gambling 'Theory' of Financial Markets for Practitioners and Theorists."" It
presents important background for that article --- why gambling is important,
even necessary, for real-world traders --- the reason for the superiority of
the strategic/gambling approach to the competing market ideologies of market
fundamentalism and the scientific approach --- and its potential to uncover
profitable trading systems. Much of this article was drawn from Chapter 1 of
the book ""The Strategic Analysis of Financial Markets (in 2 volumes)"" World
Scientific, 2017.
"
1,268,"  We propose a robust implementation of the Nerlove--Arrow model using a
Bayesian structural time series model to explain the relationship between
advertising expenditures of a country-wide fast-food franchise network with its
weekly sales. Thanks to the flexibility and modularity of the model, it is well
suited to generalization to other markets or situations. Its Bayesian nature
facilitates incorporating \emph{a priori} information (the manager's views),
which can be updated with relevant data. This aspect of the model will be used
to present a strategy of budget scheduling across time and channels.
"
1,269,"  Dynamic Discrete Choice Models (DDCMs) are important in the structural
estimation literature. Since the structural errors are practically always
continuous and unbounded in nature, researchers often use the expected value
function. The idea to solve for the expected value function made solution more
practical and estimation feasible. However, as we show in this paper, the
expected value function is impractical compared to an alternative: the
integrated (ex ante) value function. We provide brief descriptions of the
inefficacy of the former, and benchmarks on actual problems with varying
cardinality of the state space and number of decisions. Though the two
approaches solve the same problem in theory, the benchmarks support the claim
that the integrated value function is preferred in practice.
"
1,270,"  In a continuous-time setting where a risk-averse agent controls the drift of
an output process driven by a Brownian motion, optimal contracts are linear in
the terminal output; this result is well-known in a setting with moral hazard
and -under stronger assumptions - adverse selection. We show that this result
continues to hold when in addition reservation utilities are type-dependent.
This type of problem occurs in the study of optimal compensation problems
involving competing principals.
"
1,271,"  This paper develops a new model and estimation procedure for panel data that
allows us to identify heterogeneous structural breaks. We model individual
heterogeneity using a grouped pattern. For each group, we allow common
structural breaks in the coefficients. However, the number, timing, and size of
these breaks can differ across groups. We develop a hybrid estimation procedure
of the grouped fixed effects approach and adaptive group fused Lasso. We show
that our method can consistently identify the latent group structure, detect
structural breaks, and estimate the regression parameters. Monte Carlo results
demonstrate the good performance of the proposed method in finite samples. An
empirical application to the relationship between income and democracy
illustrates the importance of considering heterogeneous structural breaks.
"
1,272,"  We characterize common assumption of rationality of 2-person games within an
incomplete information framework. We use the lexicographic model with
incomplete information and show that a belief hierarchy expresses common
assumption of rationality within a complete information framework if and only
if there is a belief hierarchy within the corresponding incomplete information
framework that expresses common full belief in caution, rationality, every good
choice is supported, and prior belief in the original utility functions.
"
1,273,"  This paper introduces estimation methods for grouped latent heterogeneity in
panel data quantile regression. We assume that the observed individuals come
from a heterogeneous population with a finite number of types. The number of
types and group membership is not assumed to be known in advance and is
estimated by means of a convex optimization problem. We provide conditions
under which group membership is estimated consistently and establish asymptotic
normality of the resulting estimators. Simulations show that the method works
well in finite samples when T is reasonably large. To illustrate the proposed
methodology we study the effects of the adoption of Right-to-Carry concealed
weapon laws on violent crime rates using panel data of 51 U.S. states from 1977
- 2010.
"
1,274,"  Many applications involve a censored dependent variable and an endogenous
independent variable. Chernozhukov et al. (2015) introduced a censored quantile
instrumental variable estimator (CQIV) for use in those applications, which has
been applied by Kowalski (2016), among others. In this article, we introduce a
Stata command, cqiv, that simplifes application of the CQIV estimator in Stata.
We summarize the CQIV estimator and algorithm, we describe the use of the cqiv
command, and we provide empirical examples.
"
1,275,"  The ongoing rapid development of the e-commercial and interest-base websites
make it more pressing to evaluate objects' accurate quality before
recommendation by employing an effective reputation system. The objects'
quality are often calculated based on their historical information, such as
selected records or rating scores, to help visitors to make decisions before
watching, reading or buying. Usually high quality products obtain a higher
average ratings than low quality products regardless of rating biases or
errors. However many empirical cases demonstrate that consumers may be misled
by rating scores added by unreliable users or deliberate tampering. In this
case, users' reputation, i.e., the ability to rating trustily and precisely,
make a big difference during the evaluating process. Thus, one of the main
challenges in designing reputation systems is eliminating the effects of users'
rating bias on the evaluation results. To give an objective evaluation of each
user's reputation and uncover an object's intrinsic quality, we propose an
iterative balance (IB) method to correct users' rating biases. Experiments on
two online video-provided Web sites, namely MovieLens and Netflix datasets,
show that the IB method is a highly self-consistent and robust algorithm and it
can accurately quantify movies' actual quality and users' stability of rating.
Compared with existing methods, the IB method has higher ability to find the
""dark horses"", i.e., not so popular yet good movies, in the Academy Awards.
"
1,276,"  We present a mixed multinomial logit (MNL) model, which leverages the
truncated stick-breaking process representation of the Dirichlet process as a
flexible nonparametric mixing distribution. The proposed model is a Dirichlet
process mixture model and accommodates discrete representations of
heterogeneity, like a latent class MNL model. Yet, unlike a latent class MNL
model, the proposed discrete choice model does not require the analyst to fix
the number of mixture components prior to estimation, as the complexity of the
discrete mixing distribution is inferred from the evidence. For posterior
inference in the proposed Dirichlet process mixture model of discrete choice,
we derive an expectation maximisation algorithm. In a simulation study, we
demonstrate that the proposed model framework can flexibly capture
differently-shaped taste parameter distributions. Furthermore, we empirically
validate the model framework in a case study on motorists' route choice
preferences and find that the proposed Dirichlet process mixture model of
discrete choice outperforms a latent class MNL model and mixed MNL models with
common parametric mixing distributions in terms of both in-sample fit and
out-of-sample predictive ability. Compared to extant modelling approaches, the
proposed discrete choice model substantially abbreviates specification
searches, as it relies on less restrictive parametric assumptions and does not
require the analyst to specify the complexity of the discrete mixing
distribution prior to estimation.
"
1,277,"  In this paper we forecast daily returns of crypto-currencies using a wide
variety of different econometric models. To capture salient features commonly
observed in financial time series like rapid changes in the conditional
variance, non-normality of the measurement errors and sharply increasing
trends, we develop a time-varying parameter VAR with t-distributed measurement
errors and stochastic volatility. To control for overparameterization, we rely
on the Bayesian literature on shrinkage priors that enables us to shrink
coefficients associated with irrelevant predictors and/or perform model
specification in a flexible manner. Using around one year of daily data we
perform a real-time forecasting exercise and investigate whether any of the
proposed models is able to outperform the naive random walk benchmark. To
assess the economic relevance of the forecasting gains produced by the proposed
models we moreover run a simple trading exercise.
"
1,278,"  The primary goal of this study is doing a meta-analysis research on two
groups of published studies. First, the ones that focus on the evaluation of
the United States Department of Agriculture (USDA) forecasts and second, the
ones that evaluate the market reactions to the USDA forecasts. We investigate
four questions. 1) How the studies evaluate the accuracy of the USDA forecasts?
2) How they evaluate the market reactions to the USDA forecasts? 3) Is there
any heterogeneity in the results of the mentioned studies? 4) Is there any
publication bias? About the first question, while some researchers argue that
the forecasts are unbiased, most of them maintain that they are biased,
inefficient, not optimal, or not rational. About the second question, while a
few studies claim that the forecasts are not newsworthy, most of them maintain
that they are newsworthy, provide useful information, and cause market
reactions. About the third and the fourth questions, based on our findings,
there are some clues that the results of the studies are heterogeneous, but we
didn't find enough evidences of publication bias.
"
1,279,"  The major perspective of this paper is to provide more evidence into the
empirical determinants of capital structure adjustment in different
macroeconomics states by focusing and discussing the relative importance of
firm-specific and macroeconomic characteristics from an alternative scope in
U.S. This study extends the empirical research on the topic of capital
structure by focusing on a quantile regression method to investigate the
behavior of firm-specific characteristics and macroeconomic variables across
all quantiles of distribution of leverage (total debt, long-terms debt and
short-terms debt). Thus, based on a partial adjustment model, we find that
long-term and short-term debt ratios varying regarding their partial adjustment
speeds; the short-term debt raises up while the long-term debt ratio slows down
for same periods.
"
1,280,"  The fractional difference operator remains to be the most popular mechanism
to generate long memory due to the existence of efficient algorithms for their
simulation and forecasting. Nonetheless, there is no theoretical argument
linking the fractional difference operator with the presence of long memory in
real data. In this regard, one of the most predominant theoretical explanations
for the presence of long memory is cross-sectional aggregation of persistent
micro units. Yet, the type of processes obtained by cross-sectional aggregation
differs from the one due to fractional differencing. Thus, this paper develops
fast algorithms to generate and forecast long memory by cross-sectional
aggregation. Moreover, it is shown that the antipersistent phenomenon that
arises for negative degrees of memory in the fractional difference literature
is not present for cross-sectionally aggregated processes. Pointedly, while the
autocorrelations for the fractional difference operator are negative for
negative degrees of memory by construction, this restriction does not apply to
the cross-sectional aggregated scheme. We show that this has implications for
long memory tests in the frequency domain, which will be misspecified for
cross-sectionally aggregated processes with negative degrees of memory.
Finally, we assess the forecast performance of high-order $AR$ and $ARFIMA$
models when the long memory series are generated by cross-sectional
aggregation. Our results are of interest to practitioners developing forecasts
of long memory variables like inflation, volatility, and climate data, where
aggregation may be the source of long memory.
"
1,281,"  Markov regime switching models have been used in numerous empirical studies
in economics and finance. However, the asymptotic distribution of the
likelihood ratio test statistic for testing the number of regimes in Markov
regime switching models has been an unresolved problem. This paper derives the
asymptotic distribution of the likelihood ratio test statistic for testing the
null hypothesis of $M_0$ regimes against the alternative hypothesis of $M_0 +
1$ regimes for any $M_0 \geq 1$ both under the null hypothesis and under local
alternatives. We show that the contiguous alternatives converge to the null
hypothesis at a rate of $n^{-1/8}$ in regime switching models with normal
density. The asymptotic validity of the parametric bootstrap is also
established.
"
1,282,"  This paper extends endogenous economic growth models to incorporate knowledge
externality. We explores whether spatial knowledge spillovers among regions
exist, whether spatial knowledge spillovers promote regional innovative
activities, and whether external knowledge spillovers affect the evolution of
regional innovations in the long run. We empirically verify the theoretical
results through applying spatial statistics and econometric model in the
analysis of panel data of 31 regions in China. An accurate estimate of the
range of knowledge spillovers is achieved and the convergence of regional
knowledge growth rate is found, with clear evidences that developing regions
benefit more from external knowledge spillovers than developed regions.
"
1,283,"  Since exchange economy considerably varies in the market assets, asset prices
have become an attractive research area for investigating and modeling
ambiguous and uncertain information in today markets. This paper proposes a new
generative uncertainty mechanism based on the Bayesian Inference and
Correntropy (BIC) technique for accurately evaluating asset pricing in markets.
This technique examines the potential processes of risk, ambiguity, and
variations of market information in a controllable manner. We apply the new BIC
technique to a consumption asset-pricing model in which the consumption
variations are modeled using the Bayesian network model with observing the
dynamics of asset pricing phenomena in the data. These dynamics include the
procyclical deviations of price, the countercyclical deviations of equity
premia and equity volatility, the leverage impact and the mean reversion of
excess returns. The key findings reveal that the precise modeling of asset
information can estimate price changes in the market effectively.
"
1,284,"  This paper analyzes consumer choices over lunchtime restaurants using data
from a sample of several thousand anonymous mobile phone users in the San
Francisco Bay Area. The data is used to identify users' approximate typical
morning location, as well as their choices of lunchtime restaurants. We build a
model where restaurants have latent characteristics (whose distribution may
depend on restaurant observables, such as star ratings, food category, and
price range), each user has preferences for these latent characteristics, and
these preferences are heterogeneous across users. Similarly, each item has
latent characteristics that describe users' willingness to travel to the
restaurant, and each user has individual-specific preferences for those latent
characteristics. Thus, both users' willingness to travel and their base utility
for each restaurant vary across user-restaurant pairs. We use a Bayesian
approach to estimation. To make the estimation computationally feasible, we
rely on variational inference to approximate the posterior distribution, as
well as stochastic gradient descent as a computational approach. Our model
performs better than more standard competing models such as multinomial logit
and nested logit models, in part due to the personalization of the estimates.
We analyze how consumers re-allocate their demand after a restaurant closes to
nearby restaurants versus more distant restaurants with similar
characteristics, and we compare our predictions to actual outcomes. Finally, we
show how the model can be used to analyze counterfactual questions such as what
type of restaurant would attract the most consumers in a given location.
"
1,285,"  We first show (1) the importance of investigating health expenditure process
using the order two Markov chain model, rather than the standard order one
model, which is widely used in the literature. Markov chain of order two is the
minimal framework that is capable of distinguishing those who experience a
certain health expenditure level for the first time from those who have been
experiencing that or other levels for some time. In addition, using the model
we show (2) that the probability of encountering a health shock first de-
creases until around age 10, and then increases with age, particularly, after
age 40, (3) that health shock distributions among different age groups do not
differ until their percentiles reach the median range, but that above the
median the health shock distributions of older age groups gradually start to
first-order dominate those of younger groups, and (4) that the persistency of
health shocks also shows a U-shape in relation to age.
"
1,286,"  We define a modification of the standard Kripke model, called the ordered
Kripke model, by introducing a linear order on the set of accessible states of
each state. We first show this model can be used to describe the lexicographic
belief hierarchy in epistemic game theory, and perfect rationalizability can be
characterized within this model. Then we show that each ordered Kripke model is
the limit of a sequence of standard probabilistic Kripke models with a modified
(common) belief operator, in the senses of structure and the
(epsilon-)permissibilities characterized within them.
"
1,287,"  We consider identification and estimation of nonseparable sample selection
models with censored selection rules. We employ a control function approach and
discuss different objects of interest based on (1) local effects conditional on
the control function, and (2) global effects obtained from integration over
ranges of values of the control function. We derive the conditions for the
identification of these different objects and suggest strategies for
estimation. Moreover, we provide the associated asymptotic theory. These
strategies are illustrated in an empirical investigation of the determinants of
female wages in the United Kingdom.
"
1,288,"  We study the problem of fairly allocating a set of indivisible goods among
agents with additive valuations. The extent of fairness of an allocation is
measured by its Nash social welfare, which is the geometric mean of the
valuations of the agents for their bundles. While the problem of maximizing
Nash social welfare is known to be APX-hard in general, we study the
effectiveness of simple, greedy algorithms in solving this problem in two
interesting special cases.
  First, we show that a simple, greedy algorithm provides a 1.061-approximation
guarantee when agents have identical valuations, even though the problem of
maximizing Nash social welfare remains NP-hard for this setting. Second, we
show that when agents have binary valuations over the goods, an exact solution
(i.e., a Nash optimal allocation) can be found in polynomial time via a greedy
algorithm. Our results in the binary setting extend to provide novel, exact
algorithms for optimizing Nash social welfare under concave valuations.
Notably, for the above mentioned scenarios, our techniques provide a simple
alternative to several of the existing, more sophisticated techniques for this
problem such as constructing equilibria of Fisher markets or using real stable
polynomials.
"
1,289,"  We test the existence of a neighborhood based peer effect around
participation in an incentive based conservation program called `Water Smart
Landscapes' (WSL) in the city of Las Vegas, Nevada. We use 15 years of
geo-coded daily records of WSL program applications and approvals compiled by
the Southern Nevada Water Authority and Clark County Tax Assessors rolls for
home characteristics. We use this data to test whether a spatially mediated
peer effect can be observed in WSL participation likelihood at the household
level. We show that epidemic spreading models provide more flexibility in
modeling assumptions, and also provide one mechanism for addressing problems
associated with correlated unobservables than hazards models which can also be
applied to address the same questions. We build networks of neighborhood based
peers for 16 randomly selected neighborhoods in Las Vegas and test for the
existence of a peer based influence on WSL participation by using a
Susceptible-Exposed-Infected-Recovered epidemic spreading model (SEIR), in
which a home can become infected via autoinfection or through contagion from
its infected neighbors. We show that this type of epidemic model can be
directly recast to an additive-multiplicative hazard model, but not to purely
multiplicative one. Using both inference and prediction approaches we find
evidence of peer effects in several Las Vegas neighborhoods.
"
1,290,"  Why women avoid participating in a competition and how can we encourage them
to participate in it? In this paper, we investigate how social image concerns
affect women's decision to compete. We first construct a theoretical model and
show that participating in a competition, even under affirmative action
policies favoring women, is costly for women under public observability since
it deviates from traditional female gender norms, resulting in women's low
appearance in competitive environments. We propose and theoretically show that
introducing prosocial incentives in the competitive environment is effective
and robust to public observability since (i) it induces women who are
intrinsically motivated by prosocial incentives to the competitive environment
and (ii) it makes participating in a competition not costly for women from
social image point of view. We conduct a laboratory experiment where we
randomly manipulate the public observability of decisions to compete and test
our theoretical predictions. The results of the experiment are fairly
consistent with our theoretical predictions. We suggest that when designing
policies to promote gender equality in competitive environments, using
prosocial incentives through company philanthropy or other social
responsibility policies, either as substitutes or as complements to traditional
affirmative action policies, could be promising.
"
1,291,"  The rational choice theory is based on this idea that people rationally
pursue goals for increasing their personal interests. In most conditions, the
behavior of an actor is not independent of the person and others' behavior.
Here, we present a new concept of rational choice as a hyper-rational choice
which in this concept, the actor thinks about profit or loss of other actors in
addition to his personal profit or loss and then will choose an action which is
desirable to him. We implement the hyper-rational choice to generalize and
expand the game theory. Results of this study will help to model the behavior
of people considering environmental conditions, the kind of behavior
interactive, valuation system of itself and others and system of beliefs and
internal values of societies. Hyper-rationality helps us understand how human
decision makers behave in interactive decisions.
"
1,292,"  We develop a new VAR model for structural analysis with mixed-frequency data.
The MIDAS-SVAR model allows to identify structural dynamic links exploiting the
information contained in variables sampled at different frequencies. It also
provides a general framework to test homogeneous frequency-based
representations versus mixed-frequency data models. A set of Monte Carlo
experiments suggests that the test performs well both in terms of size and
power. The MIDAS-SVAR is then used to study how monetary policy and financial
market volatility impact on the dynamics of gross capital inflows to the US.
While no relation is found when using standard quarterly data, exploiting the
variability present in the series within the quarter shows that the effect of
an interest rate shock is greater the longer the time lag between the month of
the shock and the end of the quarter
"
1,293,"  We analyzed 2012 and 2016 YouGov pre-election polls in order to understand
how different population groups voted in the 2012 and 2016 elections. We broke
the data down by demographics and state. We display our findings with a series
of graphs and maps. The R code associated with this project is available at
https://github.com/rtrangucci/mrp_2016_election/.
"
1,294,"  In previous studies of spatial public goods game, each player is able to
establish a group. However, in real life, some players cannot successfully
organize groups for various reasons. In this paper, we propose a mechanism of
reputation-driven group formation, in which groups can only be organized by
players whose reputation reaches or exceeds a threshold. We define a player's
reputation as the frequency of cooperation in the last $T$ time steps. We find
that the highest cooperation level can be obtained when groups are only
established by pure cooperators who always cooperate in the last $T$ time
steps. Effects of the memory length $T$ on cooperation are also studied.
"
1,295,"  The development and deployment of matching procedures that incentivize
truthful preference reporting is considered one of the major successes of
market design research. In this study, we test the degree to which these
procedures succeed in eliminating preference misrepresentation. We administered
an online experiment to 1,714 medical students immediately after their
participation in the medical residency match--a leading field application of
strategy-proof market design. When placed in an analogous, incentivized
matching task, we find that 23% of participants misrepresent their preferences.
We explore the factors that predict preference misrepresentation, including
cognitive ability, strategic positioning, overconfidence, expectations, advice,
and trust. We discuss the implications of this behavior for the design of
allocation mechanisms and the social welfare in markets that use them.
"
1,296,"  This study proposes a mixed logit model with multivariate nonparametric
finite mixture distributions. The support of the distribution is specified as a
high-dimensional grid over the coefficient space, with equal or unequal
intervals between successive points along the same dimension; the location of
each point on the grid and the probability mass at that point are model
parameters that need to be estimated. The framework does not require the
analyst to specify the shape of the distribution prior to model estimation, but
can approximate any multivariate probability distribution function to any
arbitrary degree of accuracy. The grid with unequal intervals, in particular,
offers greater flexibility than existing multivariate nonparametric
specifications, while requiring the estimation of a small number of additional
parameters. An expectation maximization algorithm is developed for the
estimation of these models. Multiple synthetic datasets and a case study on
travel mode choice behavior are used to demonstrate the value of the model
framework and estimation algorithm. Compared to extant models that incorporate
random taste heterogeneity through continuous mixture distributions, the
proposed model provides better out-of-sample predictive ability. Findings
reveal significant differences in willingness to pay measures between the
proposed model and extant specifications. The case study further demonstrates
the ability of the proposed model to endogenously recover patterns of attribute
non-attendance and choice set formation.
"
1,297,"  Consumers are creatures of habit, often periodic, tied to work, shopping and
other schedules. We analyzed one month of data from the world's largest
bike-sharing company to elicit demand behavioral cycles, initially using models
from animal tracking that showed large customers fit an Ornstein-Uhlenbeck
model with demand peaks at periodicities of 7, 12, 24 hour and 7-days. Lorenz
curves of bicycle demand showed that the majority of customer usage was
infrequent, and demand cycles from time-series models would strongly overfit
the data yielding unreliable models. Analysis of thresholded wavelets for the
space-time tensor of bike-sharing contracts was able to compress the data into
a 56-coefficient model with little loss of information, suggesting that
bike-sharing demand behavior is exceptionally strong and regular. Improvements
to predicted demand could be made by adjusting for 'noise' filtered by our
model from air quality and weather information and demand from infrequent
riders.
"
1,298,"  To what extent, hiring incentives targeting a specific group of vulnerable
unemployed (i.e. long term unemployed) are more effective, with respect to
generalised incentives (without a definite target), to increase hirings of the
targeted group? Are generalized incentives able to influence hirings of the
vulnerable group? Do targeted policies have negative side effects too important
to accept them? Even though there is a huge literature on hiring subsidies,
these questions remained unresolved. We tried to answer them, comparing the
impact of two similar hiring policies, one oriented towards a target group and
one generalised, implemented on the italian labour market. We used
administrative data on job contracts, and counterfactual analysis methods. The
targeted policy had a positive and significant impact, while the generalized
policy didn't have a significant impact on the vulnerable group. Moreover, we
concluded the targeted policy didn't have any indirect negative side effect.
"
1,299,"  Cryptocurrencies return cross-predictability and technological similarity
yield information on risk propagation and market segmentation. To investigate
these effects, we build a time-varying network for cryptocurrencies, based on
the evolution of return cross-predictability and technological similarities. We
develop a dynamic covariate-assisted spectral clustering method to consistently
estimate the latent community structure of cryptocurrencies network that
accounts for both sets of information. We demonstrate that investors can
achieve better risk diversification by investing in cryptocurrencies from
different communities. A cross-sectional portfolio that implements an
inter-crypto momentum trading strategy earns a 1.08% daily return. By
dissecting the portfolio returns on behavioral factors, we confirm that our
results are not driven by behavioral mechanisms.
"
1,300,"  We develop a behavioral asset pricing model in which agents trade in a market
with information friction. Profit-maximizing agents switch between trading
strategies in response to dynamic market conditions. Due to noisy private
information about the fundamental value, the agents form different evaluations
about heterogeneous strategies. We exploit a thin set---a small
sub-population---to pointly identify this nonlinear model, and estimate the
structural parameters using extended method of moments. Based on the estimated
parameters, the model produces return time series that emulate the moments of
the real data. These results are robust across different sample periods and
estimation methods.
"
